<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Advanced Topics in CS Notes</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../mockup.css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  <style>
  :root {
    --header-color: #622; 
    --link-color: #A32; 
  }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Advanced Topics in CS Notes</h1>
</header>
<h2 id="computer-science-480---spring-2024">Computer Science 480 - Spring 2024</h2>
<center>
Jump to: <a href="index.html">Syllabus</a>, <a href="#week-1-notes">Week 1</a>, <a href="#week-2-notes">Week 2</a>, <a href="#week-3-notes">Week 3</a>, <a href="#week-4-notes">Week 4</a>, <a href="#week-5-notes">Week 5</a>, <a href="#week-6-notes">Week 6</a>, <a href="#week-7-notes">Week 7</a>, <a href="#week-8-notes">Week 8</a>, <a href="#week-9-notes">Week 9</a>, <a href="#week-10-notes">Week 10</a>, <a href="#week-11-notes">Week 11</a>, <a href="#week-12-notes">Week 12</a>, <a href="#week-13-notes">Week 13</a>, <a href="#week-14-notes">Week 14</a>
</center>
<h3 id="week-1-notes">Week 1 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Wed, Jan 17</td>
<td style="text-align: left;">Vectors and matrices</td>
</tr>
<tr class="even">
<td style="text-align: center;">Fri, Jan 19</td>
<td style="text-align: left;">Markov chains</td>
</tr>
</tbody>
</table>
<h3 id="wed-jan-17">Wed, Jan 17</h3>
<!--
Today we introduced some simple examples of Markov chains.  We also reviewed matrix multiplication.  We also defined **probability vectors** and the **dot product** (which is also known as the **inner product**).  We talked about the geometric meaning of the dot product of two vectors. We did the following examples in class. 
-->
<p>Today we reviewed vectors and matrices. Recall that a <strong>vector</strong> is all three of the following things:</p>
<ol type="1">
<li>A list of <span class="math inline">\(n\)</span> numbers.</li>
<li>A point in <span class="math inline">\(n\)</span>-dimensional space.</li>
<li>An arrow indicating a length and a direction.</li>
</ol>
<p>We denote the set of all real number vectors with <span class="math inline">\(n\)</span> entries by <span class="math inline">\(\mathbb{R}^n\)</span>. Recall that the <strong>length</strong> of a vector (also known as the <strong>norm</strong>) is: <span class="math display">\[\|v\| = \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2 }.\]</span></p>
<p>We also talked about how to multiply vectors by constants (<strong>scalar multiplication</strong>) and how to calculate the <strong>dot product</strong> (see this <a href="https://youtu.be/WNuIhXo39_k">Kahn academy video</a> for example). We stated (without proof) the fact that for any two vectors <span class="math inline">\(v, w \in \mathbb{R}^n\)</span>, <span class="math display">\[ v \cdot w = \|v\| \, \|w\| \, \cos \theta\]</span> where <span class="math inline">\(\theta\)</span> is the angle between the vectors <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>. We finished our review of vectors by saying that two vectors <span class="math inline">\(v, w\)</span> are <strong>orthogonal</strong> when <span class="math inline">\(v \cdot w = 0\)</span>. Then the <strong>orthogonal complement</strong> of a single vector <span class="math inline">\(v \in \mathbb{R}^n\)</span> is the set <span class="math display">\[v^\perp = \{w \in \mathbb{R}^n \, : \, v \cdot w = 0 \}.\]</span> If <span class="math inline">\(v\)</span> is a nonzero vector, then <span class="math inline">\(v^\perp\)</span> is an <span class="math inline">\((n-1)\)</span>-dimensional hyperspace inside of <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>After reviewing vectors we reviewed matrices and how to <a href="https://youtu.be/OMA2Mwo0aZg">multiply matrices</a>. We did the following example in class:</p>
<ol type="1">
<li>Multiply <span class="math inline">\(\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; -1 \end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 0 \\ 1 &amp; -1 \end{pmatrix}\)</span>.</li>
</ol>
<p>We finished by talking briefly about this problem from <a href="https://math.dartmouth.edu/~prob/prob/prob.pdf">Introduction to Probability by Grinstead &amp; Snell</a>:</p>
<ol start="2" type="1">
<li>The Land of Oz is blessed by many things, but not by good weather. They never have two nice days in a row. If they have a nice day, they are just as likely to have snow as rain the next day. If they have snow or rain, they have an even chance of having the same the next day. If there is change from snow or rain, only half of the time is this a change to a nice day.</li>
</ol>
<center>
<img src="Oz.png"></img>
</center>
<h3 id="fri-jan-19">Fri, Jan 19</h3>
<p>Today we looked in more detail at the Land of Oz Markov chain from last time. We started by defining the following terminology.</p>
<p>A <strong>Markov chain</strong> is a model with stages where the next state is randomly determined based only on the current state. A Markov with <span class="math inline">\(n\)</span> states can be described by a <strong>transition matrix</strong> <span class="math inline">\(Q\)</span> which has entries <span class="math inline">\(Q_{ij}\)</span> equal to the probability that the next state will be <span class="math inline">\(j\)</span> if the current state is <span class="math inline">\(i\)</span>. In the Land of Oz example, if the states Nice, Rain, Snow correspond to the row/column numbers 0, 1, 2, respectively, then the transition matrix is <span class="math display">\[Q = \begin{pmatrix} 0 &amp; 0.5 &amp; 0.5 \\ 0.25 &amp; 0.5 &amp; 0.25 \\ 0.25 &amp; 0.25 &amp; 0.5 \end{pmatrix}\]</span></p>
<p>A <strong>probability vector</strong> is a vector with nonnegative entries that add up to 1. You can use probability vectors to model your knowledge about the current state in a Markov chain. For example, if it were raining today in the Land of Oz, then you could use the probability vector <span class="math inline">\(v = (0, 1, 0)\)</span> to indicated that we are 100% sure that we are in the middle state (raining). If you multiply <span class="math inline">\(vQ\)</span>, then you get the probability row vector representing the probabilities for the states the next day. Here is how to use matrices in the Python using the <code>numpy</code> library.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>Q <span class="op">=</span> np.matrix(<span class="st">&quot;0 0.5 0.5; 0.25 0.5 0.25; 0.25 0.25 0.5&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>v <span class="op">=</span> np.matrix(<span class="st">&quot;0 1 0&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="bu">print</span>(v <span class="op">*</span> Q)</span></code></pre></div>
<ol type="1">
<li><p>Suppose that today is a nice day. What is the probability vector that describes how the weather might be the day after tomorrow?</p></li>
<li><p>What will the weather be like after 1 week if today is nice? What about if today is rainy or snowy? How much difference does the weather today make after 1 week? You can answer this last problem by computing <span class="math inline">\(Q^7\)</span>.</p></li>
</ol>
<p>We finished with this additional example which is <a href="https://math.dartmouth.edu/~doyle/docs/finite/fm2/scan/5.pdf#page=64">problem 5.7.12 in Introduction to Finite Mathematics</a> by Kemeny, Snell, Thompson.</p>
<ol start="3" type="1">
<li>A professor tries not to be late too often. On days when he is late, he is 90% sure to arrive on time the next day. When he is on time, there is a 30% chance he will be late the next day. How often is this professor late in the long run?</li>
</ol>
<!-- To calculate repeated matrix multiplications, it helps to use the [numpy matrix power function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html). For example, continuing the code above, we can compute 
```python 
print(np.linalg.matrix_power(Q,7))
```
-->
<hr />
<h3 id="week-2-notes">Week 2 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 22</td>
<td style="text-align: left;">Examples of Markov chains</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 24</td>
<td style="text-align: left;">Stationary distributions</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Jan 26</td>
<td style="text-align: left;">Random text generation</td>
</tr>
</tbody>
</table>
<h3 id="mon-jan-22">Mon, Jan 22</h3>
<p>Today we did the following workshop about Markov chains.</p>
<ul>
<li>Workshop: <a href="Workshops/MarkovChains.pdf">Markov chains</a></li>
</ul>
<h3 id="wed-jan-24">Wed, Jan 24</h3>
<p>Today we talked about some of the features we’ve seen in Markov chains. Recall that you can think of a Markov chain as a weighted directed graph where the total weight of all the edges leaving a vertex must add up to 1 (the weights correspond to probabilities).</p>
<p>In all of the examples we’ve considered so far, we have looked for the final long run probabilities for the states after many transitions.</p>
<p><strong>Definition.</strong> A probability vector <span class="math inline">\(w\)</span> is a <strong>stationary distribution</strong> for a Markov chain with transition matrix <span class="math inline">\(Q\)</span> if <span class="math display">\[wQ = w.\]</span></p>
<p>In all of the examples we’ve looked at, <span class="math display">\[\lim_{k \rightarrow \infty} v Q^k\]</span> exists and is a stationary distribution for any initial probability vector <span class="math inline">\(v\)</span>. But this doesn’t always happen for Markov chains.</p>
<ol type="1">
<li>Find a simple Markov chain and an initial probability vector <span class="math inline">\(v\)</span> such that <span class="math inline">\(\lim_{k \rightarrow \infty} v Q^k\)</span> does not converge.</li>
</ol>
<p>To better understand the long-run behavior of Markov chains, we need to review strongly connected components of a directed graph (digraph for short).</p>
<p><strong>Definition.</strong> A digraph is <strong>strongly connected</strong> if you can find a path from any start vertex <span class="math inline">\(i\)</span> to any other end vertex <span class="math inline">\(j\)</span>. A <strong>strongly connected component</strong> of a graph is a set of vertices such that (i) you can travel from any one vertex in the set to any other, and (ii) you cannot returns to the set if you leave it. Strongly connected components are also known as <strong>classes</strong> and they partition the vertices of a directed graph. A class is <strong>final</strong> if there are no edges that leave the class.</p>
<center>
<img src = "https://upload.wikimedia.org/wikipedia/commons/e/e1/Scc-1.svg" width = 300></img>
</center>
<p>In the digraph above, there is one final class <span class="math inline">\(\{f,g\}\)</span> and two other non-final classes.</p>
<div class="Theorem">
<p><strong>Theorem (Perron-Frobenius).</strong> A Markov chain always has a stationary distribution. The stationary distribution is unique if and only if the Markov chain has only one final class.</p>
</div>
<p>We did not prove this theorem, but we did apply it to the following question.</p>
<ol start="2" type="1">
<li><p>Which of the Markov chains that we’ve considered (the Land of Oz, the Tardy Professor, the Gambler’s Ruin problem, and the Coupon Collector’s problem) has a unique stationary distribution? For the one(s) that don’t have a unique stationary distribution, describe two different stationary distributions.</p></li>
<li><p>Does the Markov chain that doesn’t converge from Q1 today have a unique stationary distribution? How can you tell? Can you find it?</p></li>
</ol>
<p>Another important is question is to have criteria for when <span class="math inline">\(\lim_{k \rightarrow \infty} vQ^k\)</span> converges.</p>
<p><strong>Definition.</strong> A Markov chain with transition matrix <span class="math inline">\(Q\)</span> is <strong>regular</strong> if there is a power <span class="math inline">\(k\)</span> such that <span class="math inline">\(Q^k\)</span> has all positive entries.</p>
<div class="Theorem">
<p><strong>Theorem.</strong> A regular Markov chain with transition matrix <span class="math inline">\(Q\)</span> always has a unique stationary distribution <span class="math inline">\(w\)</span> and for any initial probability vector <span class="math inline">\(v\)</span>, <span class="math display">\[\lim_{k \rightarrow \infty} v Q^k = w.\]</span></p>
</div>
<p>We finished class by talking about how the Google PageRank algorithm uses the stationary distribution of a simple regular Markov chain to rank websites. The algorithm starts by imagining a random web surfer who clicks on links completely randomly to visit new website. You can imagine the internet as a giant directed graph and this websurfer can be modeled with an <span class="math inline">\(N\)</span> state Markov chain where <span class="math inline">\(N\)</span> is the number of websites on the internet. Unfortunately the <span class="math inline">\(N\)</span>-by-<span class="math inline">\(N\)</span> transition matrix might not be regular, so the PageRank algorithm creates a new regular Markov chain by using the following algorithm:</p>
<ul>
<li>85% of the time, the random websurfer picks a new website by randomly clicking a link.</li>
<li>15% of the time, the random websurfer picks any one of the <span class="math inline">\(N\)</span> websites on the internet (all equally likely).</li>
</ul>
<p>The 85/15 percent split was chosen because the resulting regular Markov chain converges relatively quickly (it still takes days for the computation to update), but it still settles on a stationary distribution where more popular websites are visited more than less popular websites.</p>
<h3 id="fri-jan-26">Fri, Jan 26</h3>
<p>Today we wrote a program to randomly generate text based on a source text using a Markov chain.</p>
<ul>
<li><strong>Workshop</strong>: <a href="Workshops/RandomTextGenerator.pdf">Random text generator</a></li>
</ul>
<p>Here are some <a href="sourceTexts.html">example source texts</a> you can use. You can also search online for other good examples if you want. When you are finished, you should have a program that can generate random nonsense like this:</p>
<blockquote>
<p>In the beginning when God created the great sea monsters and every winged bird of every kind on earth that bear fruit with the seed in it." And it was good. And there was morning, the second day. And God saw that it was good. Then God said, "Let there be lights in the dome and separated the waters that were gathered together he called Night. And there was evening and there was morning, the fourth day. And God saw that the light Day, and the waters bring forth living creatures of every kind, and everything that has the breath of life, I have given you every plant yielding seed of every kind, and trees of every kind bearing fruit with the seed in it.</p>
</blockquote>
<hr />
<h3 id="week-3-notes">Week 3 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 29</td>
<td style="text-align: left;">Least squares regression</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 31</td>
<td style="text-align: left;">Least squares regression - con’d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 2</td>
<td style="text-align: left;">Logistic regression</td>
</tr>
</tbody>
</table>
<h3 id="mon-jan-29">Mon, Jan 29</h3>
<p>Today we started talking about linear regression. We started with the simplest case where you want to predict a response variable (<span class="math inline">\(y\)</span>) using a single explanatory variable (<span class="math inline">\(x\)</span>). Based on the observed <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values, you want to find the best fit trend-line. We judge how good a trend-line fits the data by calculating the sum of squared deviations between the predicted <span class="math inline">\(y\)</span>-values (denoted <span class="math inline">\(\hat{y}_i\)</span>) and the actual <span class="math inline">\(y\)</span>-values (<span class="math inline">\(y_i\)</span>) at each <span class="math inline">\(x_i\)</span>.<br />
<span class="math display">\[\text{Sum of squared error} = \sum_{i = 1}^n (\hat{y}_i - y_i)^2.\]</span> We’ll see later that minimizing the sum of squared error has some nice properties. We looked at the following example. <!--_--></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/classes/spring18/math222/data/bac.csv&quot;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="bu">print</span>(df)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>x <span class="op">=</span> np.array(df.Beers)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>y <span class="op">=</span> np.array(df.BAC)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;Beers&quot;</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;BAC&quot;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>plt.plot(x,y,<span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<center>
<img src="beersBAC.png" width = 400></img>
</center>
<p>The least squares regression line will have a formula <span class="math display">\[\hat{y} = b_0 + b_1 x\]</span> where <span class="math inline">\(b_0\)</span> is the <span class="math inline">\(y\)</span>-intercept and <span class="math inline">\(b_1\)</span> is the slope. You can find these two numbers by using the <strong>normal equation</strong></p>
<!--
1. [Beers and BAC](http://people.hsc.edu/faculty-staff/blins/classes/spring18/math222/data/bac.csv)
2. [Marriage ages](http://people.hsc.edu/faculty-staff/blins/StatsExamples/marriageAges.xls) 
3. [Midterm exam grades](http://people.hsc.edu/faculty-staff/blins/StatsExamples/MidtermRegression.xlsx)
-->
<p><span class="math display">\[X^T X \beta = X^T y\]</span> where <span class="math inline">\(\beta = \begin{pmatrix} b_0 \\ b_1 \end{pmatrix}\)</span> is a column vector with the intercept and slope that we want to find, <span class="math inline">\(y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\y_n\end{pmatrix}\)</span> is the column vector with the <span class="math inline">\(y\)</span>-values from the data, and <span class="math display">\[X = \begin{pmatrix} 
1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_n 
\end{pmatrix}\]</span> is an <span class="math inline">\(n\)</span>-by-2 matrix will all 1’s in its first column and the <span class="math inline">\(x\)</span>-values from the data in its second column. The notation <span class="math inline">\(X^T\)</span> means that <strong>transpose</strong> of the matrix <span class="math inline">\(X\)</span>, which is the matrix you get if you switch all columns of <span class="math inline">\(X\)</span> to rows: <span class="math display">\[X^T = \begin{pmatrix} 
1 &amp; 1 &amp; \ldots &amp; 1 \\
x_1 &amp; x_2 &amp; \ldots &amp; x_n  
\end{pmatrix}.\]</span> One way to solve the normal equations is to multiply both sides by the inverse of the matrix <span class="math inline">\((X^T X)\)</span>:<br />
<span class="math display">\[\beta = (X^T X)^{-1} X^T y.\]</span> The <strong>inverse</strong> of a matrix <span class="math inline">\(M\)</span> is denoted <span class="math inline">\(M^{-1}\)</span>. You can only find the inverse of square matrices (same number of rows &amp; columns). Even then, not every square matrix has an inverse, but this formula almost always works for least squares regression. I only gave a vague explanation in class of why the normal equations work. But we did use Python to compute the normal equations for the example above:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>X <span class="op">=</span> np.matrix([[<span class="fl">1.0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x))],<span class="bu">list</span>(x)]).T</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>beta <span class="op">=</span> (X.T<span class="op">*</span>X).I<span class="op">*</span>X.T <span class="op">*</span> np.matrix(y).T</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="bu">print</span>(beta) <span class="co"># [[-0.0127006 ], [ 0.01796376]]</span></span></code></pre></div>
<p>Notice that if <code>A</code> is a numpy matrix, then <code>A.T</code> is its transpose, and <code>A.I</code> is its inverse (if one exists). The entries of <span class="math inline">\(\beta\)</span> are the intercept followed by the slope, so the least squares regression line for predicting blood alcohol content from the number of beers someone drinks is: <span class="math display">\[\hat{y} = -0.01270 + 0.01796 x.\]</span></p>
<p>In addition, we can tell from the slope that each extra beer someone drinks tends to increase their BAC by 0.018 points.</p>
<h3 id="wed-jan-31">Wed, Jan 31</h3>
<p>Today we continued looking at least squares regression. We covered these additional facts about the least squares regression line <span class="math display">\[\hat{y} = b_0 + b_1 x\]</span> when there is only one explanatory variable (<span class="math inline">\(x\)</span>).</p>
<ul>
<li>The slope is <span class="math inline">\(b_1 = R \dfrac{s_y}{s_x}\)</span> and</li>
<li>The line always passes through the point <span class="math inline">\((\bar{x}, \bar{y})\)</span>, which lets you find the <span class="math inline">\(y\)</span>-intercept.</li>
</ul>
<p>Here <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> denote the average <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>-values respectively, <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the standard deviations of the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>-values, and <span class="math inline">\(R\)</span> is the correlation coefficient. We defined these quantities using the norm (length) and dot products.</p>
<p><span class="math display">\[s_x = \frac{\|x - \bar{x}\mathbf{1} \| }{\sqrt{n-1}},  ~~~~~ s_y = \frac{ \|y - \bar{y}\mathbf{1} \| }{\sqrt{n-1}}, ~~~~~ R = \frac{x - \bar{x}\mathbf{1}}{\|x - \bar{x}\mathbf{1} \|} \cdot \frac{y - \bar{y}\mathbf{1}}{\|y - \bar{y}\mathbf{1} \|}\]</span></p>
<p>We used a spreadsheet to investigate these examples.</p>
<ol type="1">
<li><a href="http://people.hsc.edu/faculty-staff/blins/StatsExamples/marriageAges.xls">Marriage ages</a></li>
<li><a href="http://people.hsc.edu/faculty-staff/blins/StatsExamples/MidtermRegression.xlsx">Midterm exam grades</a></li>
<li><a href="http://people.hsc.edu/faculty-staff/blins/statsexamples/Lightning.xlsx">Lightning fatalities</a></li>
</ol>
<p>In the USA, there has been a striking decline in the number of people killed by lightning every year. The trend is strong, but it isn’t really a linear trend. So we used the normal equations to find a best fit quadratic polynomial <span class="math display">\[\hat{y} = b_0 + b_1 x + b_2 x^2.\]</span> Here is how to do this with numpy.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>df <span class="op">=</span> pd.read_excel(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/StatsExamples/Lightning.xlsx&quot;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>x <span class="op">=</span> np.array(df.year)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>y <span class="op">=</span> np.array(df.deaths)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a><span class="co"># You can use a list comprehension to enter the matrix X.</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>X <span class="op">=</span> np.matrix([[xi<span class="op">**</span>k <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)] <span class="cf">for</span> xi <span class="kw">in</span> <span class="bu">list</span>(x)])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>beta <span class="op">=</span> (X.T<span class="op">*</span>X).I<span class="op">*</span>X.T <span class="op">*</span> np.matrix(y).T</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="bu">print</span>(beta) <span class="co"># [[5.61778330e+04], [-5.42074197e+01], [ 1.30717749e-02]]</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>years <span class="op">=</span> np.array(<span class="bu">range</span>(<span class="dv">1960</span>,<span class="dv">2021</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;Year&quot;</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;Fatalities&quot;</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a>plt.plot(x,y,<span class="st">&quot;o&quot;</span>) <span class="op">+</span> plt.plot(years,<span class="fl">56177.8</span> <span class="op">-</span> <span class="fl">54.207</span><span class="op">*</span>years <span class="op">+</span> <span class="fl">0.01307</span><span class="op">*</span>years<span class="op">**</span><span class="dv">2</span>,linestyle<span class="op">=</span><span class="st">&quot;-&quot;</span>)</span></code></pre></div>
<center>
<img src = "lightning.png" width=400></img>
</center>
<!--
An even better approximation might look for a power law relationship $\hat{y} = C x^\alpha$.  We'll consider that next time. 
-->
<h3 id="fri-feb-2">Fri, Feb 2</h3>
<p>Today we did this workshop.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/Regression.pdf">Least squares regression</a></li>
</ul>
<p>Here is the Python code to download the two datasets:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>df <span class="op">=</span> pd.read_excel(<span class="st">&quot;https://people.hsc.edu/faculty-staff/blins/classes/spring23/math121/halfmarathon.xlsx&quot;</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>genders <span class="op">=</span> <span class="bu">list</span>(df.Gender)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>ages <span class="op">=</span> <span class="bu">list</span>(df.Age)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>minutes <span class="op">=</span> <span class="bu">list</span>(df.Minutes)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>df2 <span class="op">=</span> pd.read_excel(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/StatsExamples/Lightning.xlsx&quot;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>years <span class="op">=</span> np.array(df2.year)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>deaths <span class="op">=</span> np.array(df2.deaths)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>logDeaths <span class="op">=</span> np.log(deaths) <span class="co"># notice that functions work elementwise on np.arrays.</span></span></code></pre></div>
<hr />
<h3 id="week-4-notes">Week 4 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 5</td>
<td style="text-align: left;">Linear classifiers</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 7</td>
<td style="text-align: left;">Loss functions &amp; gradient descent</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 9</td>
<td style="text-align: left;">Stochastic gradient descent</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-5">Mon, Feb 5</h3>
<p>Last time we saw came up with a model to predict a runner’s race time based on their age and gender. Our model had the form <span class="math display">\[\hat{y} = b_0 + b_1 x_1 + b_2 x_2\]</span> where <span class="math inline">\(\hat{y}\)</span> is the predicted race time in minutes, <span class="math inline">\(x_1\)</span> is the runner’s age, and <span class="math inline">\(x_2\)</span> is an indicator variable which is 0 for men and 1 for women. An <strong>indicator variable</strong> is a numerical variable that is 1 if a Boolean condition is true and 0 otherwise. We can re-write our model using a dot product as: <span class="math display">\[\hat{y} = [b_0, b_1, b_2] \cdot [1, x_1, x_2].\]</span> <!-- The weight vector was [84.24795527  0.97029783 21.00086375] --> In this formula, the vector <span class="math inline">\([b_0, b_1, b_2]\)</span> is called the <strong>weight vector</strong> and <span class="math inline">\([1, x_1, x_2]\)</span> is called the <strong>feature vector</strong>. Each runner has a different feature vector, but you use the same weight vector for every runner to make a prediction about their race time.</p>
<p>If we use age and race time to predict gender using least squares, then we get this formula: <span class="math display">\[\text{predicted gender} = 0.0694 - 0.0112 \, \text{age} + 0.00705 \, \text{race_time}.\]</span> We could use the number <span class="math inline">\(\tfrac{1}{2}\)</span> as a dividing line to separate runners who we would predict are female vs. runners we would predict are male. This is a simple example of a linear classifier. A <strong>linear classifier</strong> is a rule that uses a weight vector <span class="math inline">\(w\)</span> and a feature vector <span class="math inline">\(x\)</span> and a threshold <span class="math inline">\(\theta\)</span> to decide how to classify data. You make a prediction based on whether the dot product <span class="math inline">\(w \cdot x\)</span> is greater than or less than the decision threshold <span class="math inline">\(\theta\)</span>. If we treated men as <span class="math inline">\(-1\)</span> instead of <span class="math inline">\(0\)</span>, then we could use the threshold <span class="math inline">\(\theta = 0\)</span>, which is a more common choice for linear classification.</p>
<center>
<img src="runners.png" width = 400></img>
</center>
<p>We were able to draw a picture of the line the separates individuals we would predict are women from individuals we would predict are men in the scatter plot for runners. If we had more than two variables, then we wouldn’t be able to draw a picture. And instead of a dividing line, we would get a dividing hyperplane to separate our predictions. But we could still use the same idea.</p>
<p>Using least squares regression to find our weight vector probably isn’t the best choice since the goal of least squared error isn’t really what we want. What we really want is the smallest zero-one error. <strong>Zero-one error</strong> is the error you get if you add a one for every prediction that is incorrect and a zero for every correct prediction. Both least squares error and zero-one error are examples of <strong>loss functions</strong> which measure how accurate our predictions are.</p>
<p>We finished by outlining where we are going in the next few classes. We are going to look at how to minimize different loss functions over the space of all possible weight vectors (called the <strong>weight space</strong>). We talked about how precise formulas for the optimal weight vector don’t always exist, but we can use a general technique called <strong>gradient descent</strong> that works for many different loss functions.</p>
<h3 id="wed-feb-7">Wed, Feb 7</h3>
<p>We talked about gradient descent today. For a multivariable function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>, the <strong>gradient</strong> of <span class="math inline">\(f\)</span> at a point <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_n)\)</span> is the vector <span class="math display">\[\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right).\]</span> We calculated the gradient for these examples. Here is a video that <a href="https://youtu.be/AXqhWeUEtQU">explains partial derivatives</a>.</p>
<ol type="1">
<li><p><span class="math inline">\(f(x,y) = x^2 + y^2\)</span> (<a href="https://youtu.be/_-02ze7tf08" class="uri">https://youtu.be/_-02ze7tf08</a>)</p></li>
<li><p><span class="math inline">\(f(x,y) = x^4 + y^4 + (x-1)^2 + y\)</span></p></li>
</ol>
<p>The important thing to understand about <span class="math inline">\(\nabla f\)</span> is that it always points in the direction of steepest increase. This idea leads inspires <strong>gradient descent</strong> which is a simple algorithm to find the minimum of a multivariable function.</p>
<div class="Theorem">
<p><strong>Gradient Descent Algorithm.</strong> To find the minimum of <span class="math inline">\(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</span>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <span class="math inline">\(\mathbf{x}\)</span> and a fixed (small) step size <span class="math inline">\(\eta\)</span>.</li>
<li>Find the gradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\nabla f(\mathbf{x})\)</span>.</li>
<li>Replace <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\mathbf{x}- \eta \, \nabla f(\mathbf{x})\)</span>.</li>
<li>Repeat steps 2 &amp; 3 until your gradient vector is very close to 0.</li>
</ol>
</div>
<p>Here is the code I wrote in class to implement this algorithm for example 2 above.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="co"># Step 1 initialize initial guess x and step size eta.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a>x <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a>    <span class="co"># step 2 calculate gradient</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    gradient <span class="op">=</span> np.array([<span class="dv">4</span><span class="op">*</span>x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">3</span><span class="op">+</span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">3</span><span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    <span class="co"># step 3 use the gradient to update x</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>    x <span class="op">=</span> x <span class="op">-</span> eta<span class="op">*</span>gradient</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>    <span class="bu">print</span>(x, gradient)</span></code></pre></div>
<p>You have to be careful when you pick the step size <span class="math inline">\(\eta\)</span> (eta). If it is too big, the algorithm will not converge. If it is too small, the algorithm will be very slow.</p>
<ol start="3" type="1">
<li><p>Try the code above with different values of <span class="math inline">\(\eta\)</span>. What happens if <span class="math inline">\(\eta = 0.5\)</span>? What about <span class="math inline">\(\eta = 0.01\)</span>?</p></li>
<li><p>What will happen if you use gradient descent on a function like this one which has more than one local min? <span class="math display">\[f(x) = x^4 + y^4 - 3xy\]</span></p></li>
<li><p>Find the gradient of the following sum of squared error loss function. Then use gradient descent to find the vector <span class="math inline">\(\mathbf{w}\)</span> with the minimum loss. <span class="math display">\[L(\mathbf{w}) = (\mathbf{w} \cdot [1, 0] - 1)^2 + (\mathbf{w} \cdot [1,1] - 1)^2 + (\mathbf{w} \cdot [1,2] -4)^2\]</span></p></li>
</ol>
<h3 id="fri-feb-9">Fri, Feb 9</h3>
<p>Today we did this workshop in class:</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/GradientDescent.pdf">Gradient descent</a></li>
</ul>
<p>As part of the workshop, we introduced the stochastic gradient descent algorithm which tends to be an effective way to get gradient descent to converge more quickly.</p>
<div class="Theorem">
<p><strong>Stochastic Gradient Descent Algorithm.</strong> Let <span class="math inline">\(L(\mathbf{w}) = \sum_{i = 1}^n L_i(\mathbf{w})\)</span> be a sum of simpler loss functions <span class="math inline">\(L_i(\mathbf{w})\)</span>. To minimize the total loss <span class="math inline">\(L(\mathbf{w})\)</span>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <span class="math inline">\(\mathbf{w}\)</span> and a fixed (small) step size <span class="math inline">\(\eta\)</span>.</li>
<li>Randomly choose <span class="math inline">\(i \in \{1, \ldots, n\}\)</span>.</li>
<li>Find the gradient of <span class="math inline">\(L_i\)</span> at <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\nabla L_i(\mathbf{w})\)</span>.</li>
<li>Replace <span class="math inline">\(\mathbf{w}\)</span> by <span class="math inline">\(\mathbf{w}- \eta \, \nabla L_i(\mathbf{w})\)</span>.</li>
<li>Repeat steps 2 - 4 until your gradient vectors are very close to 0.</li>
</ol>
</div>
<hr />
<h3 id="week-5-notes">Week 5 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 12</td>
<td style="text-align: left;">Hinge Loss</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 14</td>
<td style="text-align: left;">Logistic regression</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 16</td>
<td style="text-align: left;">Nonlinear classification</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-12">Mon, Feb 12</h3>
<p>In the workshop last time, we had to calculate the gradient of a function of the form <span class="math inline">\(\mathbf{w} \mapsto (\mathbf{w} \cdot \mathbf{x} - y)^2\)</span>. This is a composition of the one variable function <span class="math inline">\(f(u) = (u - y)^2\)</span> with the dot product <span class="math inline">\(\mathbf{x} \cdot \mathbf{w}\)</span>. In general, we have the following nice lemma which is one special case of the chain rule.</p>
<div class="Theorem">
<p><strong>Lemma.</strong> If <span class="math inline">\(f: \mathbb{R}\rightarrow \mathbb{R}\)</span> is differentiable, and <span class="math inline">\(L(\mathbf{w}) = f(\mathbf{x} \cdot \mathbf{w})\)</span>, then <span class="math display">\[\nabla L(\mathbf{w}) = f&#39;(\mathbf{x} \cdot \mathbf{w}) \mathbf{x}.\]</span></p>
</div>
<p>Today we looked at linear classification and talked about some of the different loss functions that we could use. We used the lemma above to help find the gradients for gradient descent. First we introduced the following terminology.</p>
<p>Suppose that we want to train a linear classifier. For each individual observed we have a feature vector <span class="math inline">\(X_i\)</span> and a category <span class="math inline">\(y_i\)</span> which is either <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span>. Our goal is to find the best weight vector <span class="math inline">\(\mathbf{w}\)</span> so for any observed feature vector <span class="math inline">\(X_i\)</span>, the sign of <span class="math inline">\(X_i \cdot \mathbf{w}\)</span> does the best possible job of predicting the corresponding value of <span class="math inline">\(y_i\)</span>. We call the number <span class="math inline">\(X_i \cdot \mathbf{w}\)</span> the <strong>score</strong> of the prediction. If we multiply the score times the correct value of <span class="math inline">\(y_i\)</span>, then we will get a positive number if the prediction is correct and a negative number if our prediction is wrong. We call this number the <strong>margin</strong> and <span class="math display">\[\text{margin}_i =  y_i \, (X_i \cdot \mathbf{w}).\]</span></p>
<p>The hinge loss function is a function of the margin that is <span class="math display">\[L_\text{hinge} (\mathbf{w}) = \begin{cases}
1 - \text{margin} &amp; \text{ if } \text{margin } &lt; 1 \\
0 &amp; \text{ if } \text{margin } \ge 1 
\end{cases}\]</span></p>
<ol type="1">
<li><p>Show that for each pair <span class="math inline">\(X_i\)</span> and <span class="math inline">\(y_i\)</span>, the gradient of the hinge loss is <span class="math display">\[\nabla L_\text{hinge} (\mathbf{w}) = \begin{cases} -y_i X_i &amp; \text{ if } \text{ margin} &lt; 1\\ 0 &amp; \text{ otherwise}. \end{cases}\]</span></p></li>
<li><p>Express the zero-one loss function as a function of the margin. Why won’t gradient descent work with zero-one loss?</p></li>
<li><p>We also looked at the absolute error loss function: <span class="math display">\[L(w) = |\text{margin} - 1|\]</span> and we calculated the gradient of that when <span class="math inline">\(y\)</span> is <span class="math inline">\(+1\)</span> and <span class="math inline">\(-1\)</span>.</p></li>
</ol>
<p>With both hinge loss and absolute error loss, the gradient vectors don’t get small when we get close to the minimum, so we have to adjust the gradient descent algorithm slightly to use a step size that gets smaller after each step.</p>
<div class="Theorem">
<p><strong>Gradient Descent Algorithm (with Variable Step Size).</strong> To find the minimum of <span class="math inline">\(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</span>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <span class="math inline">\(\mathbf{x}\)</span>, a (small) step size <span class="math inline">\(\eta\)</span>, and <span class="math inline">\(k = 1\)</span>.</li>
<li>Find the gradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\nabla f(\mathbf{x})\)</span>.</li>
<li>Replace <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\mathbf{x}- \tfrac{\eta}{\sqrt{k}} \, \nabla f(\mathbf{x})\)</span>.</li>
<li>Increment <span class="math inline">\(k\)</span>.</li>
<li>Repeat steps 2 - 4 until your function <span class="math inline">\(f\)</span> stops getting smaller.</li>
</ol>
</div>
<!--
Today we talked about using sum of absolute error instead of sum of squared error to do regression.  Although sum of squares regression is more common, there are some advantages to using sum of absolute error instead. 

1. Absolute error regression isn't affected as much by outliers. 
2. In problems where the data is sparse with lots of variables, absolute error regression is more likely come up with simpler models where some unimportant variables have coefficients equal to zero.  

We can use gradient descent to minimize the total absolute error in our predictions.  

1. What is the gradient of $L_{\mathbf{x},y}(\mathbf{w}) = |\mathbf{x} \cdot \mathbf{w} - y|$ when $\mathbf{x}$ is a feature vector, $\mathbf{w}$ is the weight vector, and $y$ is the correct output? 

-->
<h3 id="wed-feb-14">Wed, Feb 14</h3>
<p>Today we talked about logistic regression which is one of the most common ways to find a linear classifier. In a logistic regression model the score <span class="math inline">\(\mathbf{w} \cdot \mathbf{x}\)</span> is interpreted as the log-odds of a success.</p>
<p>Recall that the probability of any event is a number <span class="math inline">\(p\)</span> between 0 and 1. Sometimes in probability we talk about the odds of an event happening instead of the probability. The <strong>odds</strong> of an event is given by the formula <span class="math display">\[\text{odds} = \frac{p}{1-p}.\]</span> For example, if an event has probability <span class="math inline">\(p = 2/3\)</span>, then the odds are <span class="math inline">\(2\)</span> (we usually say 2 to 1 odds). You can also easily convert from odds back to probability by computing <span class="math display">\[p = \frac{\text{odds}}{\text{odds} + 1}.\]</span> Unlike probabilities, odds can be bigger than 1. In logistic regression, we are looking for a model of the form <span class="math display">\[\log (\text{odds}) = w_0  + w_1 x_1 + \ldots + w_n x_n = \mathbf{w} \cdot \mathbf{x}.\]</span> For example, we came up with a very simple logistic regression model to predict whether someone is male or female based on their height: <span class="math display">\[\log(\text{odds}_\text{male}) = 0.5 (\text{height}) - 33,\]</span> where height is measured in inches. We based this model on guessing the odds that someone is male or female at a couple different heights, and then guessing a simple linear trend for the log-odd.</p>
<ol type="1">
<li><p>Find the log-odds, the odds, and the probability that this model would give for someone who is 67 inches tall to be male.</p></li>
<li><p>If we randomly selected 3 people, with heights 64, 69, and 72 inches respectively, what is the probability (according to the model) that the 64 inch tall person is female and the other two are male? We call this number the <strong>likelihood</strong> of that event.</p></li>
</ol>
<p>We guessed the slope <span class="math inline">\(0.5\)</span> and y-intercept <span class="math inline">\(-33\)</span> in our model. Those probably aren’t the best possible coefficients. In logistic regression, we want the model that results in the highest likelihood of the actual data happening. We showed in class that the best coefficients <span class="math inline">\(\mathbf{w}\)</span> happen when we minimize the <strong>logistic loss function</strong> which is the same as the negative logarithm of the likelihood function.</p>
<p><span class="math display">\[L(w) = \sum_{i : y_i \text{ is a success} } -\log (p_i) + \sum_{i : y_i \text{ is a failure}} - \log(1-p_i)\]</span></p>
<p>where <span class="math inline">\(p_i = \dfrac{e^{\mathbf{w} X_i}}{e^{\mathbf{w} X_i} + 1}\)</span> is the probability of a “success” predicted by the model. <!--
We evaluate the model based on the probability that the observed results would happen if the model was true.  For each $y_i$, the predicted probability of $y_i$ being 1 is
$$p_i = \frac{e^{\mathbf{w} \cdot X_i} }{e^{\mathbf{w} \cdot X_i} + 1}$$
and if $y_i$ is $-1$, then the predicted probability for that event is 
$$ 1- p_i = \frac{1 }{e^{\mathbf{w} \cdot X_i} + 1}$$
Instead of multiplying these probabilities together, we add the logarithms to get the log-likelihood function:
$$\operatorname{LLF} = \sum_{i:\, y_i =1} \log( p_i) + \sum_{i: \, y_i = -1} \log (1 - p_i)$$
We can convert this into a loss function by making it negative. Each individual term in the loss function can be expressed as:
$$L_i(\mathbf{w}) = -(1+y_i) \log (p_i) - (1 - y_{i}) \log (1-p_i)$$
The simplest case turns out to be when $y_i = -1$, because then the loss function is 
$$L_i(\mathbf{w}) = \log(1+e^{\mathbf{w} \cdot X_i})$$
which has gradient
$$\nabla L_i(\mathbf{w}) = \frac{e^{\mathbf{w} \cdot X_i}}{1+e^{\mathbf{w} \cdot X_i}} X_i = p_i X_i$$
--> We also noted that the gradient of the terms in the logistic loss function are <span class="math display">\[\nabla L_i (\mathbf{w}) = \begin{cases} -(1-p_i) X_i &amp; \text{ if } y_i \text{ is a success} \\ p_i X_i &amp; \text{ if } y_i \text{ is a failure}. \end{cases}\]</span> So you can use (stochastic) gradient descent to find the best coefficients in a logistic regression model.</p>
<p>We looked at this example:</p>
<ul>
<li><strong>Example:</strong> <a href="https://people.hsc.edu/faculty-staff/blins/predictors.html">Predictors of success in calculus</a></li>
</ul>
<h3 id="fri-feb-16">Fri, Feb 16</h3>
<p>Today we talked about using linear classifiers to do nonlinear classification. The idea is that you can classify points <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> based on a <strong>feature extractor function</strong> <span class="math inline">\(\phi(\mathbf{x})\)</span> instead of on the raw values of <span class="math inline">\(\mathbf{x}\)</span>. We used the example of finding the best circle to separate points inside from points outside. You can do this by using the feature extractor function <span class="math display">\[\phi(\mathbf{x}) = (1, x_1, x_2, x_1^2 + x_2^2)\]</span> and then finding the best parameters <span class="math inline">\(\mathbf{w}\)</span> for the linear classifier <span class="math display">\[\operatorname{sign}(\mathbf{w} \cdot \phi(\mathbf{x})).\]</span> Even though the classifier is a nonlinear function of the data <span class="math inline">\(\mathbf{x}\)</span> it is still a linear function of the parameters <span class="math inline">\(\mathbf{w}\)</span>, so you can still use the same gradient descent techniques we’ve already discussed.</p>
<p>We also talked about the dangers of complex models with lots of parameters and complicated feature extractors. These models tend to <strong>overfit</strong> the data, which means they predict the test data very well, but then fail on real world data.</p>
<p>We finished by starting this workshop.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/LinearClassifiers.pdf">Linear classifiers</a></li>
</ul>
<hr />
<h3 id="week-6-notes">Week 6 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 19</td>
<td style="text-align: left;">Review</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 21</td>
<td style="text-align: left;"><strong>Midterm 1</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 23</td>
<td style="text-align: left;">Regularization</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-19">Mon, Feb 19</h3>
<p>Went over what you should know going in to the midterm on Wednesday. Make sure you know all of the terms in <strong>bold</strong> and how to do any of the indicated calculations by hand.</p>
<h4 id="markov-chains">Markov chains</h4>
<ul>
<li>How to represent <strong>Markov chains</strong> with graphs and <strong>transition matrices</strong>.</li>
<li>How to multiply transition matrices, find powers of matrices. How to interpret <strong>probability vectors</strong> and update them using the transition matrix.</li>
<li>Know what a <strong>stationary distribution</strong> is for a Markov chain.</li>
<li>Know how to find the <strong>classes</strong> of a Markov chain graph.</li>
<li>Know the <strong>Perron-Frobenius theorem</strong> and the definition of a <strong>final class</strong>.</li>
<li>Know the difference between <strong>absorbing</strong> and <strong>transient</strong> states.</li>
<li>Know what a <strong>regular</strong> Markov chain is.</li>
</ul>
<h4 id="regression">Regression</h4>
<ul>
<li>Be able to use a regression model <span class="math display">\[\hat{y} = b_0 + b_1 x_1 + \ldots + b_n x_n\]</span> to make predictions <span class="math inline">\(\hat{y}\)</span> about a variable <span class="math inline">\(y\)</span> based on the values of <span class="math inline">\(\mathbf{x}\)</span>.<br />
</li>
<li>Understand different <strong>loss functions</strong>.<br />
</li>
<li>Know how to find the <strong>least squares error</strong> <span class="math inline">\(\|\hat{y} - y\|^2\)</span>.</li>
<li>Be aware of the <strong>normal equations</strong> for finding the least squares error solution of <span class="math inline">\(X \beta = y\)</span>.</li>
</ul>
<h4 id="linear-classification">Linear Classification</h4>
<ul>
<li>Understand linear classification models of the form <span class="math inline">\(\hat{y} = \operatorname{sign}(\mathbf{w} \cdot \mathbf{x})\)</span>.</li>
<li>Be comfortable using a <strong>feature extractor function</strong> <span class="math inline">\(\phi\)</span> to do simple nonlinear classification tasks using a model of the form <span class="math display">\[\hat{y} = \operatorname{sign}(\mathbf{w} \cdot \phi(\mathbf{x})).\]</span></li>
</ul>
<h4 id="gradient-descent">Gradient Descent</h4>
<ul>
<li>Know the definition of the <strong>gradient</strong> <span class="math inline">\(\nabla f\)</span> of a function <span class="math inline">\(f\)</span>.</li>
<li>Be able to calculate gradients of simple (polynomial) functions.</li>
<li>Be able to calculate the gradient of functions of the form <span class="math inline">\(L(\mathbf{w}) = f(\mathbf{w} \cdot \mathbf{x})\)</span>.</li>
<li>Understand the <strong>gradient descent algorithm.</strong></li>
<li>Understand the hyperparameters <span class="math inline">\(\eta\)</span> (step size) and <span class="math inline">\(n\)</span> (number of iterations) in the gradient descent algorithm.</li>
<li>Know the following loss functions: <strong>squared error</strong>, <strong>absolute error</strong>, <strong>hinge loss</strong>, and <strong>zero-one loss</strong>.</li>
</ul>
<h4 id="logistic-regression">Logistic Regression</h4>
<ul>
<li>Be able to interpret models of the form <span class="math display">\[\log(\text{odds}) = \mathbf{w} \cdot \mathbf{x}.\]</span></li>
<li>Be able to convert from log-odds to odds and from odds to probability and vice versa.</li>
</ul>
<h3 id="fri-feb-23">Fri, Feb 23</h3>
<p>Today we looked at data from a large set of e-mails to determine which features indicate that the e-mail might be spam. We also implemented a technique called <strong>regularization</strong> where our goal is not just to minimize a loss function, but to minimize <span class="math display">\[\text{Loss} + \text{Complexity}\]</span> where the second term is a <strong>complexity function</strong> that gets larger as the weight vector <span class="math inline">\(\mathbf{w}\)</span> gets more complicated. A simple, but commonly used, complexity function is the 1-norm (AKA <span class="math inline">\(L_1\)</span>-norm) of the weight vector <span class="math display">\[\|\mathbf{w}\|_1 = |\mathbf{w}_1| + |\mathbf{w}_2| + \ldots + |\mathbf{w}_n|.\]</span> In class we used gradient descent to minimize <span class="math display">\[L(\mathbf{w}) + \lambda \|\mathbf{w}\|\]</span> where <span class="math inline">\(L(\mathbf{w})\)</span> is a loss function and <span class="math inline">\(\lambda\)</span> is a <strong>regularization constant</strong> which is another hyperparameter we can adjust to tune our model. Larger values of <span class="math inline">\(\lambda\)</span> tend to penalize large coefficients in the weight vector and also frequently lead to many less important variables getting a coefficient of zero. So regularization helps use find simpler models that are less likely to overfit the data.</p>
<p>We did the following two examples in class:</p>
<ul>
<li><p><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1mYcrmibkeViIwbr_XMJxqvGEdnmoB7rb?usp=sharing">Linear classifier for spam emails</a></p></li>
<li><p><strong>Example 2:</strong> <a href="https://colab.research.google.com/drive/18QF84AOyVu09Tc8nfWv4V-y0zd1GIHv0?usp=sharing">Linear regression to predict baby birthweight</a></p></li>
</ul>
<!--_-->
<hr />
<h3 id="week-7-notes">Week 7 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 26</td>
<td style="text-align: left;">Neural networks</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 28</td>
<td style="text-align: left;">Backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 1</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-26">Mon, Feb 26</h3>
<p>Today we introduced <strong>neural networks</strong>. These are often depicted using graphs like this.</p>
<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/800px-Colored_neural_network.svg.png" width=300></img>
</center>
<p>The image above shows a very simple neural network with just one hidden layer. It reads an input vector <span class="math inline">\(\mathbf{x}\)</span> with three entries, and then find values for 4 nodes in the hidden layer, which are then used to find the values of the 2 nodes in the output layer.</p>
<p>The simplest types of neural networks are <strong>feed forward networks</strong> which are used to convert input vectors to output vectors using weights that are determined by training. More complicated neural networks (<strong>recurrent neural networks</strong>) can recycle their outputs back to input. We won’t worry about those for now.</p>
<p>All neural networks used in machine learning focus on a very simple type of function to go from one layer to the next. Each step from layer <span class="math inline">\(k-1\)</span> to layer <span class="math inline">\(k\)</span> is a function <span class="math inline">\(F_k\)</span> which combines an <strong>affine linear transformation</strong> <span class="math inline">\(W_k \mathbf{v}_{k-1} + \mathbf{b}_k\)</span> where <span class="math inline">\(W_k\)</span> is a matrix and <span class="math inline">\(\mathbf{b}_k\)</span> is a vector with a nonlinear <strong>activation function</strong> <span class="math inline">\(\sigma\)</span>: <span class="math display">\[\mathbf{v}_{k} = F_k(\mathbf{v}_{k-1}) = \sigma(A_k \mathbf{v}_{k-1} + \mathbf{b}_k).\]</span><br />
Common choices for the activation function <span class="math inline">\(\sigma\)</span> are</p>
<ul>
<li><strong>Rectified linear unit.</strong> <span class="math inline">\(\operatorname{ReLU}(x) = \max(0, x)\)</span></li>
<li><strong>Sigmoid (hyperbolic tangent).</strong> <span class="math inline">\(\tanh(x) = \dfrac{e^x-1}{e^x + 1}\)</span>.</li>
</ul>
<center>
<figure>
<img src="HiddenLayers.png"></img>
<figcaption style="text-align:left">
<strong>Figure.</strong> An example showing how a simple neural network with two hidden layers might be structured.
</figcaption>
</figure>
</center>
<p>Notice that row <span class="math inline">\(i\)</span> of the matrix <span class="math inline">\(W_k\)</span> is a weight vector corresponding to all of the arrows that enter node <span class="math inline">\(i\)</span> in the <span class="math inline">\(k\)</span>-th layer of the neural network. The vector <span class="math inline">\(\mathbf{b}_k\)</span> is called a <strong>bias vector</strong> and it contains the constant terms in the computation.</p>
<p>It is important to have a nonlinear activation function as part of each step between layers, otherwise we would just be composing (affine) linear maps, which would just result in a single (affine) linear map at the end.</p>
<p>We can still use (stochastic) gradient descent to find all of the weights for the model, but there will be a lot more weights in a large neural network! Each entry of each of the matrices <span class="math inline">\(W_k\)</span> and <span class="math inline">\(\mathbf{b}_k\)</span> for each step is one of the weights.</p>
<p>Once we got these definitions out of the way, we took a look at this really cool website to get a feeling for how neural networks work and what they can do.</p>
<ul>
<li><strong>Example.</strong> <a href="https://playground.tensorflow.org/" class="uri">https://playground.tensorflow.org/</a></li>
</ul>
<hr />
<h3 id="week-8-notes">Week 8 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 4</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 6</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 8</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-9-notes">Week 9 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 18</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 20</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 22</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-10-notes">Week 10 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 25</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 27</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 29</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-11-notes">Week 11 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 1</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 3</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 5</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-12-notes">Week 12 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 8</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 10</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 12</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-13-notes">Week 13 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 15</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 17</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 19</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-14-notes">Week 14 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 22</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 24</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 26</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">Mon, Apr 29</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<hr />
<p><br> <br> <br> <br> <br> <br> <br> <br></p>
</body>
</html>
