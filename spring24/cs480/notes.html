<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Advanced Topics in CS Notes</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../mockup.css" />
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  <style>
  :root {
    --header-color: #622; 
    --link-color: #A32; 
  }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Advanced Topics in CS Notes</h1>
</header>
<h2 id="computer-science-480---spring-2024">Computer Science 480 - Spring 2024</h2>
<center>
Jump to: <a href="index.html">Syllabus</a>, <a href="#week-1-notes">Week 1</a>, <a href="#week-2-notes">Week 2</a>, <a href="#week-3-notes">Week 3</a>, <a href="#week-4-notes">Week 4</a>, <a href="#week-5-notes">Week 5</a>, <a href="#week-6-notes">Week 6</a>, <a href="#week-7-notes">Week 7</a>, <a href="#week-8-notes">Week 8</a>, <a href="#week-9-notes">Week 9</a>, <a href="#week-10-notes">Week 10</a>, <a href="#week-11-notes">Week 11</a>, <a href="#week-12-notes">Week 12</a>, <a href="#week-13-notes">Week 13</a>, <a href="#week-14-notes">Week 14</a>
</center>
<h3 id="week-1-notes">Week 1 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Wed, Jan 17</td>
<td style="text-align: left;">Vectors and matrices</td>
</tr>
<tr class="even">
<td style="text-align: center;">Fri, Jan 19</td>
<td style="text-align: left;">Markov chains</td>
</tr>
</tbody>
</table>
<h3 id="wed-jan-17">Wed, Jan 17</h3>
<!--
Today we introduced some simple examples of Markov chains.  We also reviewed matrix multiplication.  We also defined **probability vectors** and the **dot product** (which is also known as the **inner product**).  We talked about the geometric meaning of the dot product of two vectors. We did the following examples in class. 
-->
<p>Today we reviewed vectors and matrices. Recall that a <strong>vector</strong> is all three of the following things:</p>
<ol type="1">
<li>A list of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> numbers.</li>
<li>A point in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-dimensional space.</li>
<li>An arrow indicating a length and a direction.</li>
</ol>
<p>We denote the set of all real number vectors with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> entries by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^n</annotation></semantics></math>. Recall that the <strong>length</strong> of a vector (also known as the <strong>norm</strong>) is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mi>v</mi><mo stretchy="false" form="postfix">∥</mo><mo>=</mo><msqrt><mrow><msubsup><mi>v</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>v</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>…</mi><mo>+</mo><msubsup><mi>v</mi><mi>n</mi><mn>2</mn></msubsup></mrow></msqrt><mi>.</mi></mrow><annotation encoding="application/x-tex">\|v\| = \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2 }.</annotation></semantics></math></p>
<p>We also talked about how to multiply vectors by constants (<strong>scalar multiplication</strong>) and how to calculate the <strong>dot product</strong> (see this <a href="https://youtu.be/WNuIhXo39_k">Kahn academy video</a> for example). We stated (without proof) the fact that for any two vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>,</mo><mi>w</mi><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">v, w \in \mathbb{R}^n</annotation></semantics></math>, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>⋅</mo><mi>w</mi><mo>=</mo><mo stretchy="false" form="postfix">∥</mo><mi>v</mi><mo stretchy="false" form="postfix">∥</mo><mspace width="0.167em"></mspace><mo stretchy="false" form="postfix">∥</mo><mi>w</mi><mo stretchy="false" form="postfix">∥</mo><mspace width="0.167em"></mspace><mo>cos</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex"> v \cdot w = \|v\| \, \|w\| \, \cos \theta</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is the angle between the vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>. We finished our review of vectors by saying that two vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">v, w</annotation></semantics></math> are <strong>orthogonal</strong> when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>⋅</mo><mi>w</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">v \cdot w = 0</annotation></semantics></math>. Then the <strong>orthogonal complement</strong> of a single vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">v \in \mathbb{R}^n</annotation></semantics></math> is the set <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mo>⊥</mo></msup><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mi>w</mi><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><mspace width="0.167em"></mspace><mo>:</mo><mspace width="0.167em"></mspace><mi>v</mi><mo>⋅</mo><mi>w</mi><mo>=</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">v^\perp = \{w \in \mathbb{R}^n \, : \, v \cdot w = 0 \}.</annotation></semantics></math> If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is a nonzero vector, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>v</mi><mo>⊥</mo></msup><annotation encoding="application/x-tex">v^\perp</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n-1)</annotation></semantics></math>-dimensional hyperspace inside of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^n</annotation></semantics></math>.</p>
<p>After reviewing vectors we reviewed matrices and how to <a href="https://youtu.be/OMA2Mwo0aZg">multiply matrices</a>. We did the following example in class:</p>
<ol type="1">
<li>Multiply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mn>2</mn></mtd><mtd columnalign="center"><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mo>−</mo><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mo>−</mo><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 0 &amp; -1 \end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 1 &amp; 0 \\ 1 &amp; -1 \end{pmatrix}</annotation></semantics></math>.</li>
</ol>
<p>We finished by talking briefly about this problem from <a href="https://math.dartmouth.edu/~prob/prob/prob.pdf">Introduction to Probability by Grinstead &amp; Snell</a>:</p>
<ol start="2" type="1">
<li>The Land of Oz is blessed by many things, but not by good weather. They never have two nice days in a row. If they have a nice day, they are just as likely to have snow as rain the next day. If they have snow or rain, they have an even chance of having the same the next day. If there is change from snow or rain, only half of the time is this a change to a nice day.</li>
</ol>
<center>
<img src="Oz.png"></img>
</center>
<h3 id="fri-jan-19">Fri, Jan 19</h3>
<p>Today we looked in more detail at the Land of Oz Markov chain from last time. We started by defining the following terminology.</p>
<p>A <strong>Markov chain</strong> is a model with stages where the next state is randomly determined based only on the current state. A Markov with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> states can be described by a <strong>transition matrix</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> which has entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">Q_{ij}</annotation></semantics></math> equal to the probability that the next state will be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> if the current state is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>. In the Land of Oz example, if the states Nice, Rain, Snow correspond to the row/column numbers 0, 1, 2, respectively, then the transition matrix is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mn>0.5</mn></mtd><mtd columnalign="center"><mn>0.5</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>0.25</mn></mtd><mtd columnalign="center"><mn>0.5</mn></mtd><mtd columnalign="center"><mn>0.25</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>0.25</mn></mtd><mtd columnalign="center"><mn>0.25</mn></mtd><mtd columnalign="center"><mn>0.5</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q = \begin{pmatrix} 0 &amp; 0.5 &amp; 0.5 \\ 0.25 &amp; 0.5 &amp; 0.25 \\ 0.25 &amp; 0.25 &amp; 0.5 \end{pmatrix}</annotation></semantics></math></p>
<p>A <strong>probability vector</strong> is a vector with nonnegative entries that add up to 1. You can use probability vectors to model your knowledge about the current state in a Markov chain. For example, if it were raining today in the Land of Oz, then you could use the probability vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">v = (0, 1, 0)</annotation></semantics></math> to indicated that we are 100% sure that we are in the middle state (raining). If you multiply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">vQ</annotation></semantics></math>, then you get the probability row vector representing the probabilities for the states the next day. Here is how to use matrices in the Python using the <code>numpy</code> library.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>Q <span class="op">=</span> np.matrix(<span class="st">&quot;0 0.5 0.5; 0.25 0.5 0.25; 0.25 0.25 0.5&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>v <span class="op">=</span> np.matrix(<span class="st">&quot;0 1 0&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="bu">print</span>(v <span class="op">*</span> Q)</span></code></pre></div>
<ol type="1">
<li><p>Suppose that today is a nice day. What is the probability vector that describes how the weather might be the day after tomorrow?</p></li>
<li><p>What will the weather be like after 1 week if today is nice? What about if today is rainy or snowy? How much difference does the weather today make after 1 week? You can answer this last problem by computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Q</mi><mn>7</mn></msup><annotation encoding="application/x-tex">Q^7</annotation></semantics></math>.</p></li>
</ol>
<p>We finished with this additional example which is <a href="https://math.dartmouth.edu/~doyle/docs/finite/fm2/scan/5.pdf#page=64">problem 5.7.12 in Introduction to Finite Mathematics</a> by Kemeny, Snell, Thompson.</p>
<ol start="3" type="1">
<li>A professor tries not to be late too often. On days when he is late, he is 90% sure to arrive on time the next day. When he is on time, there is a 30% chance he will be late the next day. How often is this professor late in the long run?</li>
</ol>
<!-- To calculate repeated matrix multiplications, it helps to use the [numpy matrix power function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html). For example, continuing the code above, we can compute 
```python 
print(np.linalg.matrix_power(Q,7))
```
-->
<hr />
<h3 id="week-2-notes">Week 2 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 22</td>
<td style="text-align: left;">Examples of Markov chains</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 24</td>
<td style="text-align: left;">Stationary distributions</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Jan 26</td>
<td style="text-align: left;">Random text generation</td>
</tr>
</tbody>
</table>
<h3 id="mon-jan-22">Mon, Jan 22</h3>
<p>Today we did the following workshop about Markov chains.</p>
<ul>
<li>Workshop: <a href="Workshops/MarkovChains.pdf">Markov chains</a></li>
</ul>
<h3 id="wed-jan-24">Wed, Jan 24</h3>
<p>Today we talked about some of the features we’ve seen in Markov chains. Recall that you can think of a Markov chain as a weighted directed graph where the total weight of all the edges leaving a vertex must add up to 1 (the weights correspond to probabilities).</p>
<p>In all of the examples we’ve considered so far, we have looked for the final long run probabilities for the states after many transitions.</p>
<p><strong>Definition.</strong> A probability vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> is a <strong>stationary distribution</strong> for a Markov chain with transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> if <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mi>Q</mi><mo>=</mo><mi>w</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">wQ = w.</annotation></semantics></math></p>
<p>In all of the examples we’ve looked at, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>lim</mo><mrow><mi>k</mi><mo>→</mo><mi>∞</mi></mrow></munder><mi>v</mi><msup><mi>Q</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\lim_{k \rightarrow \infty} v Q^k</annotation></semantics></math> exists and is a stationary distribution for any initial probability vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>. But this doesn’t always happen for Markov chains.</p>
<ol type="1">
<li>Find a simple Markov chain and an initial probability vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>lim</mo><mrow><mi>k</mi><mo>→</mo><mi>∞</mi></mrow></msub><mi>v</mi><msup><mi>Q</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\lim_{k \rightarrow \infty} v Q^k</annotation></semantics></math> does not converge.</li>
</ol>
<p>To better understand the long-run behavior of Markov chains, we need to review strongly connected components of a directed graph (digraph for short).</p>
<p><strong>Definition.</strong> A digraph is <strong>strongly connected</strong> if you can find a path from any start vertex <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> to any other end vertex <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. A <strong>strongly connected component</strong> of a graph is a set of vertices such that (i) you can travel from any one vertex in the set to any other, and (ii) you cannot returns to the set if you leave it. Strongly connected components are also known as <strong>classes</strong> and they partition the vertices of a directed graph. A class is <strong>final</strong> if there are no edges that leave the class.</p>
<center>
<img src = "https://upload.wikimedia.org/wikipedia/commons/e/e1/Scc-1.svg" width = 300></img>
</center>
<p>In the digraph above, there is one final class <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><mi>f</mi><mo>,</mo><mi>g</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{f,g\}</annotation></semantics></math> and two other non-final classes.</p>
<div class="Theorem">
<p><strong>Theorem (Perron-Frobenius).</strong> A Markov chain always has a stationary distribution. The stationary distribution is unique if and only if the Markov chain has only one final class.</p>
</div>
<p>We did not prove this theorem, but we did apply it to the following question.</p>
<ol start="2" type="1">
<li><p>Which of the Markov chains that we’ve considered (the Land of Oz, the Tardy Professor, the Gambler’s Ruin problem, and the Coupon Collector’s problem) has a unique stationary distribution? For the one(s) that don’t have a unique stationary distribution, describe two different stationary distributions.</p></li>
<li><p>Does the Markov chain that doesn’t converge from Q1 today have a unique stationary distribution? How can you tell? Can you find it?</p></li>
</ol>
<p>Another important is question is to have criteria for when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>lim</mo><mrow><mi>k</mi><mo>→</mo><mi>∞</mi></mrow></msub><mi>v</mi><msup><mi>Q</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">\lim_{k \rightarrow \infty} vQ^k</annotation></semantics></math> converges.</p>
<p><strong>Definition.</strong> A Markov chain with transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is <strong>regular</strong> if there is a power <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Q</mi><mi>k</mi></msup><annotation encoding="application/x-tex">Q^k</annotation></semantics></math> has all positive entries.</p>
<div class="Theorem">
<p><strong>Theorem.</strong> A regular Markov chain with transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> always has a unique stationary distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> and for any initial probability vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>lim</mo><mrow><mi>k</mi><mo>→</mo><mi>∞</mi></mrow></munder><mi>v</mi><msup><mi>Q</mi><mi>k</mi></msup><mo>=</mo><mi>w</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\lim_{k \rightarrow \infty} v Q^k = w.</annotation></semantics></math></p>
</div>
<p>We finished class by talking about how the Google PageRank algorithm uses the stationary distribution of a simple regular Markov chain to rank websites. The algorithm starts by imagining a random web surfer who clicks on links completely randomly to visit new website. You can imagine the internet as a giant directed graph and this websurfer can be modeled with an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> state Markov chain where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the number of websites on the internet. Unfortunately the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>-by-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> transition matrix might not be regular, so the PageRank algorithm creates a new regular Markov chain by using the following algorithm:</p>
<ul>
<li>85% of the time, the random websurfer picks a new website by randomly clicking a link.</li>
<li>15% of the time, the random websurfer picks any one of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> websites on the internet (all equally likely).</li>
</ul>
<p>The 85/15 percent split was chosen because the resulting regular Markov chain converges relatively quickly (it still takes days for the computation to update), but it still settles on a stationary distribution where more popular websites are visited more than less popular websites.</p>
<h3 id="fri-jan-26">Fri, Jan 26</h3>
<p>Today we wrote a program to randomly generate text based on a source text using a Markov chain.</p>
<ul>
<li><strong>Workshop</strong>: <a href="Workshops/RandomTextGenerator.pdf">Random text generator</a></li>
</ul>
<p>Here are some <a href="sourceTexts.html">example source texts</a> you can use. You can also search online for other good examples if you want. When you are finished, you should have a program that can generate random nonsense like this:</p>
<blockquote>
<p>In the beginning when God created the great sea monsters and every winged bird of every kind on earth that bear fruit with the seed in it." And it was good. And there was morning, the second day. And God saw that it was good. Then God said, "Let there be lights in the dome and separated the waters that were gathered together he called Night. And there was evening and there was morning, the fourth day. And God saw that the light Day, and the waters bring forth living creatures of every kind, and everything that has the breath of life, I have given you every plant yielding seed of every kind, and trees of every kind bearing fruit with the seed in it.</p>
</blockquote>
<hr />
<h3 id="week-3-notes">Week 3 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 29</td>
<td style="text-align: left;">Least squares regression</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 31</td>
<td style="text-align: left;">Least squares regression - con’d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 2</td>
<td style="text-align: left;">Logistic regression</td>
</tr>
</tbody>
</table>
<h3 id="mon-jan-29">Mon, Jan 29</h3>
<p>Today we started talking about linear regression. We started with the simplest case where you want to predict a response variable (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>) using a single explanatory variable (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>). Based on the observed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> values, you want to find the best fit trend-line. We judge how good a trend-line fits the data by calculating the sum of squared deviations between the predicted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-values (denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{y}_i</annotation></semantics></math>) and the actual <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-values (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>) at each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>.<br />
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Sum of squared error</mtext><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{Sum of squared error} = \sum_{i = 1}^n (\hat{y}_i - y_i)^2.</annotation></semantics></math> We’ll see later that minimizing the sum of squared error has some nice properties. We looked at the following example. <!--_--></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/classes/spring18/math222/data/bac.csv&quot;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="bu">print</span>(df)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>x <span class="op">=</span> np.array(df.Beers)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>y <span class="op">=</span> np.array(df.BAC)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;Beers&quot;</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;BAC&quot;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>plt.plot(x,y,<span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<center>
<img src="beersBAC.png" width = 400></img>
</center>
<p>The least squares regression line will have a formula <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">\hat{y} = b_0 + b_1 x</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mn>0</mn></msub><annotation encoding="application/x-tex">b_0</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-intercept and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mn>1</mn></msub><annotation encoding="application/x-tex">b_1</annotation></semantics></math> is the slope. You can find these two numbers by using the <strong>normal equation</strong></p>
<!--
1. [Beers and BAC](http://people.hsc.edu/faculty-staff/blins/classes/spring18/math222/data/bac.csv)
2. [Marriage ages](http://people.hsc.edu/faculty-staff/blins/StatsExamples/marriageAges.xls) 
3. [Midterm exam grades](http://people.hsc.edu/faculty-staff/blins/StatsExamples/MidtermRegression.xlsx)
-->
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><mi>β</mi><mo>=</mo><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">X^T X \beta = X^T y</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><msub><mi>b</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>b</mi><mn>1</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\beta = \begin{pmatrix} b_0 \\ b_1 \end{pmatrix}</annotation></semantics></math> is a column vector with the intercept and slope that we want to find, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>y</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>y</mi><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\y_n\end{pmatrix}</annotation></semantics></math> is the column vector with the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-values from the data, and <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center"><mi>⋮</mi></mtd><mtd columnalign="center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><msub><mi>x</mi><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X = \begin{pmatrix} 
1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_n 
\end{pmatrix}</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-by-2 matrix will all 1’s in its first column and the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>-values from the data in its second column. The notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>X</mi><mi>T</mi></msup><annotation encoding="application/x-tex">X^T</annotation></semantics></math> means that <strong>transpose</strong> of the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, which is the matrix you get if you switch all columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> to rows: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mi>…</mi></mtd><mtd columnalign="center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd columnalign="center"><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd columnalign="center"><mi>…</mi></mtd><mtd columnalign="center"><msub><mi>x</mi><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">X^T = \begin{pmatrix} 
1 &amp; 1 &amp; \ldots &amp; 1 \\
x_1 &amp; x_2 &amp; \ldots &amp; x_n  
\end{pmatrix}.</annotation></semantics></math> One way to solve the normal equations is to multiply both sides by the inverse of the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(X^T X)</annotation></semantics></math>:<br />
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\beta = (X^T X)^{-1} X^T y.</annotation></semantics></math> The <strong>inverse</strong> of a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>M</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">M^{-1}</annotation></semantics></math>. You can only find the inverse of square matrices (same number of rows &amp; columns). Even then, not every square matrix has an inverse, but this formula almost always works for least squares regression. I only gave a vague explanation in class of why the normal equations work. But we did use Python to compute the normal equations for the example above:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>X <span class="op">=</span> np.matrix([[<span class="fl">1.0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x))],<span class="bu">list</span>(x)]).T</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>beta <span class="op">=</span> (X.T<span class="op">*</span>X).I<span class="op">*</span>X.T <span class="op">*</span> np.matrix(y).T</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="bu">print</span>(beta) <span class="co"># [[-0.0127006 ], [ 0.01796376]]</span></span></code></pre></div>
<p>Notice that if <code>A</code> is a numpy matrix, then <code>A.T</code> is its transpose, and <code>A.I</code> is its inverse (if one exists). The entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> are the intercept followed by the slope, so the least squares regression line for predicting blood alcohol content from the number of beers someone drinks is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mo>−</mo><mn>0.01270</mn><mo>+</mo><mn>0.01796</mn><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\hat{y} = -0.01270 + 0.01796 x.</annotation></semantics></math></p>
<p>In addition, we can tell from the slope that each extra beer someone drinks tends to increase their BAC by 0.018 points.</p>
<h3 id="wed-jan-31">Wed, Jan 31</h3>
<p>Today we continued looking at least squares regression. We covered these additional facts about the least squares regression line <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">\hat{y} = b_0 + b_1 x</annotation></semantics></math> when there is only one explanatory variable (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>).</p>
<ul>
<li>The slope is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo>=</mo><mi>R</mi><mstyle displaystyle="true"><mfrac><msub><mi>s</mi><mi>y</mi></msub><msub><mi>s</mi><mi>x</mi></msub></mfrac></mstyle></mrow><annotation encoding="application/x-tex">b_1 = R \dfrac{s_y}{s_x}</annotation></semantics></math> and</li>
<li>The line always passes through the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mo>,</mo><mover><mi>y</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\bar{x}, \bar{y})</annotation></semantics></math>, which lets you find the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-intercept.</li>
</ul>
<p>Here <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{y}</annotation></semantics></math> denote the average <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-values respectively, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>x</mi></msub><annotation encoding="application/x-tex">s_x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>y</mi></msub><annotation encoding="application/x-tex">s_y</annotation></semantics></math> are the standard deviations of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>-values, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> is the correlation coefficient. We defined these quantities using the norm (length) and dot products.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>x</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">∥</mo><mi>x</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle><mo stretchy="false" form="postfix">∥</mo></mrow><msqrt><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msqrt></mfrac><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><msub><mi>s</mi><mi>y</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">∥</mo><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle><mo stretchy="false" form="postfix">∥</mo></mrow><msqrt><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msqrt></mfrac><mo>,</mo><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>R</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><mi>x</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><mi>y</mi><mo>−</mo><mover><mi>y</mi><mo accent="true">‾</mo></mover><mstyle mathvariant="bold"><mn>1</mn></mstyle><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">s_x = \frac{\|x - \bar{x}\mathbf{1} \| }{\sqrt{n-1}},  ~~~~~ s_y = \frac{ \|y - \bar{y}\mathbf{1} \| }{\sqrt{n-1}}, ~~~~~ R = \frac{x - \bar{x}\mathbf{1}}{\|x - \bar{x}\mathbf{1} \|} \cdot \frac{y - \bar{y}\mathbf{1}}{\|y - \bar{y}\mathbf{1} \|}</annotation></semantics></math></p>
<p>We used a spreadsheet to investigate these examples.</p>
<ol type="1">
<li><a href="http://people.hsc.edu/faculty-staff/blins/StatsExamples/marriageAges.xls">Marriage ages</a></li>
<li><a href="http://people.hsc.edu/faculty-staff/blins/StatsExamples/MidtermRegression.xlsx">Midterm exam grades</a></li>
<li><a href="http://people.hsc.edu/faculty-staff/blins/statsexamples/Lightning.xlsx">Lightning fatalities</a></li>
</ol>
<p>In the USA, there has been a striking decline in the number of people killed by lightning every year. The trend is strong, but it isn’t really a linear trend. So we used the normal equations to find a best fit quadratic polynomial <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msup><mi>x</mi><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\hat{y} = b_0 + b_1 x + b_2 x^2.</annotation></semantics></math> Here is how to do this with numpy.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>df <span class="op">=</span> pd.read_excel(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/StatsExamples/Lightning.xlsx&quot;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>x <span class="op">=</span> np.array(df.year)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>y <span class="op">=</span> np.array(df.deaths)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a><span class="co"># You can use a list comprehension to enter the matrix X.</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>X <span class="op">=</span> np.matrix([[xi<span class="op">**</span>k <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)] <span class="cf">for</span> xi <span class="kw">in</span> <span class="bu">list</span>(x)])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>beta <span class="op">=</span> (X.T<span class="op">*</span>X).I<span class="op">*</span>X.T <span class="op">*</span> np.matrix(y).T</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="bu">print</span>(beta) <span class="co"># [[5.61778330e+04], [-5.42074197e+01], [ 1.30717749e-02]]</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>years <span class="op">=</span> np.array(<span class="bu">range</span>(<span class="dv">1960</span>,<span class="dv">2021</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;Year&quot;</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;Fatalities&quot;</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a>plt.plot(x,y,<span class="st">&quot;o&quot;</span>) <span class="op">+</span> plt.plot(years,<span class="fl">56177.8</span> <span class="op">-</span> <span class="fl">54.207</span><span class="op">*</span>years <span class="op">+</span> <span class="fl">0.01307</span><span class="op">*</span>years<span class="op">**</span><span class="dv">2</span>,linestyle<span class="op">=</span><span class="st">&quot;-&quot;</span>)</span></code></pre></div>
<center>
<img src = "lightning.png" width=400></img>
</center>
<!--
An even better approximation might look for a power law relationship $\hat{y} = C x^\alpha$.  We'll consider that next time. 
-->
<h3 id="fri-feb-2">Fri, Feb 2</h3>
<p>Today we did this workshop.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/Regression.pdf">Least squares regression</a></li>
</ul>
<p>Here is the Python code to download the two datasets:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>df <span class="op">=</span> pd.read_excel(<span class="st">&quot;https://people.hsc.edu/faculty-staff/blins/classes/spring23/math121/halfmarathon.xlsx&quot;</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>genders <span class="op">=</span> <span class="bu">list</span>(df.Gender)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>ages <span class="op">=</span> <span class="bu">list</span>(df.Age)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>minutes <span class="op">=</span> <span class="bu">list</span>(df.Minutes)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>df2 <span class="op">=</span> pd.read_excel(<span class="st">&quot;http://people.hsc.edu/faculty-staff/blins/StatsExamples/Lightning.xlsx&quot;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>years <span class="op">=</span> np.array(df2.year)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>deaths <span class="op">=</span> np.array(df2.deaths)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>logDeaths <span class="op">=</span> np.log(deaths) <span class="co"># notice that functions work elementwise on np.arrays.</span></span></code></pre></div>
<hr />
<h3 id="week-4-notes">Week 4 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 5</td>
<td style="text-align: left;">Linear classifiers</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 7</td>
<td style="text-align: left;">Loss functions &amp; gradient descent</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 9</td>
<td style="text-align: left;">Stochastic gradient descent</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-5">Mon, Feb 5</h3>
<p>Last time we saw came up with a model to predict a runner’s race time based on their age and gender. Our model had the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\hat{y} = b_0 + b_1 x_1 + b_2 x_2</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math> is the predicted race time in minutes, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math> is the runner’s age, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math> is an indicator variable which is 0 for men and 1 for women. An <strong>indicator variable</strong> is a numerical variable that is 1 if a Boolean condition is true and 0 otherwise. We can re-write our model using a dot product as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">[</mo><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">]</mo><mo>⋅</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">]</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\hat{y} = [b_0, b_1, b_2] \cdot [1, x_1, x_2].</annotation></semantics></math> <!-- The weight vector was [84.24795527  0.97029783 21.00086375] --> In this formula, the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[b_0, b_1, b_2]</annotation></semantics></math> is called the <strong>weight vector</strong> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[1, x_1, x_2]</annotation></semantics></math> is called the <strong>feature vector</strong>. Each runner has a different feature vector, but you use the same weight vector for every runner to make a prediction about their race time.</p>
<p>If we use age and race time to predict gender using least squares, then we get this formula: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">predicted gender</mtext><mo>=</mo><mn>0.0694</mn><mo>−</mo><mn>0.0112</mn><mspace width="0.167em"></mspace><mtext mathvariant="normal">age</mtext><mo>+</mo><mn>0.00705</mn><mspace width="0.167em"></mspace><mtext mathvariant="normal">race_time</mtext><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{predicted gender} = 0.0694 - 0.0112 \, \text{age} + 0.00705 \, \text{race_time}.</annotation></semantics></math> We could use the number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle displaystyle="false"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><annotation encoding="application/x-tex">\tfrac{1}{2}</annotation></semantics></math> as a dividing line to separate runners who we would predict are female vs. runners we would predict are male. This is a simple example of a linear classifier. A <strong>linear classifier</strong> is a rule that uses a weight vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> and a feature vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and a threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> to decide how to classify data. You make a prediction based on whether the dot product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">w \cdot x</annotation></semantics></math> is greater than or less than the decision threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. If we treated men as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math> instead of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>, then we could use the threshold <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\theta = 0</annotation></semantics></math>, which is a more common choice for linear classification.</p>
<center>
<img src="runners.png" width = 400></img>
</center>
<p>We were able to draw a picture of the line the separates individuals we would predict are women from individuals we would predict are men in the scatter plot for runners. If we had more than two variables, then we wouldn’t be able to draw a picture. And instead of a dividing line, we would get a dividing hyperplane to separate our predictions. But we could still use the same idea.</p>
<p>Using least squares regression to find our weight vector probably isn’t the best choice since the goal of least squared error isn’t really what we want. What we really want is the smallest zero-one error. <strong>Zero-one error</strong> is the error you get if you add a one for every prediction that is incorrect and a zero for every correct prediction. Both least squares error and zero-one error are examples of <strong>loss functions</strong> which measure how accurate our predictions are.</p>
<p>We finished by outlining where we are going in the next few classes. We are going to look at how to minimize different loss functions over the space of all possible weight vectors (called the <strong>weight space</strong>). We talked about how precise formulas for the optimal weight vector don’t always exist, but we can use a general technique called <strong>gradient descent</strong> that works for many different loss functions.</p>
<h3 id="wed-feb-7">Wed, Feb 7</h3>
<p>We talked about gradient descent today. For a multivariable function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><mo>→</mo><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math>, the <strong>gradient</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at a point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{x} = (x_1, \ldots, x_n)</annotation></semantics></math> is the vector <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac><mo>,</mo><mi>…</mi><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right).</annotation></semantics></math> We calculated the gradient for these examples. Here is a video that <a href="https://youtu.be/AXqhWeUEtQU">explains partial derivatives</a>.</p>
<ol type="1">
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msup><mi>y</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x,y) = x^2 + y^2</annotation></semantics></math> (<a href="https://youtu.be/_-02ze7tf08" class="uri">https://youtu.be/_-02ze7tf08</a>)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><msup><mi>y</mi><mn>4</mn></msup><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>−</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mo>+</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">f(x,y) = x^4 + y^4 + (x-1)^2 + y</annotation></semantics></math></p></li>
</ol>
<p>The important thing to understand about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is that it always points in the direction of steepest increase. This idea leads inspires <strong>gradient descent</strong> which is a simple algorithm to find the minimum of a multivariable function.</p>
<div class="Theorem">
<p><strong>Gradient Descent Algorithm.</strong> To find the minimum of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><mo>→</mo><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> and a fixed (small) step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>.</li>
<li>Find the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla f(\mathbf{x})</annotation></semantics></math>.</li>
<li>Replace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>−</mo><mi>η</mi><mspace width="0.167em"></mspace><mi>∇</mi><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}- \eta \, \nabla f(\mathbf{x})</annotation></semantics></math>.</li>
<li>Repeat steps 2 &amp; 3 until your gradient vector is very close to 0.</li>
</ol>
</div>
<p>Here is the code I wrote in class to implement this algorithm for example 2 above.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="co"># Step 1 initialize initial guess x and step size eta.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a>x <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a>    <span class="co"># step 2 calculate gradient</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    gradient <span class="op">=</span> np.array([<span class="dv">4</span><span class="op">*</span>x[<span class="dv">0</span>]<span class="op">**</span><span class="dv">3</span><span class="op">+</span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">**</span><span class="dv">3</span><span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    <span class="co"># step 3 use the gradient to update x</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>    x <span class="op">=</span> x <span class="op">-</span> eta<span class="op">*</span>gradient</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>    <span class="bu">print</span>(x, gradient)</span></code></pre></div>
<p>You have to be careful when you pick the step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> (eta). If it is too big, the algorithm will not converge. If it is too small, the algorithm will be very slow.</p>
<ol start="3" type="1">
<li><p>Try the code above with different values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>. What happens if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\eta = 0.5</annotation></semantics></math>? What about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\eta = 0.01</annotation></semantics></math>?</p></li>
<li><p>What will happen if you use gradient descent on a function like this one which has more than one local min? <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><msup><mi>y</mi><mn>4</mn></msup><mo>−</mo><mn>3</mn><mi>x</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">f(x) = x^4 + y^4 - 3xy</annotation></semantics></math></p></li>
<li><p>Find the gradient of the following sum of squared error loss function. Then use gradient descent to find the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> with the minimum loss. <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="false" form="postfix">]</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">]</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false" form="postfix">]</mo><mo>−</mo><mn>4</mn><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) = (\mathbf{w} \cdot [1, 0] - 1)^2 + (\mathbf{w} \cdot [1,1] - 1)^2 + (\mathbf{w} \cdot [1,2] -4)^2</annotation></semantics></math></p></li>
</ol>
<h3 id="fri-feb-9">Fri, Feb 9</h3>
<p>Today we did this workshop in class:</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/GradientDescent.pdf">Gradient descent</a></li>
</ul>
<p>As part of the workshop, we introduced the stochastic gradient descent algorithm which tends to be an effective way to get gradient descent to converge more quickly.</p>
<div class="Theorem">
<p><strong>Stochastic Gradient Descent Algorithm.</strong> Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) = \sum_{i = 1}^n L_i(\mathbf{w})</annotation></semantics></math> be a sum of simpler loss functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L_i(\mathbf{w})</annotation></semantics></math>. To minimize the total loss <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w})</annotation></semantics></math>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> and a fixed (small) step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>.</li>
<li>Randomly choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">i \in \{1, \ldots, n\}</annotation></semantics></math>.</li>
<li>Find the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding="application/x-tex">L_i</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla L_i(\mathbf{w})</annotation></semantics></math>.</li>
<li>Replace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>−</mo><mi>η</mi><mspace width="0.167em"></mspace><mi>∇</mi><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{w}- \eta \, \nabla L_i(\mathbf{w})</annotation></semantics></math>.</li>
<li>Repeat steps 2 - 4 until your gradient vectors are very close to 0.</li>
</ol>
</div>
<hr />
<h3 id="week-5-notes">Week 5 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 12</td>
<td style="text-align: left;">Hinge Loss</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 14</td>
<td style="text-align: left;">Logistic regression</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 16</td>
<td style="text-align: left;">Nonlinear classification</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-12">Mon, Feb 12</h3>
<p>In the workshop last time, we had to calculate the gradient of a function of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>↦</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>−</mo><mi>y</mi><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathbf{w} \mapsto (\mathbf{w} \cdot \mathbf{x} - y)^2</annotation></semantics></math>. This is a composition of the one variable function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>u</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mi>u</mi><mo>−</mo><mi>y</mi><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(u) = (u - y)^2</annotation></semantics></math> with the dot product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle></mrow><annotation encoding="application/x-tex">\mathbf{x} \cdot \mathbf{w}</annotation></semantics></math>. In general, we have the following nice lemma which is one special case of the chain rule.</p>
<div class="Theorem">
<p><strong>Lemma.</strong> If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mo>→</mo><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle></mrow><annotation encoding="application/x-tex">f: \mathbb{R}\rightarrow \mathbb{R}</annotation></semantics></math> is differentiable, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) = f(\mathbf{x} \cdot \mathbf{w})</annotation></semantics></math>, then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>f</mi><mi>′</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla L(\mathbf{w}) = f&#39;(\mathbf{x} \cdot \mathbf{w}) \mathbf{x}.</annotation></semantics></math></p>
</div>
<p>Today we looked at linear classification and talked about some of the different loss functions that we could use. We used the lemma above to help find the gradients for gradient descent. First we introduced the following terminology.</p>
<p>Suppose that we want to train a linear classifier. For each individual observed we have a feature vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math> and a category <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math> which is either <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math>. Our goal is to find the best weight vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> so for any observed feature vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>, the sign of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle></mrow><annotation encoding="application/x-tex">X_i \cdot \mathbf{w}</annotation></semantics></math> does the best possible job of predicting the corresponding value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>. We call the number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle></mrow><annotation encoding="application/x-tex">X_i \cdot \mathbf{w}</annotation></semantics></math> the <strong>score</strong> of the prediction. If we multiply the score times the correct value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>, then we will get a positive number if the prediction is correct and a negative number if our prediction is wrong. We call this number the <strong>margin</strong> and <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">margin</mtext><mi>i</mi></msub><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{margin}_i =  y_i \, (X_i \cdot \mathbf{w}).</annotation></semantics></math></p>
<p>The hinge loss function is a function of the margin that is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext mathvariant="normal">hinge</mtext></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>−</mo><mtext mathvariant="normal">margin</mtext></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> if </mtext><mspace width="0.333em"></mspace></mrow><mrow><mtext mathvariant="normal">margin </mtext><mspace width="0.333em"></mspace></mrow><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> if </mtext><mspace width="0.333em"></mspace></mrow><mrow><mtext mathvariant="normal">margin </mtext><mspace width="0.333em"></mspace></mrow><mo>≥</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">L_\text{hinge} (\mathbf{w}) = \begin{cases}
1 - \text{margin} &amp; \text{ if } \text{margin } &lt; 1 \\
0 &amp; \text{ if } \text{margin } \ge 1 
\end{cases}</annotation></semantics></math></p>
<ol type="1">
<li><p>Show that for each pair <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>, the gradient of the hinge loss is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><msub><mi>L</mi><mtext mathvariant="normal">hinge</mtext></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left"><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> if </mtext><mspace width="0.333em"></mspace></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> margin</mtext></mrow><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> otherwise</mtext></mrow><mi>.</mi></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\nabla L_\text{hinge} (\mathbf{w}) = \begin{cases} -y_i X_i &amp; \text{ if } \text{ margin} &lt; 1\\ 0 &amp; \text{ otherwise}. \end{cases}</annotation></semantics></math></p></li>
<li><p>Express the zero-one loss function as a function of the margin. Why won’t gradient descent work with zero-one loss?</p></li>
<li><p>We also looked at the absolute error loss function: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">margin</mtext><mo>−</mo><mn>1</mn><mo stretchy="false" form="prefix">|</mo></mrow><annotation encoding="application/x-tex">L(w) = |\text{margin} - 1|</annotation></semantics></math> and we calculated the gradient of that when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math>.</p></li>
</ol>
<p>With both hinge loss and absolute error loss, the gradient vectors don’t get small when we get close to the minimum, so we have to adjust the gradient descent algorithm slightly to use a step size that gets smaller after each step.</p>
<div class="Theorem">
<p><strong>Gradient Descent Algorithm (with Variable Step Size).</strong> To find the minimum of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup><mo>→</mo><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle></mrow><annotation encoding="application/x-tex">f:\mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math>,</p>
<ol type="1">
<li>Start with an initial guess for the minimum <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, a (small) step size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k = 1</annotation></semantics></math>.</li>
<li>Find the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla f(\mathbf{x})</annotation></semantics></math>.</li>
<li>Replace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>−</mo><mstyle displaystyle="false"><mfrac><mi>η</mi><msqrt><mi>k</mi></msqrt></mfrac></mstyle><mspace width="0.167em"></mspace><mi>∇</mi><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}- \tfrac{\eta}{\sqrt{k}} \, \nabla f(\mathbf{x})</annotation></semantics></math>.</li>
<li>Increment <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>.</li>
<li>Repeat steps 2 - 4 until your function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> stops getting smaller.</li>
</ol>
</div>
<!--
Today we talked about using sum of absolute error instead of sum of squared error to do regression.  Although sum of squares regression is more common, there are some advantages to using sum of absolute error instead. 

1. Absolute error regression isn't affected as much by outliers. 
2. In problems where the data is sparse with lots of variables, absolute error regression is more likely come up with simpler models where some unimportant variables have coefficients equal to zero.  

We can use gradient descent to minimize the total absolute error in our predictions.  

1. What is the gradient of $L_{\mathbf{x},y}(\mathbf{w}) = |\mathbf{x} \cdot \mathbf{w} - y|$ when $\mathbf{x}$ is a feature vector, $\mathbf{w}$ is the weight vector, and $y$ is the correct output? 

-->
<h3 id="wed-feb-14">Wed, Feb 14</h3>
<p>Today we talked about logistic regression which is one of the most common ways to find a linear classifier. In a logistic regression model the score <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle></mrow><annotation encoding="application/x-tex">\mathbf{w} \cdot \mathbf{x}</annotation></semantics></math> is interpreted as the log-odds of a success.</p>
<p>Recall that the probability of any event is a number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> between 0 and 1. Sometimes in probability we talk about the odds of an event happening instead of the probability. The <strong>odds</strong> of an event is given by the formula <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">odds</mtext><mo>=</mo><mfrac><mi>p</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{odds} = \frac{p}{1-p}.</annotation></semantics></math> For example, if an event has probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">p = 2/3</annotation></semantics></math>, then the odds are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> (we usually say 2 to 1 odds). You can also easily convert from odds back to probability by computing <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mfrac><mtext mathvariant="normal">odds</mtext><mrow><mtext mathvariant="normal">odds</mtext><mo>+</mo><mn>1</mn></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">p = \frac{\text{odds}}{\text{odds} + 1}.</annotation></semantics></math> Unlike probabilities, odds can be bigger than 1. In logistic regression, we are looking for a model of the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mo stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">odds</mtext><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>w</mi><mn>0</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi>…</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><mo>=</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>.</mi></mrow><annotation encoding="application/x-tex">\log (\text{odds}) = w_0  + w_1 x_1 + \ldots + w_n x_n = \mathbf{w} \cdot \mathbf{x}.</annotation></semantics></math> For example, we came up with a very simple logistic regression model to predict whether someone is male or female based on their height: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mo stretchy="false" form="prefix">(</mo><msub><mtext mathvariant="normal">odds</mtext><mtext mathvariant="normal">male</mtext></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>0.5</mn><mo stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">height</mtext><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mn>33</mn><mo>,</mo></mrow><annotation encoding="application/x-tex">\log(\text{odds}_\text{male}) = 0.5 (\text{height}) - 33,</annotation></semantics></math> where height is measured in inches. We based this model on guessing the odds that someone is male or female at a couple different heights, and then guessing a simple linear trend for the log-odd.</p>
<ol type="1">
<li><p>Find the log-odds, the odds, and the probability that this model would give for someone who is 67 inches tall to be male.</p></li>
<li><p>If we randomly selected 3 people, with heights 64, 69, and 72 inches respectively, what is the probability (according to the model) that the 64 inch tall person is female and the other two are male? We call this number the <strong>likelihood</strong> of that event.</p></li>
</ol>
<p>We guessed the slope <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math> and y-intercept <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>33</mn></mrow><annotation encoding="application/x-tex">-33</annotation></semantics></math> in our model. Those probably aren’t the best possible coefficients. In logistic regression, we want the model that results in the highest likelihood of the actual data happening. We showed in class that the best coefficients <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> happen when we minimize the <strong>logistic loss function</strong> which is the same as the negative logarithm of the likelihood function.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>w</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>:</mo><msub><mi>y</mi><mi>i</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> is a success</mtext></mrow></mrow></munder><mo>−</mo><mo>log</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>:</mo><msub><mi>y</mi><mi>i</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> is a failure</mtext></mrow></mrow></munder><mo>−</mo><mo>log</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(w) = \sum_{i : y_i \text{ is a success} } -\log (p_i) + \sum_{i : y_i \text{ is a failure}} - \log(1-p_i)</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><msup><mi>e</mi><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><msub><mi>X</mi><mi>i</mi></msub></mrow></msup><mrow><msup><mi>e</mi><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><msub><mi>X</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">p_i = \dfrac{e^{\mathbf{w} X_i}}{e^{\mathbf{w} X_i} + 1}</annotation></semantics></math> is the probability of a “success” predicted by the model. <!--
We evaluate the model based on the probability that the observed results would happen if the model was true.  For each $y_i$, the predicted probability of $y_i$ being 1 is
$$p_i = \frac{e^{\mathbf{w} \cdot X_i} }{e^{\mathbf{w} \cdot X_i} + 1}$$
and if $y_i$ is $-1$, then the predicted probability for that event is 
$$ 1- p_i = \frac{1 }{e^{\mathbf{w} \cdot X_i} + 1}$$
Instead of multiplying these probabilities together, we add the logarithms to get the log-likelihood function:
$$\operatorname{LLF} = \sum_{i:\, y_i =1} \log( p_i) + \sum_{i: \, y_i = -1} \log (1 - p_i)$$
We can convert this into a loss function by making it negative. Each individual term in the loss function can be expressed as:
$$L_i(\mathbf{w}) = -(1+y_i) \log (p_i) - (1 - y_{i}) \log (1-p_i)$$
The simplest case turns out to be when $y_i = -1$, because then the loss function is 
$$L_i(\mathbf{w}) = \log(1+e^{\mathbf{w} \cdot X_i})$$
which has gradient
$$\nabla L_i(\mathbf{w}) = \frac{e^{\mathbf{w} \cdot X_i}}{1+e^{\mathbf{w} \cdot X_i}} X_i = p_i X_i$$
--> We also noted that the gradient of the terms in the logistic loss function are <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><msub><mi>L</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left"><mo>−</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo><msub><mi>X</mi><mi>i</mi></msub></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> if </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>y</mi><mi>i</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> is a success</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><msub><mi>p</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub></mtd><mtd columnalign="left"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> if </mtext><mspace width="0.333em"></mspace></mrow><msub><mi>y</mi><mi>i</mi></msub><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> is a failure</mtext></mrow><mi>.</mi></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\nabla L_i (\mathbf{w}) = \begin{cases} -(1-p_i) X_i &amp; \text{ if } y_i \text{ is a success} \\ p_i X_i &amp; \text{ if } y_i \text{ is a failure}. \end{cases}</annotation></semantics></math> So you can use (stochastic) gradient descent to find the best coefficients in a logistic regression model.</p>
<p>We looked at this example:</p>
<ul>
<li><strong>Example:</strong> <a href="https://people.hsc.edu/faculty-staff/blins/predictors.html">Predictors of success in calculus</a></li>
</ul>
<h3 id="fri-feb-16">Fri, Feb 16</h3>
<p>Today we talked about using linear classifiers to do nonlinear classification. The idea is that you can classify points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x} \in \mathbb{R}^n</annotation></semantics></math> based on a <strong>feature extractor function</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\phi(\mathbf{x})</annotation></semantics></math> instead of on the raw values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>. We used the example of finding the best circle to separate points inside from points outside. You can do this by using the feature extractor function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\phi(\mathbf{x}) = (1, x_1, x_2, x_1^2 + x_2^2)</annotation></semantics></math> and then finding the best parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> for the linear classifier <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>sign</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mi>ϕ</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\operatorname{sign}(\mathbf{w} \cdot \phi(\mathbf{x})).</annotation></semantics></math> Even though the classifier is a nonlinear function of the data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> it is still a linear function of the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math>, so you can still use the same gradient descent techniques we’ve already discussed.</p>
<p>We also talked about the dangers of complex models with lots of parameters and complicated feature extractors. These models tend to <strong>overfit</strong> the data, which means they predict the test data very well, but then fail on real world data.</p>
<p>We finished by starting this workshop.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/LinearClassifiers.pdf">Linear classifiers</a></li>
</ul>
<hr />
<h3 id="week-6-notes">Week 6 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 19</td>
<td style="text-align: left;">Review</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 21</td>
<td style="text-align: left;"><strong>Midterm 1</strong></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 23</td>
<td style="text-align: left;">Regularization</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-19">Mon, Feb 19</h3>
<p>Went over what you should know going in to the midterm on Wednesday. Make sure you know all of the terms in <strong>bold</strong> and how to do any of the indicated calculations by hand.</p>
<h4 id="markov-chains">Markov chains</h4>
<ul>
<li>How to represent <strong>Markov chains</strong> with graphs and <strong>transition matrices</strong>.</li>
<li>How to multiply transition matrices, find powers of matrices. How to interpret <strong>probability vectors</strong> and update them using the transition matrix.</li>
<li>Know what a <strong>stationary distribution</strong> is for a Markov chain.</li>
<li>Know how to find the <strong>classes</strong> of a Markov chain graph.</li>
<li>Know the <strong>Perron-Frobenius theorem</strong> and the definition of a <strong>final class</strong>.</li>
<li>Know the difference between <strong>absorbing</strong> and <strong>transient</strong> states.</li>
<li>Know what a <strong>regular</strong> Markov chain is.</li>
</ul>
<h4 id="regression">Regression</h4>
<ul>
<li>Be able to use a regression model <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi>…</mi><mo>+</mo><msub><mi>b</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y} = b_0 + b_1 x_1 + \ldots + b_n x_n</annotation></semantics></math> to make predictions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math> about a variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> based on the values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>.<br />
</li>
<li>Understand different <strong>loss functions</strong>.<br />
</li>
<li>Know how to find the <strong>least squares error</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>y</mi><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\|\hat{y} - y\|^2</annotation></semantics></math>.</li>
<li>Be aware of the <strong>normal equations</strong> for finding the least squares error solution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>β</mi><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">X \beta = y</annotation></semantics></math>.</li>
</ul>
<h4 id="linear-classification">Linear Classification</h4>
<ul>
<li>Understand linear classification models of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mo>sign</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = \operatorname{sign}(\mathbf{w} \cdot \mathbf{x})</annotation></semantics></math>.</li>
<li>Be comfortable using a <strong>feature extractor function</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math> to do simple nonlinear classification tasks using a model of the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><mo>sign</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mi>ϕ</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\hat{y} = \operatorname{sign}(\mathbf{w} \cdot \phi(\mathbf{x})).</annotation></semantics></math></li>
</ul>
<h4 id="gradient-descent">Gradient Descent</h4>
<ul>
<li>Know the definition of the <strong>gradient</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> of a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.</li>
<li>Be able to calculate gradients of simple (polynomial) functions.</li>
<li>Be able to calculate the gradient of functions of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) = f(\mathbf{w} \cdot \mathbf{x})</annotation></semantics></math>.</li>
<li>Understand the <strong>gradient descent algorithm.</strong></li>
<li>Understand the hyperparameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> (step size) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> (number of iterations) in the gradient descent algorithm.</li>
<li>Know the following loss functions: <strong>squared error</strong>, <strong>absolute error</strong>, <strong>hinge loss</strong>, and <strong>zero-one loss</strong>.</li>
</ul>
<h4 id="logistic-regression">Logistic Regression</h4>
<ul>
<li>Be able to interpret models of the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mo stretchy="false" form="prefix">(</mo><mtext mathvariant="normal">odds</mtext><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>.</mi></mrow><annotation encoding="application/x-tex">\log(\text{odds}) = \mathbf{w} \cdot \mathbf{x}.</annotation></semantics></math></li>
<li>Be able to convert from log-odds to odds and from odds to probability and vice versa.</li>
</ul>
<h3 id="fri-feb-23">Fri, Feb 23</h3>
<p>Today we looked at data from a large set of e-mails to determine which features indicate that the e-mail might be spam. We also implemented a technique called <strong>regularization</strong> where our goal is not just to minimize a loss function, but to minimize <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Loss</mtext><mo>+</mo><mtext mathvariant="normal">Complexity</mtext></mrow><annotation encoding="application/x-tex">\text{Loss} + \text{Complexity}</annotation></semantics></math> where the second term is a <strong>complexity function</strong> that gets larger as the weight vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math> gets more complicated. A simple, but commonly used, complexity function is the 1-norm (AKA <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_1</annotation></semantics></math>-norm) of the weight vector <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">∥</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><msub><mo stretchy="false" form="postfix">∥</mo><mn>1</mn></msub><mo>=</mo><mo stretchy="false" form="prefix">|</mo><msub><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mn>1</mn></msub><mo stretchy="false" form="prefix">|</mo><mo>+</mo><mo stretchy="false" form="prefix">|</mo><msub><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mn>2</mn></msub><mo stretchy="false" form="prefix">|</mo><mo>+</mo><mi>…</mi><mo>+</mo><mo stretchy="false" form="prefix">|</mo><msub><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mi>n</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\|\mathbf{w}\|_1 = |\mathbf{w}_1| + |\mathbf{w}_2| + \ldots + |\mathbf{w}_n|.</annotation></semantics></math> In class we used gradient descent to minimize <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>λ</mi><mo stretchy="false" form="postfix">∥</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">∥</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) + \lambda \|\mathbf{w}\|</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L(\mathbf{w})</annotation></semantics></math> is a loss function and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is a <strong>regularization constant</strong> which is another hyperparameter we can adjust to tune our model. Larger values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> tend to penalize large coefficients in the weight vector and also frequently lead to many less important variables getting a coefficient of zero. So regularization helps use find simpler models that are less likely to overfit the data.</p>
<p>We did the following two examples in class:</p>
<ul>
<li><p><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1mYcrmibkeViIwbr_XMJxqvGEdnmoB7rb?usp=sharing">Linear classifier for spam emails</a></p></li>
<li><p><strong>Example 2:</strong> <a href="https://colab.research.google.com/drive/18QF84AOyVu09Tc8nfWv4V-y0zd1GIHv0?usp=sharing">Linear regression to predict baby birthweight</a></p></li>
</ul>
<!--_-->
<hr />
<h3 id="week-7-notes">Week 7 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 26</td>
<td style="text-align: left;">Neural networks</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 28</td>
<td style="text-align: left;">Backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 1</td>
<td style="text-align: left;">Backpropagation - con’d</td>
</tr>
</tbody>
</table>
<h3 id="mon-feb-26">Mon, Feb 26</h3>
<p>Today we introduced <strong>neural networks</strong>. These are often depicted using graphs like this.</p>
<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/800px-Colored_neural_network.svg.png" width=300></img>
</center>
<p>The image above shows a very simple neural network with just one hidden layer. It reads an input vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math> with three entries, and then find values for 4 nodes in the hidden layer, which are then used to find the values of the 2 nodes in the output layer.</p>
<p>The simplest types of neural networks are <strong>feed forward networks</strong> which are used to convert input vectors to output vectors using weights that are determined by training. More complicated neural networks (<strong>recurrent neural networks</strong>) can recycle their outputs back to input. We won’t worry about those for now.</p>
<p>All neural networks used in machine learning focus on a very simple type of function to go from one layer to the next. Each step from layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math> to layer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>F</mi><mi>k</mi></msub><annotation encoding="application/x-tex">F_k</annotation></semantics></math> which combines an <strong>affine linear transformation</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐯</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mstyle mathvariant="bold"><mi>𝐛</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{(k)} \mathbf{v}^{(k-1)} + \mathbf{b}^{(k)}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>W</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">W^{(k)}</annotation></semantics></math> is a matrix and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="bold"><mi>𝐛</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(k)}</annotation></semantics></math> is a vector with a nonlinear <strong>activation function</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mstyle mathvariant="bold"><mi>𝐯</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>=</mo><msub><mi>F</mi><mi>k</mi></msub><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐯</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>W</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐯</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo>+</mo><msup><mstyle mathvariant="bold"><mi>𝐛</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\mathbf{v}^{(k)} = F_k(\mathbf{v}^{(k-1)}) = \sigma(W^{(k)} \mathbf{v}^{(k-1)} + \mathbf{b}^{(k)}).</annotation></semantics></math><br />
Common choices for the activation function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> are</p>
<ul>
<li><strong>Rectified linear unit.</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ReLU</mo><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>max</mo><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\operatorname{ReLU}(x) = \max(0, x)</annotation></semantics></math></li>
<li><strong>Hyperbolic tangent.</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>tanh</mo><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><msup><mi>e</mi><mrow><mn>2</mn><mi>x</mi></mrow></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}} = \dfrac{e^{2x}-1}{e^{2x} + 1}</annotation></semantics></math></li>
<li><strong>Sigmoid function.</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true"><mfrac><msup><mi>e</mi><mi>x</mi></msup><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mstyle><mi>.</mi></mrow><annotation encoding="application/x-tex">\sigma(x) = \dfrac{1}{1+e^{-x}} = \dfrac{e^x}{e^x + 1}.</annotation></semantics></math></li>
</ul>
<p><em>Note: In class I wrote the formula for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>tanh</mo><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mi>/</mi><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\tanh(x/2)</annotation></semantics></math> instead of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>tanh</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\tanh x</annotation></semantics></math> when I defined the hyperbolic tangent on the board, which was a mistake. I also said that the hyperbolic tangent function is a sigmoid function, which is true, but there are other sigmoid functions. The one I’ve added above is the one many textbooks refer to as “the” sigmoid function.</em></p>
<center>
<figure>
<img src="HiddenLayers.png"></img>
<figcaption style="text-align:left">
<strong>Figure.</strong> An example showing how a simple neural network with two hidden layers might be structured.
</figcaption>
</figure>
</center>
<p>Notice that row <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> of the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>W</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">W^{(k)}</annotation></semantics></math> is a weight vector corresponding to all of the arrows that enter node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-th layer of the neural network. The vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="bold"><mi>𝐛</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(k)}</annotation></semantics></math> is called a <strong>bias vector</strong> and it contains the constant terms in the computation.</p>
<p>It is important to have a nonlinear activation function as part of each step between layers, otherwise we would just be composing (affine) linear maps, which would just result in a single (affine) linear map at the end.</p>
<p>We can still use (stochastic) gradient descent, but there are some caveats.</p>
<ol type="1">
<li><p>A neural network tends to have many more parameters than a simple linear classifier. Each entry of the matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>W</mi><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">W^{(k)}</annotation></semantics></math> and the vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="bold"><mi>𝐛</mi></mstyle><mrow><mo stretchy="false" form="prefix">(</mo><mi>k</mi><mo stretchy="false" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{b}^{(k)}</annotation></semantics></math> is a parameter that needs to be considered as we minimize the loss function.</p></li>
<li><p>Because the loss functions tend to be very complicated, they might have more than one local minimum. A single run of gradient descent might get stuck in the wrong local minimum.</p></li>
<li><p>It’s very hard to know which settings (steps size, etc.) to choose to get gradient descent to work well.</p></li>
</ol>
<p>Once we got these definitions out of the way, we took a look at this really cool website to get a feeling for how neural networks work and what they can do.</p>
<ul>
<li><strong>Example.</strong> <a href="https://playground.tensorflow.org/" class="uri">https://playground.tensorflow.org/</a></li>
</ul>
<p>We played with creating some simple neural networks for different classification problems, and saw that more complicated problems required multiple layers. We also talked briefly about epochs, but we’ll talk about that in more detail later.</p>
<h3 id="wed-feb-28">Wed, Feb 28</h3>
<p>Today we introduced <strong>backpropagation</strong> which is an algorithm for finding the gradients of functions efficiently. It is one of the main ideas that makes working with neural networks feasible.</p>
<p>We started by describing how any formula built from simple building blocks like addition, multiplication, max, powers, etc. can be re-written using a computation graph.</p>
<div class="Theorem">
<p><strong>Definition.</strong> A <strong>computation graph</strong> is a directed acyclic graph whose root node represents the final mathematical expression and every other node represents intermediate subexpressions.</p>
</div>
<p>We can label each node in a computation graph with the subexpression in the computation and possibly a variable name to represent that subexpression. A directed edge from a node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> to a node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> can be labeled with the partial derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>v</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac></mstyle><annotation encoding="application/x-tex">\dfrac{\partial v}{\partial u}</annotation></semantics></math>. Once we have a computation graph, the backpropagation algorithm works as follows.</p>
<section id="backpropagation-algorithm." class="Theorem">
<h4>Backpropagation algorithm.</h4>
<ol type="1">
<li><p>Forward pass: Starting from the values of the nodes in the graph with node inbound edges, compute each subexpression in the graph.</p></li>
<li><p>Backward pass: Once you finish computing the final expression (the root), work backwards from the root to find the partial derivatives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle displaystyle="true"><mfrac><mrow><mi>∂</mi><mspace width="0.167em"></mspace><mtext mathvariant="normal">root</mtext></mrow><mrow><mi>∂</mi><mspace width="0.167em"></mspace><mtext mathvariant="normal">subexpression</mtext></mrow></mfrac></mstyle><annotation encoding="application/x-tex">\dfrac{\partial \, \text{root}}{\partial \, \text{subexpression}}</annotation></semantics></math> for every subexpression in the graph.<br />
</p></li>
</ol>
</section>
<p>We did these examples in class:</p>
<ol type="1">
<li>Draw the computation graph for this function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>2</mn><mi>z</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>y</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x,y,z) = 2z (x+y)</annotation></semantics></math>.
<details>
<summary>Solution</summary>
<center>
<img src = "computationGraph1b.png" width = 500></img>
</center>
<p>
There is more than one way to create a computation graph for an expression, but you want to break the computation into many steps small so that it is easier to find the partial derivatives for each edge.
</details></li>
<li>Use the computation graph and the backpropagation algorithm to find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> at the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x,y,z) = (1,2,3)</annotation></semantics></math>.
<details>
<summary>Solution</summary> Below we added the forward pass computations in blue and the backward pass computations in green. Notice that each partial derivative in green is the product of the (red) partial derivative on the edge above it with the (green) partial derivative for the node above it in the graph (because of the chain rule).
<center>
<img src = "computationGraph1c.png" width = 500></img>
</center>
<p>
Therefore the gradient is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>6</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>6</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla f = (6,6,6)</annotation></semantics></math>.
</details></li>
<li>Use backpropagation to find the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla L(\mathbf{w})</annotation></semantics></math> for the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>⋅</mo><mo stretchy="false" form="prefix">[</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo stretchy="false" form="postfix">]</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(\mathbf{w}) = (\mathbf{w} \cdot [1,3] - 1)^2</annotation></semantics></math> at the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>4</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{w} = [0,4]</annotation></semantics></math>.<br />

<details>
<summary>Solution</summary> As in the previous solution we added the forward pass computations in blue and the backward pass computations in green.<br />

<center>
<img src = "computationGraph2a.png" width = 500></img>
</center>
<p>
The gradient is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle></mrow></mfrac><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>22</mn><mo>,</mo><mn>66</mn><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{w}} = [22,66]</annotation></semantics></math>.
</details></li>
</ol>
<p>One of the things we talked about in the last example is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial L}{\partial \mathbf{w}}</annotation></semantics></math> is another notation for the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mstyle mathvariant="bold"><mi>𝐰</mi></mstyle><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\nabla L(\mathbf{w})</annotation></semantics></math>. They both represent the same thing.</p>
<p>If you want more examples (along with more explanation of the process) these slides from the Stanford CS221 course are worth looking over:</p>
<ul>
<li><strong>Example</strong> <a href="https://stanford-cs221.github.io/spring2023/modules/module.html#include=machine-learning%2Flearning3.js&amp;slideId=lecture-machine-learning&amp;level=0">Stanford AI Lecture Notes on Backpropagation</a></li>
</ul>
<h3 id="fri-mar-1">Fri, Mar 1</h3>
<p>Today we did this workshop in class.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/Backpropagation.pdf">Backpropagation</a></li>
</ul>
<hr />
<h3 id="week-8-notes">Week 8 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 4</td>
<td style="text-align: left;">Tensorflow introduction</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 6</td>
<td style="text-align: left;">Classifying traffic signs</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 8</td>
<td style="text-align: left;">Image convolution</td>
</tr>
</tbody>
</table>
<h3 id="mon-march-4">Mon, March 4</h3>
<p>Today we introduced Tensorflow to create a neural network that can identify handwritten digits. The example we created in class is based on this <a href="https://www.tensorflow.org/tutorials/quickstart/beginner">TensorFlow 2 quickstart for beginners</a>.</p>
<p>We started by downloading an example dataset (the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>) that is included with TensorFlow. Then we trained a simple neural network with one hidden layer to classify numbers. We played around with it and found that it works pretty well on the training and test data, but doesn’t always work well on other handwritten examples.</p>
<ul>
<li><strong>Example:</strong> <a href="drawingTool.html">Drawing tool</a></li>
</ul>
<p>In order to explain the model, we also introduced the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a>. The code for two neural networks we trained is here:</p>
<ul>
<li><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1eaUCB1VikT1IHVayYrtbai7Cl9RaGJe8?usp=sharing">TensorFlow basic example</a></li>
</ul>
<h3 id="wed-march-6">Wed, March 6</h3>
<p>Today I gave everyone time in class to create their own neural network to classify images of German street signs into one of three categories:</p>
<ul>
<li>Category 0 - Stop signs</li>
<li>Category 1 - Left turn signs</li>
<li>Category 2 - Speed limit signs</li>
</ul>
<p>The images are stored as 32-by-32 numpy arrays of integers between 0 (black) and 255 (white). You can download the images using the following commands in Python:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="im">import</span> pickle</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="im">import</span> urllib.request</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>images, labels <span class="op">=</span> pickle.load(urllib.request.urlopen(<span class="st">&quot;https://bclins.github.io/spring24/cs480/trafficSigns.pkl&quot;</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>categories <span class="op">=</span> [<span class="st">&quot;stop sign&quot;</span>,<span class="st">&quot;turn sign&quot;</span>,<span class="st">&quot;speed limit sign&quot;</span>]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>train_images, test_images, train_labels, test_labels <span class="op">=</span> train_test_split(images,labels)</span></code></pre></div>
<p>Here pickle is a library that lets you store Python data in a file that other people can access. The URLlib lets you get the pickle data from my website. The last function <code>train_test_split()</code> from the Scikit-learn library lets us randomly separate some of the data to use for training and some to keep for testing our neural network after it is trained.</p>
<p>To see what the images look like, use a command like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a>i<span class="op">=</span><span class="dv">0</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a><span class="bu">print</span>(images[i])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a>plt.imshow(images[i])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a>plt.gray()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<center>
<img src="speedlimit.png"></img>
</center>
<p>Notice that this is a (German) speed limit sign, and has label <code>labels[i]</code> equal to 2, which is the correct label for a speed limit sign.</p>
<ol type="1">
<li>Use the code above to load the data. Then use TensorFlow to create and train a neural network model that classifies images of traffic signs. You can crib off the <a href="https://colab.research.google.com/drive/1eaUCB1VikT1IHVayYrtbai7Cl9RaGJe8?usp=sharing">TensorFlow neural network I created in class on Monday</a> to get started (see the notes below). How accurate is your neural network on the training data? What about the test data?</li>
</ol>
<p>Here are a couple of issues that will come up as you do this.</p>
<ul>
<li><p>You need to divide each image array by 255 (just like what we did on Monday) so that each entry is between 0 and 1. But because the data is a list of numpy arrays, you can’t just divide the whole list by 255 (unlike on Monday where the data was a single giant numpy array). So you’ll have to convert the variables <code>train_images</code> and <code>test_images</code> to numpy arrays:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a>train_images <span class="op">=</span> np.array(train_images)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>test_images <span class="op">=</span> np.array(test_images)</span></code></pre></div>
<p>Then it will be easy to scale everything by dividing by 255.</p></li>
<li><p>When you get to the command to train the TensorFlow model, it expects both the image data and the labels data to be numpy arrays not regular python lists. So you’ll have to convert <code>train_labels</code> and <code>test_labels</code> as well.</p></li>
<li><p>When you define the model, make sure to use the correct image size (32-by-32) for the inputs (not the 28-by-28 image size from Monday). Also, since we are only classifying three categories of signs, your final output layer only needs 3 nodes.</p></li>
<li><p>There are only 840 images in the data set, so each epoch should run pretty fast. You might want to do more than 5 epochs, but don’t do too many. Once the accuracy levels off, there is no point in running more epochs. Too many epochs increases the risk of overfitting the data.</p></li>
</ul>
<p>In class, a lot of people got neural networks that were 100% accurate. That raises the following question:</p>
<ol start="2" type="1">
<li>Which image does the neural network have the most trouble classifying? In other words, which image has the lowest maximum probability in its probability model?</li>
</ol>
<h3 id="fri-mar-8">Fri, Mar 8</h3>
<p>Today we talked briefly about the idea of a <strong>convolutional neural network</strong> and <strong>image convolution</strong> without going into too many details. The idea is to add convolution layers to the model before the regular neural network layers.</p>
<p>Here is a nice video explanation of how image convolution works with a kernel matrix.</p>
<ul>
<li><strong>Video:</strong> <a href="https://youtu.be/KuXjwB4LzSA?t=514">3Blue1Brown - But what is a convolution?</a></li>
</ul>
<p>Here is the wikipedia entry about image convolution kernels with several examples.</p>
<ul>
<li><strong>Examples:</strong> <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">Wikipedia - Kernel (image processing)</a></li>
</ul>
<p>After talking briefly about image convolution, we did this workshop in class:</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/ImageConvolution.pdf">Image convolution</a></li>
</ul>
<hr />
<h3 id="week-9-notes">Week 9 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 18</td>
<td style="text-align: left;">Unsupervised learning: k-means</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 20</td>
<td style="text-align: left;">k-means clustering - con’d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 22</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h3 id="mon-mar-18">Mon, Mar 18</h3>
<p>Today we introduced the <strong><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-means clustering algorithm</strong> which is a popular form of <strong>unsupervised learning</strong>. Unlike linear classifiers, regression, and neural networks which all need training data to learn from, unsupervised learning algorithms do not require any training information.</p>
<p>The goal of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-means clustering algorithm is to take a large collection of vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_n</annotation></semantics></math> and partition them into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> different sets (called <strong>clusters</strong>) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><msub><mi>C</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, C_2, \ldots, C_k</annotation></semantics></math> so that the sum of the squared distances from each vector to the average of its cluster is minimized. We can summarize this goal by saying that we want to minimize the following <strong>objective function</strong>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munder><mo>∑</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder><mo stretchy="false" form="postfix">∥</mo><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>z</mi><mi>i</mi></msub><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i = 1}^k \sum_{x_j \in C_i} \|x_j - z_i\|^2</annotation></semantics></math></p>
<p>where <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false" form="prefix">|</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo></mrow></mfrac></mstyle><munder><mo>∑</mo><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∈</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></munder><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">z_i = \dfrac{1}{|C_i|} \sum_{x_j \in C_i} x_j</annotation></semantics></math> is the average of the vectors in the cluster <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>i</mi></msub><annotation encoding="application/x-tex">C_i</annotation></semantics></math> (sometimes called the <strong>centroid</strong>). Notice that the objective function is a function of the clusters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_1, \ldots, C_k</annotation></semantics></math>, not the vectors. If we choose good clusters, then points in the clusters will be close together and the objective function will be small. If we choose bad clusters, then the points in a cluster will be spread out far from their average. Because clusters are discrete rather than continuous variables, we can’t use gradient descent to find the minimum, so we need an alternative. The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-mean algorithm is a way to find clusters that minimize the objective function. It is not guaranteed to find the absolute minimum, but it tends to work well in practice.</p>
<section id="k-means-algorithm" class="Theorem">
<h4><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-Means Algorithm</h4>
<p>Given a list of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, \ldots, x_n</annotation></semantics></math>, and an initial list of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> representative vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>z</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">z_1, \ldots, z_k</annotation></semantics></math> repeat the following steps several times:</p>
<ol type="1">
<li><p><strong>Update the partitition.</strong> Loop through the vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math>. For each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math> find the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math> that is closest to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math> and put <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math> into cluster <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>i</mi></msub><annotation encoding="application/x-tex">C_i</annotation></semantics></math>.</p></li>
<li><p><strong>Update the averages.</strong> Replace each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math> with the average of cluster <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>i</mi></msub><annotation encoding="application/x-tex">C_i</annotation></semantics></math>.</p></li>
</ol>
</section>
<p>There are a couple of edge cases and other issues in the algorithm to consider.</p>
<ol type="1">
<li><p>To pick the initial representative vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>z</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">z_1, \ldots, z_k</annotation></semantics></math>, one simple option is to just pick a random sample of size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> from the vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_n</annotation></semantics></math>. Other more complicated methods are available, but this simple option often works well.</p></li>
<li><p>If at any step a cluster is empty, then you can remove that cluster and its corresponding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math> in the next round of the algorithm. It is okay to end up with fewer than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> clusters at the end.</p></li>
<li><p>If you get a tie when you calculate the minimum distances from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_j</annotation></semantics></math> to different <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math>, just use the lower <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> as a tie breaker.</p></li>
<li><p>If there is a step in the algorithm where the cluster assignments don’t change, then you are done because from that point on you will always get the same clusters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>i</mi></msub><annotation encoding="application/x-tex">C_i</annotation></semantics></math> and averages <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math>.</p></li>
<li><p>The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-means algorithm might only converge to a local minimum, so you might want to try different randomly chosen starting vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math> to run the algorithm multiple times and get closer to the absolute minimum.</p></li>
<li><p>How do you know how many clusters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> to look for? The answer is that people usually try several different <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and look to see what works best.</p></li>
</ol>
<p>We finished by looking at some examples of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-means clustering.</p>
<ol type="1">
<li><p>We used it to group points in the 2-dimensional plane into clusters.</p></li>
<li><p>We also used it to cluster images of handwritten numerals.</p></li>
</ol>
<h3 id="wed-mar-20">Wed, Mar 20</h3>
<p>Today we did a workshop in class to program the k-means algorithm and apply it to some data.</p>
<ul>
<li><strong>Workshop</strong>: <a href="Workshops/Clustering.pdf">k-Means Clustering</a></li>
</ul>
<p>We looked at the following example data sets.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="co"># Example 1</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a>A<span class="op">=</span>np.array([[<span class="fl">3.39298264</span>,<span class="fl">1.94834851</span>],[<span class="fl">1.41747742</span>,<span class="op">-</span><span class="fl">1.06609251</span>],[<span class="fl">3.01888289</span>,<span class="fl">0.46840267</span>],[<span class="fl">2.59476459</span>,<span class="fl">1.13654766</span>],[<span class="fl">3.17962937</span>,<span class="fl">1.53188254</span>],[<span class="fl">4.82441491</span>,<span class="fl">1.1639604</span>],[<span class="fl">2.28793317</span>,<span class="fl">1.42545649</span>],[<span class="fl">3.92271615</span>,<span class="op">-</span><span class="fl">0.43760635</span>],[<span class="fl">1.3398049</span>,<span class="fl">0.69655122</span>],[<span class="fl">2.08559661</span>,<span class="fl">1.54877526</span>],[<span class="fl">0.62474841</span>,<span class="fl">1.83644512</span>],[<span class="fl">3.30539001</span>,<span class="fl">1.58518931</span>],[<span class="fl">2.60604184</span>,<span class="fl">1.54313128</span>],[<span class="fl">3.79532225</span>,<span class="fl">1.93029646</span>],[<span class="fl">3.18970132</span>,<span class="fl">1.37181024</span>],[<span class="fl">3.05993575</span>,<span class="fl">0.65569489</span>],[<span class="fl">2.72167775</span>,<span class="fl">1.14615515</span>],[<span class="fl">3.1319596</span>,<span class="fl">1.44343859</span>],[<span class="fl">2.04141107</span>,<span class="fl">1.16246335</span>],[<span class="fl">1.50404055</span>,<span class="op">-</span><span class="fl">0.72858893</span>],[<span class="fl">3.71654022</span>,<span class="fl">1.01262813</span>],[<span class="fl">2.15896926</span>,<span class="op">-</span><span class="fl">0.38455158</span>],[<span class="fl">3.68823967</span>,<span class="fl">0.83976529</span>],[<span class="fl">3.74768895</span>,<span class="fl">2.51485743</span>],[<span class="fl">2.60539076</span>,<span class="fl">1.59746209</span>],[<span class="fl">2.7209025</span>,<span class="fl">1.15981584</span>],[<span class="fl">2.74492997</span>,<span class="fl">0.49898255</span>],[<span class="fl">2.05625716</span>,<span class="fl">1.35438043</span>],[<span class="fl">3.35753586</span>,<span class="fl">0.72353369</span>],[<span class="fl">3.83737683</span>,<span class="fl">1.49555398</span>],[<span class="fl">4.16856373</span>,<span class="op">-</span><span class="fl">0.0796789</span>],[<span class="fl">2.72119861</span>,<span class="fl">0.48606978</span>],[<span class="fl">1.11115148</span>,<span class="fl">1.78233831</span>],[<span class="fl">2.28710689</span>,<span class="fl">0.52219863</span>],[<span class="fl">4.43789009</span>,<span class="fl">0.77498025</span>],[<span class="fl">2.63243216</span>,<span class="fl">0.20844207</span>],[<span class="fl">2.45771129</span>,<span class="fl">0.63496389</span>],[<span class="fl">4.58781488</span>,<span class="fl">0.84390828</span>],[<span class="fl">4.49266737</span>,<span class="fl">1.51652135</span>],[<span class="fl">2.19225968</span>,<span class="op">-</span><span class="fl">0.20082593</span>],[<span class="op">-</span><span class="fl">3.33108966</span>,<span class="fl">1.84044684</span>],[<span class="op">-</span><span class="fl">2.87952061</span>,<span class="fl">0.26483827</span>],[<span class="op">-</span><span class="fl">1.91099489</span>,<span class="fl">1.69754136</span>],[<span class="op">-</span><span class="fl">0.38340388</span>,<span class="op">-</span><span class="fl">0.77760197</span>],[<span class="op">-</span><span class="fl">3.27463826</span>,<span class="fl">2.28404138</span>],[<span class="op">-</span><span class="fl">2.82674904</span>,<span class="fl">1.10755789</span>],[<span class="op">-</span><span class="fl">3.58454208</span>,<span class="fl">2.46692198</span>],[<span class="op">-</span><span class="fl">2.7890395</span>,<span class="fl">3.35552427</span>],[<span class="op">-</span><span class="fl">2.13916854</span>,<span class="fl">2.91111573</span>],[<span class="op">-</span><span class="fl">1.75712374</span>,<span class="fl">2.91114338</span>],[<span class="op">-</span><span class="fl">0.19020027</span>,<span class="fl">3.05790698</span>],[<span class="op">-</span><span class="fl">1.03813622</span>,<span class="fl">1.68133303</span>],[<span class="op">-</span><span class="fl">2.45250168</span>,<span class="fl">2.00668745</span>],[<span class="op">-</span><span class="fl">3.32914595</span>,<span class="fl">2.98236229</span>],[<span class="op">-</span><span class="fl">2.7359661</span>,<span class="fl">0.74214967</span>],[<span class="op">-</span><span class="fl">2.14185246</span>,<span class="fl">1.5616797</span>],[<span class="op">-</span><span class="fl">1.14146589</span>,<span class="fl">2.77219574</span>],[<span class="op">-</span><span class="fl">1.17915129</span>,<span class="fl">2.97068053</span>],[<span class="op">-</span><span class="fl">3.91823034</span>,<span class="fl">2.36180472</span>],[<span class="op">-</span><span class="fl">2.77497322</span>,<span class="fl">2.75405217</span>],[<span class="op">-</span><span class="fl">1.74017177</span>,<span class="op">-</span><span class="fl">0.0112908</span>],[<span class="op">-</span><span class="fl">2.54457885</span>,<span class="op">-</span><span class="fl">0.08030844</span>],[<span class="op">-</span><span class="fl">2.24386365</span>,<span class="fl">1.68118681</span>],[<span class="op">-</span><span class="fl">2.13929886</span>,<span class="fl">1.80696979</span>],[<span class="op">-</span><span class="fl">3.38453642</span>,<span class="fl">1.91595502</span>],[<span class="op">-</span><span class="fl">2.74969769</span>,<span class="fl">2.98272972</span>],[<span class="op">-</span><span class="fl">2.02762516</span>,<span class="fl">2.28374355</span>],[<span class="op">-</span><span class="fl">1.57081931</span>,<span class="fl">0.95091447</span>],[<span class="op">-</span><span class="fl">2.25055478</span>,<span class="fl">2.97364212</span>],[<span class="op">-</span><span class="fl">2.05333346</span>,<span class="fl">2.21319462</span>],[<span class="op">-</span><span class="fl">0.30966156</span>,<span class="fl">1.68360366</span>],[<span class="op">-</span><span class="fl">2.663673</span>,<span class="fl">2.40502817</span>],[<span class="op">-</span><span class="fl">2.39529563</span>,<span class="fl">1.10011018</span>],[<span class="op">-</span><span class="fl">2.1959445</span>,<span class="fl">1.71060711</span>],[<span class="op">-</span><span class="fl">2.97219654</span>,<span class="fl">2.11575234</span>],[<span class="op">-</span><span class="fl">2.84580477</span>,<span class="fl">0.20557117</span>],[<span class="op">-</span><span class="fl">2.6173328</span>,<span class="fl">0.74921416</span>],[<span class="op">-</span><span class="fl">1.52159101</span>,<span class="fl">2.65001544</span>],[<span class="op">-</span><span class="fl">2.4433551</span>,<span class="fl">1.02685595</span>],[<span class="op">-</span><span class="fl">2.12929786</span>,<span class="fl">0.72600607</span>],[<span class="fl">0.19492455</span>,<span class="op">-</span><span class="fl">1.67703055</span>],[<span class="op">-</span><span class="fl">0.76991854</span>,<span class="op">-</span><span class="fl">3.11650723</span>],[<span class="fl">1.82199478</span>,<span class="op">-</span><span class="fl">2.65757475</span>],[<span class="fl">1.77235937</span>,<span class="op">-</span><span class="fl">4.37291202</span>],[<span class="fl">1.62185379</span>,<span class="op">-</span><span class="fl">2.73819575</span>],[<span class="fl">0.53052864</span>,<span class="op">-</span><span class="fl">2.87300213</span>],[<span class="fl">2.211509</span>,<span class="op">-</span><span class="fl">1.5945835</span>],[<span class="fl">0.75629947</span>,<span class="op">-</span><span class="fl">2.34063626</span>],[<span class="fl">0.48747361</span>,<span class="op">-</span><span class="fl">4.712812</span>],[<span class="fl">0.98946836</span>,<span class="op">-</span><span class="fl">1.6469933</span>],[<span class="fl">0.14140644</span>,<span class="op">-</span><span class="fl">2.45932312</span>],[<span class="fl">1.59394818</span>,<span class="op">-</span><span class="fl">3.54982431</span>],[<span class="fl">0.82900382</span>,<span class="op">-</span><span class="fl">1.49088422</span>],[<span class="fl">1.91330565</span>,<span class="op">-</span><span class="fl">3.10304953</span>],[<span class="fl">1.55071551</span>,<span class="op">-</span><span class="fl">1.3019889</span>],[<span class="fl">2.07262932</span>,<span class="op">-</span><span class="fl">2.83695848</span>],[<span class="fl">0.6309725</span>,<span class="op">-</span><span class="fl">2.43503632</span>],[<span class="fl">0.40672592</span>,<span class="op">-</span><span class="fl">3.17878599</span>],[<span class="fl">0.63487873</span>,<span class="op">-</span><span class="fl">1.64755885</span>],[<span class="fl">0.14652861</span>,<span class="op">-</span><span class="fl">2.56779286</span>],[<span class="fl">2.89050861</span>,<span class="op">-</span><span class="fl">4.12863705</span>],[<span class="fl">0.65470396</span>,<span class="op">-</span><span class="fl">3.0693579</span>],[<span class="op">-</span><span class="fl">0.89294102</span>,<span class="op">-</span><span class="fl">2.13657012</span>],[<span class="fl">1.93693144</span>,<span class="op">-</span><span class="fl">3.23388432</span>],[<span class="fl">1.79613072</span>,<span class="op">-</span><span class="fl">3.77096511</span>],[<span class="fl">1.81757032</span>,<span class="op">-</span><span class="fl">4.82310411</span>],[<span class="fl">2.04578013</span>,<span class="op">-</span><span class="fl">3.37053541</span>],[<span class="fl">1.16715492</span>,<span class="op">-</span><span class="fl">2.41535253</span>],[<span class="op">-</span><span class="fl">0.45211248</span>,<span class="op">-</span><span class="fl">2.70447155</span>],[<span class="fl">1.12654038</span>,<span class="op">-</span><span class="fl">2.70539792</span>],[<span class="fl">1.77919211</span>,<span class="op">-</span><span class="fl">3.27680203</span>],[<span class="fl">0.71004689</span>,<span class="op">-</span><span class="fl">3.606527</span>],[<span class="fl">2.45568189</span>,<span class="op">-</span><span class="fl">3.32399397</span>],[<span class="fl">2.70726689</span>,<span class="op">-</span><span class="fl">5.08994183</span>],[<span class="fl">1.46983163</span>,<span class="op">-</span><span class="fl">4.02313571</span>],[<span class="fl">1.20150021</span>,<span class="op">-</span><span class="fl">5.0482586</span>],[<span class="fl">1.43997273</span>,<span class="op">-</span><span class="fl">3.06096932</span>],[<span class="fl">0.52519365</span>,<span class="op">-</span><span class="fl">2.15182127</span>],[<span class="fl">1.7338365</span>,<span class="op">-</span><span class="fl">4.58949412</span>],[<span class="fl">2.20227891</span>,<span class="op">-</span><span class="fl">2.41147888</span>]])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a><span class="co"># Example 2</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a>X <span class="op">=</span> np.array([[<span class="dv">62606</span>,<span class="fl">7.96</span>],[<span class="dv">48265</span>,<span class="fl">9.15</span>],[<span class="dv">42330</span>,<span class="fl">8.52</span>],[<span class="dv">47603</span>,<span class="fl">8.68</span>],[<span class="dv">42878</span>,<span class="fl">8.12</span>],[<span class="dv">40247</span>,<span class="fl">7.99</span>],[<span class="dv">10098</span>,<span class="fl">3.32</span>],[<span class="dv">2104</span>,<span class="fl">7.23</span>],[<span class="dv">8943</span>,<span class="fl">6.86</span>],[<span class="dv">11290</span>,<span class="fl">5.62</span>],[<span class="dv">55060</span>,<span class="fl">9.09</span>],[<span class="dv">6374</span>,<span class="fl">7.02</span>],[<span class="dv">33440</span>,<span class="dv">8</span>],[<span class="dv">9898</span>,<span class="fl">6.45</span>],[<span class="dv">34260</span>,<span class="fl">7.99</span>],[<span class="dv">30645</span>,<span class="fl">8.3</span>],[<span class="dv">56530</span>,<span class="fl">8.89</span>],[<span class="dv">83832</span>,<span class="fl">9.03</span>],[<span class="dv">54708</span>,<span class="fl">9.25</span>],[<span class="dv">75504</span>,<span class="fl">9.87</span>],[<span class="dv">58118</span>,<span class="fl">9.22</span>],[<span class="dv">49678</span>,<span class="fl">9.25</span>],[<span class="dv">51557</span>,<span class="fl">8.29</span>],[<span class="dv">50195</span>,<span class="fl">8.99</span>],[<span class="dv">65233</span>,<span class="fl">6.02</span>],[<span class="dv">42823</span>,<span class="fl">7.89</span>],[<span class="dv">81347</span>,<span class="fl">9.24</span>],[<span class="dv">42084</span>,<span class="fl">9.26</span>],[<span class="dv">22670</span>,<span class="fl">8.11</span>],[<span class="dv">23203</span>,<span class="fl">8.75</span>],[<span class="dv">16593</span>,<span class="fl">7.88</span>],[<span class="dv">17876</span>,<span class="fl">7.88</span>],[<span class="dv">22850</span>,<span class="dv">8</span>],[<span class="dv">9126</span>,<span class="fl">4.56</span>],[<span class="dv">7594</span>,<span class="fl">5.98</span>],[<span class="dv">11387</span>,<span class="fl">6.88</span>],[<span class="dv">4048</span>,<span class="fl">6.39</span>],[<span class="dv">3109</span>,<span class="fl">6.94</span>],[<span class="dv">2715</span>,<span class="fl">3.08</span>],[<span class="dv">9421</span>,<span class="fl">7.02</span>],[<span class="dv">6178</span>,<span class="fl">6.78</span>],[<span class="dv">15560</span>,<span class="dv">8</span>],[<span class="dv">6051</span>,<span class="fl">7.15</span>],[<span class="dv">2028</span>,<span class="fl">4.2</span>],[<span class="dv">3035</span>,<span class="fl">3.9</span>],[<span class="dv">23219</span>,<span class="fl">2.89</span>],[<span class="dv">70474</span>,<span class="fl">2.75</span>],[<span class="dv">1541</span>,<span class="dv">4</span>],[<span class="dv">1887</span>,<span class="fl">4.61</span>],[<span class="dv">5314</span>,<span class="fl">2.14</span>],[<span class="dv">4867</span>,<span class="fl">3.21</span>],[<span class="dv">507</span>,<span class="fl">1.55</span>]])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a><span class="co"># Example 3</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a>Z <span class="op">=</span> np.array([[<span class="fl">1.38416797780957</span>,<span class="fl">0.484344328440116</span>],[<span class="fl">0.799874365341908</span>,<span class="fl">1.02175202153451</span>],[<span class="fl">0.558065384796926</span>,<span class="fl">0.73724206636689</span>],[<span class="fl">0.772902580148853</span>,<span class="fl">0.809498562917397</span>],[<span class="fl">0.580392481905376</span>,<span class="fl">0.556600824990623</span>],[<span class="fl">0.473197970058055</span>,<span class="fl">0.497892421543337</span>],[<span class="op">-</span><span class="fl">0.755159056808105</span>,<span class="op">-</span><span class="fl">1.61109407152458</span>],[<span class="op">-</span><span class="fl">1.08085762302151</span>,<span class="fl">0.154674062928429</span>],[<span class="op">-</span><span class="fl">0.802217080823178</span>,<span class="op">-</span><span class="fl">0.0124190853446179</span>],[<span class="op">-</span><span class="fl">0.706593546309433</span>,<span class="op">-</span><span class="fl">0.572406933611046</span>],[<span class="fl">1.0767222209111</span>,<span class="fl">0.994655835328071</span>],[<span class="op">-</span><span class="fl">0.90688553423852</span>,<span class="fl">0.0598374112058886</span>],[<span class="fl">0.195861199953642</span>,<span class="fl">0.502408452577743</span>],[<span class="op">-</span><span class="fl">0.763307632395131</span>,<span class="op">-</span><span class="fl">0.197576357755292</span>],[<span class="fl">0.229270359860446</span>,<span class="fl">0.497892421543337</span>],[<span class="fl">0.0819848561249604</span>,<span class="fl">0.637889383609944</span>],[<span class="fl">1.13661425147574</span>,<span class="fl">0.904335214639938</span>],[<span class="fl">2.24897630486059</span>,<span class="fl">0.967559649121631</span>],[<span class="fl">1.06238072787793</span>,<span class="fl">1.06691233187858</span>],[<span class="fl">1.90966961741685</span>,<span class="fl">1.34690625601179</span>],[<span class="fl">1.20131394163672</span>,<span class="fl">1.05336423877536</span>],[<span class="fl">0.857444051864243</span>,<span class="fl">1.06691233187858</span>],[<span class="fl">0.933999919504348</span>,<span class="fl">0.633373352575536</span>],[<span class="fl">0.878508119756704</span>,<span class="fl">0.949495524984004</span>],[<span class="fl">1.49119951814515</span>,<span class="op">-</span><span class="fl">0.391765692234779</span>],[<span class="fl">0.578151623618944</span>,<span class="fl">0.45273211119927</span>],[<span class="fl">2.1477302531918</span>,<span class="fl">1.06239630084417</span>],[<span class="fl">0.548042636824884</span>,<span class="fl">1.07142836291298</span>],[<span class="op">-</span><span class="fl">0.242939595407682</span>,<span class="fl">0.552084793956216</span>],[<span class="op">-</span><span class="fl">0.221223641468259</span>,<span class="fl">0.841110780158244</span>],[<span class="op">-</span><span class="fl">0.490534064619452</span>,<span class="fl">0.448216080164863</span>],[<span class="op">-</span><span class="fl">0.438260952228683</span>,<span class="fl">0.448216080164863</span>],[<span class="op">-</span><span class="fl">0.235605877379359</span>,<span class="fl">0.502408452577743</span>],[<span class="op">-</span><span class="fl">0.794761134161049</span>,<span class="op">-</span><span class="fl">1.05110622325815</span>],[<span class="op">-</span><span class="fl">0.857179223157665</span>,<span class="op">-</span><span class="fl">0.409829816372405</span>],[<span class="op">-</span><span class="fl">0.702641487149726</span>,<span class="op">-</span><span class="fl">0.00338702327580474</span>],[<span class="op">-</span><span class="fl">1.00165346831563</span>,<span class="op">-</span><span class="fl">0.224672543961732</span>],[<span class="op">-</span><span class="fl">1.03991103069671</span>,<span class="fl">0.0237091629306356</span>],[<span class="op">-</span><span class="fl">1.05596372460315</span>,<span class="op">-</span><span class="fl">1.71947881635034</span>],[<span class="op">-</span><span class="fl">0.782741985170187</span>,<span class="fl">0.0598374112058886</span>],[<span class="op">-</span><span class="fl">0.914871138313805</span>,<span class="op">-</span><span class="fl">0.0485473336198714</span>],[<span class="op">-</span><span class="fl">0.532621457526439</span>,<span class="fl">0.502408452577743</span>],[<span class="op">-</span><span class="fl">0.920045483811567</span>,<span class="fl">0.118545814653176</span>],[<span class="op">-</span><span class="fl">1.08395408174458</span>,<span class="op">-</span><span class="fl">1.21368334049679</span>],[<span class="op">-</span><span class="fl">1.04292600366391</span>,<span class="op">-</span><span class="fl">1.34916427152899</span>],[<span class="op">-</span><span class="fl">0.220571755421297</span>,<span class="op">-</span><span class="fl">1.80528340600407</span>],[<span class="fl">1.70473294140316</span>,<span class="op">-</span><span class="fl">1.86850784048576</span>],[<span class="op">-</span><span class="fl">1.10379586329899</span>,<span class="op">-</span><span class="fl">1.30400396118493</span>],[<span class="op">-</span><span class="fl">1.08969882753344</span>,<span class="op">-</span><span class="fl">1.02852606808612</span>],[<span class="op">-</span><span class="fl">0.950072984849755</span>,<span class="op">-</span><span class="fl">2.14398573358457</span>],[<span class="op">-</span><span class="fl">0.968285051286758</span>,<span class="op">-</span><span class="fl">1.66077041290306</span>],[<span class="op">-</span><span class="fl">1.14592399908391</span>,<span class="op">-</span><span class="fl">2.41043156461456</span>]])</span></code></pre></div>
<p>Since each of these data sets contains 2-dimensional data, you can plot the clusters when you are done with this code.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a>    clusterArray <span class="op">=</span> np.array(cluster)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a>    plt.plot(clusterArray[:,<span class="dv">0</span>], clusterArray[:,<span class="dv">1</span>],<span class="st">&#39;o&#39;</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<h3 id="fri-mar-22">Fri, Mar 22</h3>
<p>Today we talked about some disadvantages of k-means clustering. We looked at the last two examples from Wednesday’s workshop and we observed that they look the same when you graph them. The only difference is the scale. The x-values in the first example are much more spread out than than the x-values in the second. Because of that, your clusters end up very different. When the x-values are much larger than the y-values, differences in the y-values get drowned out by the differences in the x-values, so you end up basing your clusters only on the x-values (see the left image below).</p>
<center>
<figure>
<table>
<tr>
<td>
<img src="Clusters2.png" width=300></img>
</td>
<td>
<img src="Clusters1.png" width=300></img>
</td>
</tr>
</table>
<!--<figcaption>When the x-values are much more spread out than the y-values (left), the clusters ignore the y-values.  Standardizing the data so they have similar measures of spread fixes the problem (right).</figcaption>-->
</figure>
</center>
<p>Standardizing the x-values and the y-values so they have similar spreads fixes the problem (right image above).</p>
<p>We talked about how to standardize data by computing z-values.<br />
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover></mrow><mi>s</mi></mfrac></mrow><annotation encoding="application/x-tex">z = \frac{x - \bar{x}}{s}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math> is the average of the data and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> is the <strong>standard deviation</strong>. I briefly described how to find the standard deviation in Excel and numpy.</p>
<p>Another problem that arises in k-means clustering is the curse of dimensionality. The <strong>curse of dimensionality</strong> refers to several problems that arise when dealing with high dimensional data including:</p>
<ul>
<li>Some problems get exponentially more complicated as the dimension increases.</li>
<li>You need a lot more data to explore high dimensional spaces.</li>
<li>Randomly points in a bounded set in high dimension tend to all be similar distances apart. For example, random points in the unit hypercube tend to all be in the corners and not in the middle!</li>
</ul>
<p>We looked at this Google Colab example.</p>
<ul>
<li><strong>Example</strong>: <a href="https://colab.research.google.com/drive/1uibgpwo_0NcA91FK4n2ndCsRYoEBYtSE?usp=sharing">The Curse of Dimensionality</a></li>
</ul>
<p>One way to deal with the curse of dimensionality is to use dimension reduction techniques, which we will talk about next week.</p>
<p>We finished by briefly mentioning some applications of k-means clustering including</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Color_quantization">Color quantization</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Document_clustering">Document clustering</a></p></li>
</ul>
<hr />
<h3 id="week-10-notes">Week 10 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 25</td>
<td style="text-align: left;">Principal component analysis</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 27</td>
<td style="text-align: left;">k-Nearest neighbors algorithm</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 29</td>
<td style="text-align: left;">Dimension reduction</td>
</tr>
</tbody>
</table>
<h3 id="mon-mar-25">Mon, Mar 25</h3>
<p>Today we talked about dimension reduction using <strong>principal component analysis</strong>. Principal component analysis is an idea from linear algebra that allows us to take data vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_n \in \mathbb{R}^d</annotation></semantics></math> where the dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is very large and replaced them with vectors in a much lower dimensional space that still capture most of the information that was contained in the original vectors. Principal component analysis is one of the most commonly used methods for <strong>dimension reduction</strong>.</p>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> be an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-by-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> data matrix with rows <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_n</annotation></semantics></math> that are each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-dimensional. Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">‾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math> be the row vector with entries equal to the average of each column of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. Then the <strong>covariance matrix</strong> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>T</mi></msup><mo stretchy="false" form="prefix">(</mo><mi>X</mi><mo>−</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q = \frac{1}{n-1} (X-\bar{x})^T (X-\bar{x})</annotation></semantics></math> You can use the command <code>np.cov(X.T)</code> to compute the covariance matrix for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. (Note: you need to compute the transpose of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> because numpy defines the covariance matrix in terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-by-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> data matrices, which is the opposite of how we defined them).</p>
<p>By the <a href="https://en.wikipedia.org/wiki/Spectral_theorem">spectral theorem</a> from linear algebra, there is a diagonal matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> (with decreasing diagonal entries) and a matrix with orthogonal columns <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> (both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>-by-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>) such that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>W</mi><mi>D</mi><msup><mi>W</mi><mi>T</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">Q = W D W^T.</annotation></semantics></math> The columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> are the <strong>principle components</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. You can transform the data matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> into a new data matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> with uncorrelated variables (orthogonal columns) by calculating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mi>X</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">T = X W</annotation></semantics></math>. You can also create a reduced data matrix by computing <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>k</mi></msub><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">T_k = X W_k</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_k</annotation></semantics></math> is the submatrix of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> containing only the first <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> columns (the first <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> principal components). You can also reduce the dimension of a single row vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^d</annotation></semantics></math> by computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x W_k</annotation></semantics></math> which will be in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="double-struck"><mi>ℝ</mi></mstyle><mi>k</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^k</annotation></semantics></math>.</p>
<p>Here is numpy code to find the matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>. Note that numpy puts the diagonal entries (which are called the <strong>eigenvalues</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>) in increasing rather than decreasing order. Therefore we need to choose the <em>last</em> k columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> if we want <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mi>k</mi></msub><annotation encoding="application/x-tex">W_k</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a>Q <span class="op">=</span> np.cov(X.T)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true"></a>d, W <span class="op">=</span> np.linalg.eigh(Q)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true"></a>k <span class="op">=</span> <span class="dv">2</span> <span class="co"># choose how many dimensions you want</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true"></a>Wk <span class="op">=</span> W[:,<span class="op">-</span>k:] <span class="co"># use the last k columns of W</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true"></a>Tk <span class="op">=</span> X <span class="op">@</span> Wk</span></code></pre></div>
<p>In order to recover (approximately) the original data matrix, you can compute <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>≈</mo><msub><mi>T</mi><mi>k</mi></msub><msubsup><mi>W</mi><mi>k</mi><mi>T</mi></msubsup><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub><msubsup><mi>W</mi><mi>k</mi><mi>T</mi></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">X \approx T_k W_k^T = X W_k W_k^T.</annotation></semantics></math><br />
If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">k = d</annotation></semantics></math>, then this will completely recover the data matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, otherwise you’ll get a matrix that is the same shape as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, but the data in it will be compressed.</p>
<p>We finished class by doing a principal component analysis of 28 images of faces from the <a href="http://cvc.cs.yale.edu/cvc/projects/yalefacesB/yalefacesB.html">Yale face database B</a>.</p>
<center>
<img src="faces.png" width = 500></img>
</center>
<ul>
<li><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1_CKDdLQY7egvofW1RPWTLHQygEhSgjRX?usp=sharing">Principal component analysis of faces</a></li>
</ul>
<h3 id="wed-mar-27">Wed, Mar 27</h3>
<p>Today we talked some more about principal component analysis. We started with a simple data matrix with 2 dimensional rows.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a>X <span class="op">=</span> np.array([[<span class="op">-</span><span class="fl">1.176</span>,<span class="op">-</span><span class="fl">0.381</span>],[<span class="fl">4.299</span>,<span class="fl">5.560</span>],[<span class="op">-</span><span class="fl">1.270</span>,<span class="op">-</span><span class="fl">0.504</span>],[<span class="fl">1.110</span>,<span class="fl">0.872</span>],[<span class="fl">0.993</span>,<span class="op">-</span><span class="fl">2.293</span>],[<span class="fl">1.072</span>,<span class="op">-</span><span class="fl">2.637</span>],[<span class="op">-</span><span class="fl">0.706</span>,<span class="fl">0.130</span>],[<span class="op">-</span><span class="fl">0.769</span>,<span class="fl">0.933</span>],[<span class="fl">1.994</span>,<span class="fl">1.891</span>],[<span class="op">-</span><span class="fl">0.519</span>,<span class="op">-</span><span class="fl">0.583</span>],[<span class="op">-</span><span class="fl">2.484</span>,<span class="op">-</span><span class="fl">3.332</span>],[<span class="fl">0.242</span>,<span class="fl">0.079</span>],[<span class="fl">2.637</span>,<span class="fl">6.201</span>],[<span class="op">-</span><span class="fl">2.355</span>,<span class="op">-</span><span class="fl">3.816</span>],[<span class="fl">2.235</span>,<span class="fl">5.870</span>],[<span class="fl">0.835</span>,<span class="fl">2.000</span>],[<span class="fl">0.102</span>,<span class="op">-</span><span class="fl">1.634</span>],[<span class="op">-</span><span class="fl">0.404</span>,<span class="op">-</span><span class="fl">1.923</span>],[<span class="fl">0.751</span>,<span class="fl">4.235</span>],[<span class="op">-</span><span class="fl">1.180</span>,<span class="op">-</span><span class="fl">2.588</span>],[<span class="op">-</span><span class="fl">0.527</span>,<span class="fl">2.032</span>],[<span class="fl">2.563</span>,<span class="fl">2.954</span>],[<span class="op">-</span><span class="fl">1.812</span>,<span class="fl">0.269</span>],[<span class="op">-</span><span class="fl">1.063</span>,<span class="op">-</span><span class="fl">2.531</span>],[<span class="op">-</span><span class="fl">0.452</span>,<span class="fl">3.400</span>],[<span class="fl">1.005</span>,<span class="fl">0.904</span>],[<span class="fl">1.989</span>,<span class="fl">1.574</span>],[<span class="op">-</span><span class="fl">1.513</span>,<span class="op">-</span><span class="fl">1.100</span>],[<span class="op">-</span><span class="fl">1.587</span>,<span class="op">-</span><span class="fl">0.461</span>],[<span class="fl">0.019</span>,<span class="op">-</span><span class="fl">0.514</span>],[<span class="fl">1.394</span>,<span class="fl">2.163</span>],[<span class="op">-</span><span class="fl">1.424</span>,<span class="fl">0.307</span>],[<span class="fl">0.665</span>,<span class="op">-</span><span class="fl">0.568</span>],[<span class="fl">1.026</span>,<span class="fl">3.185</span>],[<span class="op">-</span><span class="fl">1.818</span>,<span class="op">-</span><span class="fl">0.114</span>],[<span class="op">-</span><span class="fl">3.490</span>,<span class="op">-</span><span class="fl">3.137</span>],[<span class="op">-</span><span class="fl">2.007</span>,<span class="op">-</span><span class="fl">0.326</span>],[<span class="op">-</span><span class="fl">0.130</span>,<span class="fl">0.283</span>],[<span class="op">-</span><span class="fl">0.010</span>,<span class="op">-</span><span class="fl">0.191</span>],[<span class="op">-</span><span class="fl">1.199</span>,<span class="op">-</span><span class="fl">1.026</span>],[<span class="op">-</span><span class="fl">1.886</span>,<span class="op">-</span><span class="fl">3.086</span>],[<span class="op">-</span><span class="fl">0.607</span>,<span class="op">-</span><span class="fl">0.866</span>],[<span class="op">-</span><span class="fl">1.525</span>,<span class="op">-</span><span class="fl">2.866</span>],[<span class="op">-</span><span class="fl">1.128</span>,<span class="op">-</span><span class="fl">1.697</span>],[<span class="fl">0.240</span>,<span class="op">-</span><span class="fl">2.340</span>],[<span class="op">-</span><span class="fl">2.117</span>,<span class="op">-</span><span class="fl">2.566</span>],[<span class="op">-</span><span class="fl">2.671</span>,<span class="op">-</span><span class="fl">3.556</span>],[<span class="fl">3.018</span>,<span class="fl">3.430</span>],[<span class="op">-</span><span class="fl">0.965</span>,<span class="op">-</span><span class="fl">1.628</span>],[<span class="fl">0.943</span>,<span class="op">-</span><span class="fl">0.191</span>],[<span class="fl">0.451</span>,<span class="fl">1.524</span>],[<span class="fl">2.159</span>,<span class="fl">1.307</span>],[<span class="op">-</span><span class="fl">1.651</span>,<span class="op">-</span><span class="fl">2.146</span>],[<span class="fl">0.476</span>,<span class="op">-</span><span class="fl">1.478</span>],[<span class="fl">1.849</span>,<span class="fl">3.421</span>],[<span class="fl">2.680</span>,<span class="fl">2.693</span>],[<span class="fl">2.079</span>,<span class="fl">3.357</span>],[<span class="fl">0.545</span>,<span class="fl">2.885</span>],[<span class="op">-</span><span class="fl">0.111</span>,<span class="op">-</span><span class="fl">2.637</span>],[<span class="op">-</span><span class="fl">0.938</span>,<span class="op">-</span><span class="fl">1.413</span>],[<span class="op">-</span><span class="fl">2.806</span>,<span class="op">-</span><span class="fl">5.201</span>],[<span class="op">-</span><span class="fl">1.805</span>,<span class="op">-</span><span class="fl">5.043</span>],[<span class="op">-</span><span class="fl">2.149</span>,<span class="op">-</span><span class="fl">1.782</span>],[<span class="fl">0.486</span>,<span class="fl">1.523</span>],[<span class="fl">2.796</span>,<span class="fl">3.460</span>],[<span class="fl">0.814</span>,<span class="fl">0.354</span>],[<span class="op">-</span><span class="fl">1.004</span>,<span class="op">-</span><span class="fl">3.153</span>],[<span class="fl">2.803</span>,<span class="fl">3.223</span>],[<span class="fl">1.253</span>,<span class="fl">1.898</span>],[<span class="fl">0.512</span>,<span class="fl">0.986</span>],[<span class="op">-</span><span class="fl">1.541</span>,<span class="fl">1.040</span>],[<span class="op">-</span><span class="fl">1.611</span>,<span class="fl">0.366</span>],[<span class="op">-</span><span class="fl">0.376</span>,<span class="op">-</span><span class="fl">0.378</span>],[<span class="fl">2.057</span>,<span class="fl">0.716</span>],[<span class="op">-</span><span class="fl">1.308</span>,<span class="op">-</span><span class="fl">2.566</span>],[<span class="fl">0.774</span>,<span class="fl">2.805</span>],[<span class="op">-</span><span class="fl">0.154</span>,<span class="fl">0.801</span>],[<span class="fl">0.932</span>,<span class="fl">0.937</span>],[<span class="fl">0.447</span>,<span class="fl">2.937</span>],[<span class="fl">1.623</span>,<span class="fl">3.251</span>],[<span class="op">-</span><span class="fl">0.522</span>,<span class="op">-</span><span class="fl">0.423</span>],[<span class="op">-</span><span class="fl">0.022</span>,<span class="fl">0.747</span>],[<span class="op">-</span><span class="fl">0.419</span>,<span class="op">-</span><span class="fl">2.039</span>],[<span class="fl">0.978</span>,<span class="fl">1.541</span>],[<span class="fl">1.116</span>,<span class="fl">1.625</span>],[<span class="op">-</span><span class="fl">0.135</span>,<span class="fl">2.210</span>],[<span class="op">-</span><span class="fl">0.111</span>,<span class="op">-</span><span class="fl">2.899</span>],[<span class="op">-</span><span class="fl">1.254</span>,<span class="op">-</span><span class="fl">0.457</span>],[<span class="fl">0.078</span>,<span class="op">-</span><span class="fl">0.065</span>],[<span class="op">-</span><span class="fl">0.214</span>,<span class="op">-</span><span class="fl">1.166</span>],[<span class="op">-</span><span class="fl">2.644</span>,<span class="op">-</span><span class="fl">2.199</span>],[<span class="fl">1.780</span>,<span class="fl">3.323</span>],[<span class="fl">1.521</span>,<span class="fl">2.152</span>],[<span class="op">-</span><span class="fl">0.276</span>,<span class="op">-</span><span class="fl">0.841</span>],[<span class="op">-</span><span class="fl">2.044</span>,<span class="op">-</span><span class="fl">3.149</span>],[<span class="fl">1.273</span>,<span class="fl">3.503</span>],[<span class="op">-</span><span class="fl">0.702</span>,<span class="op">-</span><span class="fl">1.164</span>],[<span class="fl">0.575</span>,<span class="fl">1.964</span>],[<span class="fl">0.465</span>,<span class="fl">1.100</span>],[<span class="fl">1.454</span>,<span class="fl">1.448</span>]])</span></code></pre></div>
<p>We computed the covariance matrix for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> and found its eigenvectors. You can see the data and the two eigenvectors here:</p>
<ul>
<li><strong>Example:</strong> <a href="https://www.desmos.com/calculator/8eatvepc4i" class="uri">https://www.desmos.com/calculator/8eatvepc4i</a></li>
</ul>
<p>The red eigenvector points in the direction of maximum variability. The other eigenvector is perpendicular to the red one. The eigenvectors in principal component analysis are always mutually orthogonal (perpendicular to each other).</p>
<p>Here is another example in 3-dimensions:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a>np.array([[<span class="fl">0.539</span>,<span class="op">-</span><span class="fl">2.63</span>,<span class="fl">0.306</span>],[<span class="fl">0.248</span>,<span class="op">-</span><span class="fl">4.35</span>,<span class="op">-</span><span class="fl">0.841</span>],[<span class="op">-</span><span class="fl">1.89</span>,<span class="fl">3.34</span>,<span class="op">-</span><span class="fl">1.08</span>],[<span class="fl">1.28</span>,<span class="op">-</span><span class="fl">3.35</span>,<span class="fl">2.06</span>],[<span class="fl">0.52</span>,<span class="op">-</span><span class="fl">2.26</span>,<span class="fl">0.0225</span>],[<span class="op">-</span><span class="fl">1.59</span>,<span class="op">-</span><span class="fl">1.36</span>,<span class="op">-</span><span class="fl">1.37</span>],[<span class="op">-</span><span class="fl">0.476</span>,<span class="fl">1.98</span>,<span class="fl">0.624</span>],[<span class="fl">0.36</span>,<span class="op">-</span><span class="fl">1.04</span>,<span class="op">-</span><span class="fl">0.451</span>],[<span class="op">-</span><span class="fl">0.803</span>,<span class="op">-</span><span class="fl">1.79</span>,<span class="op">-</span><span class="fl">1.14</span>],[<span class="fl">0.152</span>,<span class="op">-</span><span class="fl">0.489</span>,<span class="fl">1.27</span>],[<span class="op">-</span><span class="fl">0.781</span>,<span class="fl">2.79</span>,<span class="fl">0.648</span>],[<span class="op">-</span><span class="fl">0.436</span>,<span class="op">-</span><span class="fl">0.203</span>,<span class="op">-</span><span class="fl">0.211</span>],[<span class="op">-</span><span class="fl">0.988</span>,<span class="op">-</span><span class="fl">0.888</span>,<span class="op">-</span><span class="fl">1.64</span>],[<span class="fl">0.363</span>,<span class="op">-</span><span class="fl">1.42</span>,<span class="fl">0.238</span>],[<span class="op">-</span><span class="fl">1.02</span>,<span class="fl">6.26</span>,<span class="fl">0.07</span>],[<span class="fl">1.47</span>,<span class="op">-</span><span class="fl">1.02</span>,<span class="fl">0.501</span>],[<span class="fl">0.608</span>,<span class="op">-</span><span class="fl">5.83</span>,<span class="fl">0.195</span>],[<span class="fl">1.07</span>,<span class="fl">1.51</span>,<span class="fl">1.54</span>],[<span class="fl">0.684</span>,<span class="op">-</span><span class="fl">0.659</span>,<span class="fl">0.831</span>],[<span class="op">-</span><span class="fl">1.31</span>,<span class="fl">0.964</span>,<span class="op">-</span><span class="fl">1.09</span>],[<span class="fl">0.127</span>,<span class="fl">0.855</span>,<span class="op">-</span><span class="fl">0.568</span>],[<span class="fl">1.94</span>,<span class="op">-</span><span class="fl">3.46</span>,<span class="fl">0.942</span>],[<span class="fl">0.319</span>,<span class="op">-</span><span class="fl">6.04</span>,<span class="fl">0.489</span>],[<span class="op">-</span><span class="fl">0.624</span>,<span class="op">-</span><span class="fl">1.19</span>,<span class="op">-</span><span class="fl">1.68</span>],[<span class="fl">0.317</span>,<span class="fl">0.415</span>,<span class="fl">2.18</span>],[<span class="fl">0.901</span>,<span class="op">-</span><span class="fl">2.64</span>,<span class="fl">0.241</span>],[<span class="op">-</span><span class="fl">0.816</span>,<span class="op">-</span><span class="fl">0.55</span>,<span class="op">-</span><span class="fl">1.13</span>],[<span class="op">-</span><span class="fl">0.372</span>,<span class="fl">1.24</span>,<span class="fl">0.143</span>],[<span class="op">-</span><span class="fl">0.409</span>,<span class="fl">6.93</span>,<span class="fl">1.3</span>],[<span class="fl">1.64</span>,<span class="op">-</span><span class="fl">7.51</span>,<span class="fl">0.545</span>],[<span class="op">-</span><span class="fl">0.833</span>,<span class="fl">3.05</span>,<span class="fl">0.776</span>],[<span class="op">-</span><span class="fl">0.451</span>,<span class="fl">0.0903</span>,<span class="op">-</span><span class="fl">0.00596</span>],[<span class="op">-</span><span class="fl">0.29</span>,<span class="op">-</span><span class="fl">0.529</span>,<span class="op">-</span><span class="fl">2.08</span>],[<span class="op">-</span><span class="fl">1.19</span>,<span class="op">-</span><span class="fl">0.957</span>,<span class="op">-</span><span class="fl">0.451</span>],[<span class="op">-</span><span class="fl">0.573</span>,<span class="fl">0.468</span>,<span class="op">-</span><span class="fl">0.416</span>],[<span class="op">-</span><span class="fl">1.32</span>,<span class="fl">1.0</span>,<span class="op">-</span><span class="fl">1.17</span>],[<span class="fl">0.213</span>,<span class="op">-</span><span class="fl">1.3</span>,<span class="fl">0.184</span>],[<span class="fl">1.9</span>,<span class="op">-</span><span class="fl">0.14</span>,<span class="fl">1.73</span>],[<span class="op">-</span><span class="fl">0.917</span>,<span class="fl">2.94</span>,<span class="fl">0.343</span>],[<span class="fl">1.44</span>,<span class="op">-</span><span class="fl">4.42</span>,<span class="fl">1.08</span>],[<span class="fl">0.315</span>,<span class="fl">2.86</span>,<span class="fl">0.894</span>],[<span class="op">-</span><span class="fl">0.0783</span>,<span class="op">-</span><span class="fl">0.0105</span>,<span class="op">-</span><span class="fl">0.857</span>],[<span class="fl">1.01</span>,<span class="op">-</span><span class="fl">1.97</span>,<span class="fl">1.24</span>],[<span class="fl">0.213</span>,<span class="op">-</span><span class="fl">0.708</span>,<span class="fl">0.198</span>],[<span class="op">-</span><span class="fl">1.84</span>,<span class="fl">2.11</span>,<span class="op">-</span><span class="fl">1.52</span>],[<span class="op">-</span><span class="fl">0.334</span>,<span class="fl">0.247</span>,<span class="op">-</span><span class="fl">0.595</span>],[<span class="fl">0.251</span>,<span class="fl">0.332</span>,<span class="fl">0.219</span>],[<span class="op">-</span><span class="fl">0.00107</span>,<span class="fl">0.589</span>,<span class="op">-</span><span class="fl">0.0736</span>],[<span class="op">-</span><span class="fl">0.825</span>,<span class="op">-</span><span class="fl">3.06</span>,<span class="op">-</span><span class="fl">1.29</span>],[<span class="op">-</span><span class="fl">1.46</span>,<span class="fl">2.57</span>,<span class="fl">0.355</span>],[<span class="fl">0.513</span>,<span class="op">-</span><span class="fl">1.9</span>,<span class="op">-</span><span class="fl">1.27</span>],[<span class="fl">0.977</span>,<span class="op">-</span><span class="fl">1.1</span>,<span class="op">-</span><span class="fl">0.0922</span>],[<span class="op">-</span><span class="fl">0.303</span>,<span class="op">-</span><span class="fl">3.36</span>,<span class="op">-</span><span class="fl">1.0</span>],[<span class="fl">0.233</span>,<span class="fl">2.39</span>,<span class="fl">1.45</span>],[<span class="op">-</span><span class="fl">2.11</span>,<span class="op">-</span><span class="fl">0.0848</span>,<span class="op">-</span><span class="fl">1.83</span>],[<span class="op">-</span><span class="fl">0.524</span>,<span class="fl">2.54</span>,<span class="fl">0.0703</span>],[<span class="op">-</span><span class="fl">0.457</span>,<span class="op">-</span><span class="fl">1.7</span>,<span class="fl">0.445</span>],[<span class="fl">0.295</span>,<span class="op">-</span><span class="fl">0.419</span>,<span class="fl">0.544</span>],[<span class="op">-</span><span class="fl">2.22</span>,<span class="fl">2.88</span>,<span class="op">-</span><span class="fl">2.25</span>],[<span class="fl">0.288</span>,<span class="op">-</span><span class="fl">0.0653</span>,<span class="fl">0.415</span>],[<span class="op">-</span><span class="fl">0.766</span>,<span class="fl">4.33</span>,<span class="fl">0.612</span>],[<span class="op">-</span><span class="fl">0.92</span>,<span class="fl">1.23</span>,<span class="op">-</span><span class="fl">0.399</span>],[<span class="op">-</span><span class="fl">0.0367</span>,<span class="fl">2.67</span>,<span class="fl">1.83</span>],[<span class="op">-</span><span class="fl">1.12</span>,<span class="op">-</span><span class="fl">1.58</span>,<span class="op">-</span><span class="fl">2.9</span>],[<span class="fl">0.78</span>,<span class="fl">0.755</span>,<span class="fl">0.408</span>],[<span class="op">-</span><span class="fl">1.68</span>,<span class="fl">7.15</span>,<span class="op">-</span><span class="fl">1.11</span>],[<span class="op">-</span><span class="fl">1.85</span>,<span class="fl">4.37</span>,<span class="op">-</span><span class="fl">0.954</span>],[<span class="fl">0.659</span>,<span class="op">-</span><span class="fl">1.21</span>,<span class="op">-</span><span class="fl">0.563</span>],[<span class="fl">0.378</span>,<span class="op">-</span><span class="fl">3.37</span>,<span class="op">-</span><span class="fl">0.126</span>],[<span class="fl">0.507</span>,<span class="op">-</span><span class="fl">0.321</span>,<span class="op">-</span><span class="fl">0.00401</span>],[<span class="op">-</span><span class="fl">0.226</span>,<span class="op">-</span><span class="fl">0.644</span>,<span class="op">-</span><span class="fl">0.629</span>],[<span class="op">-</span><span class="fl">0.712</span>,<span class="fl">2.01</span>,<span class="op">-</span><span class="fl">1.34</span>],[<span class="op">-</span><span class="fl">1.15</span>,<span class="op">-</span><span class="fl">2.24</span>,<span class="op">-</span><span class="fl">1.62</span>],[<span class="fl">0.778</span>,<span class="fl">1.95</span>,<span class="fl">0.365</span>],[<span class="op">-</span><span class="fl">0.227</span>,<span class="op">-</span><span class="fl">0.0463</span>,<span class="op">-</span><span class="fl">0.116</span>],[<span class="fl">0.155</span>,<span class="fl">0.0193</span>,<span class="fl">0.222</span>],[<span class="fl">0.317</span>,<span class="fl">0.685</span>,<span class="op">-</span><span class="fl">0.648</span>],[<span class="op">-</span><span class="fl">1.14</span>,<span class="fl">0.106</span>,<span class="op">-</span><span class="fl">1.49</span>],[<span class="op">-</span><span class="fl">0.582</span>,<span class="op">-</span><span class="fl">0.625</span>,<span class="op">-</span><span class="fl">1.36</span>],[<span class="op">-</span><span class="fl">1.3</span>,<span class="fl">1.36</span>,<span class="fl">0.0372</span>],[<span class="op">-</span><span class="fl">1.03</span>,<span class="op">-</span><span class="fl">0.818</span>,<span class="op">-</span><span class="fl">0.353</span>],[<span class="op">-</span><span class="fl">0.375</span>,<span class="fl">1.96</span>,<span class="fl">0.0616</span>],[<span class="op">-</span><span class="fl">0.408</span>,<span class="fl">3.1</span>,<span class="fl">0.983</span>],[<span class="fl">0.655</span>,<span class="fl">0.214</span>,<span class="fl">1.26</span>],[<span class="op">-</span><span class="fl">0.2</span>,<span class="op">-</span><span class="fl">0.418</span>,<span class="op">-</span><span class="fl">1.22</span>],[<span class="fl">0.137</span>,<span class="fl">0.836</span>,<span class="fl">0.523</span>],[<span class="op">-</span><span class="fl">2.18</span>,<span class="fl">8.95</span>,<span class="fl">0.156</span>],[<span class="op">-</span><span class="fl">0.449</span>,<span class="op">-</span><span class="fl">2.32</span>,<span class="op">-</span><span class="fl">2.0</span>],[<span class="op">-</span><span class="fl">0.398</span>,<span class="op">-</span><span class="fl">1.78</span>,<span class="op">-</span><span class="fl">0.755</span>],[<span class="fl">0.886</span>,<span class="op">-</span><span class="fl">0.523</span>,<span class="fl">2.47</span>],[<span class="fl">0.56</span>,<span class="op">-</span><span class="fl">4.45</span>,<span class="fl">0.234</span>],[<span class="fl">1.26</span>,<span class="fl">0.946</span>,<span class="fl">2.63</span>],[<span class="fl">0.807</span>,<span class="op">-</span><span class="fl">1.8</span>,<span class="op">-</span><span class="fl">0.74</span>],[<span class="op">-</span><span class="fl">0.341</span>,<span class="fl">3.25</span>,<span class="op">-</span><span class="fl">0.207</span>],[<span class="fl">1.96</span>,<span class="op">-</span><span class="fl">3.78</span>,<span class="fl">0.472</span>],[<span class="op">-</span><span class="fl">0.386</span>,<span class="op">-</span><span class="fl">0.14</span>,<span class="op">-</span><span class="fl">0.602</span>],[<span class="op">-</span><span class="fl">0.767</span>,<span class="fl">4.67</span>,<span class="op">-</span><span class="fl">1.14</span>],[<span class="fl">0.685</span>,<span class="fl">0.0478</span>,<span class="fl">1.29</span>],[<span class="op">-</span><span class="fl">1.14</span>,<span class="op">-</span><span class="fl">0.0269</span>,<span class="op">-</span><span class="fl">2.7</span>],[<span class="fl">0.494</span>,<span class="op">-</span><span class="fl">1.73</span>,<span class="op">-</span><span class="fl">0.111</span>]])</span></code></pre></div>
<ul>
<li><strong>Example:</strong> <a href="https://www.desmos.com/3d/4e6e49f03d" class="uri">https://www.desmos.com/3d/4e6e49f03d</a></li>
</ul>
<p>You should understand the following key ideas about principal component analysis.</p>
<ol type="1">
<li><p>The dominant principal component (the one with the largest eigenvalue) points in the direction of greatest variability in the data.</p></li>
<li><p>The principal components are all perpendicular (orthogonal) to each other.</p></li>
<li><p>You can use the principal components as an alternative coordinate system where it is easier to see which coordinates are the most important.</p></li>
</ol>
<p>We finished today by introducing the <strong>k-nearest neighbors algorithm</strong>. This is a supervised learning algorithm used to predict how to label a new data point based on the labels for a collection of training data.</p>
<div class="Theorem">
<p><strong>k-Nearest Neighbors Algorithm.</strong> Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and an set of labeled vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_n</annotation></semantics></math>, apply the following steps to predict how to classify a new point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math></p>
<ol type="1">
<li><p>Find the nearest <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> elements of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{x_1, \ldots, x_n\}</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p></li>
<li><p>If the majority of those <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> elements have the same label, the that is the predicted label for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p></li>
</ol>
</div>
<p>There are lots of variants of this algorithm to decide what to do if there is not a clear majority. One option is to do a weighted k-nearest neighbors algorithm where the vote to decided the majority of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> neighbors is weighted so that the votes of points that a closer to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> count for more.</p>
<p>The main disadvantage of the k-nearest neighbors algorithm is that it does not work well in higher dimensions because of the curse of dimensionality.</p>
<h3 id="fri-mar-29">Fri, Mar 29</h3>
<p>Today we did a workshop combining dimension reduction (with principal component analysis) and the k-nearest neighbors algorithm.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/DimensionReduction.pdf">Dimension reduction</a></li>
</ul>
<p>To get started, we used this code to load the data and do the principal component analysis.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="co"># Load the data</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a>mnist <span class="op">=</span> tf.keras.datasets.mnist</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a>x_train, x_test <span class="op">=</span> x_train <span class="op">/</span> <span class="fl">255.0</span>, x_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true"></a><span class="co"># Find the principal components</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true"></a>X <span class="op">=</span> np.array([image.flatten() <span class="cf">for</span> image <span class="kw">in</span> x_train])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true"></a>Q <span class="op">=</span> np.cov(X.T)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true"></a>d, W <span class="op">=</span> np.linalg.eigh(Q)</span></code></pre></div>
<hr />
<h3 id="week-11-notes">Week 11 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 1</td>
<td style="text-align: left;"><em>no class</em></td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 3</td>
<td style="text-align: left;">Introduction to Markov decision processes</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 5</td>
<td style="text-align: left;">Markov chains with rewards</td>
</tr>
</tbody>
</table>
<h3 id="wed-apr-3">Wed, Apr 3</h3>
<p>Today we introduced <strong>Markov decision processes</strong> (MDPs). MDPs are the theoretical foundation of a third type of machine learning called <strong>reinforcement learning</strong> which is different from the other two types we’ve studied before (supervised and unsupervised learning).</p>
<p>A Markov descision process has four parts:</p>
<ol type="1">
<li><strong>States.</strong> A finite number of states.</li>
<li><strong>Rewards.</strong> Each state has a bonus (or penalty) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> that you earn at the start of the round if you are in that state.</li>
<li><strong>Actions.</strong> In each state, there is a set of actions you can choose.</li>
<li><strong>Transition probabilities.</strong> Depending on the state you are currently in and the action you choose, there is a probability distribution that determines what your next state will be.</li>
</ol>
<p>We looked at the following very simple example of an MDP where there are three states (which you can think of as safe, risky, and dead). The safe state has a reward of 1 point per round, while the risky state earns 2 points per round. The dead state is absorbing, once you get there, you don’t get any more actions and your reward is zero from that point on. In the other two states, you can choose either the red or the blue action. The blue action corresponds to trying to play it safe, while the red action corresponds to preferring the risky state. The transition probabilities corresponding to the red and blue actions are shown in the graph below.</p>
<center>
<img src="MarkovDescisionProcess.png" width=293></img>
</center>
<p>We started with this question:</p>
<ol type="1">
<li>What if you start in the risky state and always pick the red action?</li>
</ol>
<p>If you always pick the same action, then the MDP is just a Markov chain with rewards. And always picking the red action results in a very simple Markov chain, so we were able to calculate the theoretical average reward. Depending on how long we are able to stay in the risky state before falling into the dead state, we have the following table of possible outcomes with their corresponding probabilities.</p>
<center>
<table class="bordered">
<tr>
<td>
Outcome (Total Reward)
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mn>2</mn><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">~~~~2~~~~</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>6</mn><annotation encoding="application/x-tex">6</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>…</mi><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">~~~~\ldots~~~~</annotation></semantics></math>
</td>
</tr>
<tr>
<td>
Probability
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mn>0.1</mn><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">~~~~0.1~~~~</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>0.1</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><mn>0.9</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0.1)(0.9)</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>0.1</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msup><mn>0.9</mn><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0.1)(0.9^2)</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>0.1</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msup><mn>0.9</mn><mn>3</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0.1)(0.9^3)</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>…</mi><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">~~~~\ldots~~~~</annotation></semantics></math>
</td>
</tr>
</table>
</center>
<p>When an outcome like the total reward is determined randomly by a probability distribution, the <strong>expected value</strong> (also known as the <strong>theoretical average</strong>) is the weighted average of the possible outcomes with the probabilities as the weights. The possible outcomes are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2n + 2</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is the number of times you return to the risky state (before falling in the hole and ending up dead). The probabilities of those outcomes are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mn>0.1</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msup><mn>0.9</mn><mi>n</mi></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0.1)(0.9^n)</annotation></semantics></math>. So the expected reward is an infinite sum which we denoted with the letter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Expected value</mtext><mo>=</mo><mi>S</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mo accent="false">∞</mo></munderover><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><mn>0.1</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><msup><mn>0.9</mn><mi>n</mi></msup><mo stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{Expected value} = S = \sum_{n = 0}^\infty (2n+2) (0.1) (0.9^n).</annotation></semantics></math></p>
<p>You could use a computer and add up the first few thousand terms of this series to approximate the sum, but we used algebra to work out the following recursive formula for the sum: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>2</mn><mo>+</mo><mn>0.9</mn><mi>S</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">S = 2 + 0.9 S.</annotation></semantics></math> This recursive formula makes intuitive sense if you think about it, since the expected reward after the current round is the 2 points we earn for the current round plus a 10% chance of getting nothing (if we die) or a 90% chance of starting over again at the risky state with an average future reward of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>.</p>
<ol start="2" type="1">
<li>Solve the equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>2</mn><mo>+</mo><mn>0.9</mn><mi>S</mi></mrow><annotation encoding="application/x-tex">S = 2 + 0.9 S</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>.<br />

<details>
<summary>Solution.</summary> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>20</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">S = 20.</annotation></semantics></math>
</details></li>
</ol>
<p>Next time we will tackle the more complicated question of what happens if you always pick the blue action. It turns out we will be able to find a similar recursive formula to get the answer.</p>
<h3 id="fri-apr-5">Fri, Apr 5</h3>
<p>Today we continued the example we started on Wednesday. This time we asked what is your theoretical average total reward if you start in any state and always choose the blue action. Notice that if you always choose the blue action, then the transition matrix is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>0.9</mn></mtd><mtd columnalign="center"><mn>0.1</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>0.9</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mn>0.1</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mn>1</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">Q = \begin{pmatrix} 0.9 &amp; 0.1 &amp; 0 \\ 0.9 &amp; 0 &amp; 0.1 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}.</annotation></semantics></math></p>
<ul>
<li>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math> be the expect value of the total reward if we start in the safe state.<br />
</li>
<li>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>2</mn></msub><annotation encoding="application/x-tex">v_2</annotation></semantics></math> be the expected total reward starting in the risky state.</li>
<li>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>3</mn></msub><annotation encoding="application/x-tex">v_3</annotation></semantics></math> be the expected total reward for the dead state (obviously <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>3</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">v_3 = 0</annotation></semantics></math>).</li>
</ul>
<p>These three numbers are the entries the expected value vector, which we’ll just denote by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>. We can set up a recursive formula for the value vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> much like we did for the expected value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> last time. It is a little more complicated, since it is a system of equations:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right"><msub><mi>v</mi><mn>1</mn></msub></mtd><mtd columnalign="left"><mo>=</mo><mn>1</mn><mo>+</mo><mn>0.9</mn><msub><mi>v</mi><mn>1</mn></msub><mo>+</mo><mn>0.1</mn><msub><mi>v</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="right"><msub><mi>v</mi><mn>2</mn></msub></mtd><mtd columnalign="left"><mo>=</mo><mn>2</mn><mo>+</mo><mn>0.9</mn><msub><mi>v</mi><mn>1</mn></msub><mo>+</mo><mn>0.1</mn><msub><mi>v</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd columnalign="right"><msub><mi>v</mi><mn>3</mn></msub></mtd><mtd columnalign="left"><mo>=</mo><mn>0</mn><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
v_1 &amp;= 1 + 0.9 v_1 + 0.1 v_2 \\
v_2 &amp;= 2 + 0.9 v_1 + 0.1 v_3 \\
v_3 &amp;= 0.                    \\
\end{align*}</annotation></semantics></math></p>
<p>This makes sense because the expected value for state 1 (the safe state) is 1 (for the reward you earn this round) plus a 90% chance that you will end back in state 1 and get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math> again plus a 10% chance that you will start the next round in state 2 (the risky state) where you have an expected value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>2</mn></msub><annotation encoding="application/x-tex">v_2</annotation></semantics></math>.</p>
<p>We can re-write this system of equations using the transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> and a reward vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>2</mn></mtd></mtr><mtr><mtd columnalign="center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R = \begin{pmatrix} 1 \\ 2 \\ 0 \end{pmatrix}</annotation></semantics></math>. Then the recursive formula is:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>R</mi><mo>+</mo><mi>Q</mi><mi>v</mi><mi>.</mi></mrow><annotation encoding="application/x-tex"> v = R + Qv.</annotation></semantics></math></p>
<p>You can solve this equation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> using linear algebra (row reduction), but there is a better way. The key idea is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>+</mo><mi>Q</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">R + Qv</annotation></semantics></math> is a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> which we can call <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>v</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(v)</annotation></semantics></math>. We are looking for a fixed point of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>. We can use a technique called <strong>fixed point iteration</strong> or <strong>value iteration.</strong></p>
<div class="Theorem">
<p><strong>Value Iteration Algorithm.</strong> Input a transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>, a reward vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>, and an accuracy level <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>.</p>
<ol type="1">
<li>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> be the all zero vector.</li>
<li>Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>v</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(v)</annotation></semantics></math>.<br />
</li>
<li>If the distance from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>v</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(v)</annotation></semantics></math> is less than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>, then return <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>v</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(v)</annotation></semantics></math>.</li>
<li>Otherwise replace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>v</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(v)</annotation></semantics></math> and repeat steps 2 through 4.</li>
</ol>
</div>
<p>The following theorem tells us when this algorithm will converge for a Markov chain with rewards.</p>
<div class="Theorem">
<p><strong>Theorem.</strong> If we have a Markov chain with transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> and reward vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">R_i = 0</annotation></semantics></math> in every state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> that is in a final class of the Markov chain, then the value iteration algorithm will converge to the value vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>.</p>
</div>
<p>We implemented the value iteration algorithm in Python and used it to find the value vector for the blue action in the MDP from last time. Then we also did the following example.</p>
<ol start="2" type="1">
<li>For the Markov chain below, how long on average does it take to get from state 1 to state 6?</li>
</ol>
<center>
<img src="MarkovChain.png" width = 400></img>
</center>
<hr />
<h3 id="week-12-notes">Week 12 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 8</td>
<td style="text-align: left;">Markov chains with rewards - con’d</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 10</td>
<td style="text-align: left;">Review</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 12</td>
<td style="text-align: left;"><strong>Midterm 2</strong></td>
</tr>
</tbody>
</table>
<h3 id="mon-apr-8">Mon, Apr 8</h3>
<p>Today we did this workshop in class:</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/MarkovChainsRewards.pdf">Markov chains with rewards</a></li>
</ul>
<p>Be sure to also look at the <a href="midterm2review.html">study guide</a> for the midterm on Friday.</p>
<h3 id="wed-apr-10">Wed, Apr 10</h3>
<p>Today we did an in class review for Friday’s midterm. We looked at some of the questions from the study guide. We also talked about neural networks and did the following examples.</p>
<ol type="1">
<li><p>Draw a computation graph for the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mn>5</mn><mi>x</mi><mi>y</mi><mo>+</mo><mn>4</mn><mi>x</mi><mi>z</mi><mo>+</mo><mn>3</mn><mi>y</mi><mi>z</mi></mrow><annotation encoding="application/x-tex">f(x,y,z) = 5xy + 4xz + 3yz</annotation></semantics></math>.</p></li>
<li><p>Use the backpropagation algorithm to find the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(x,y,z)</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mo>−</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(-1,1,2)</annotation></semantics></math>.</p></li>
<li><p>Suppose a neural network inputs 40-by-40 pixel grayscale images, has two hidden layers with 100 nodes each, and has an output layer with 10 nodes. How many real number parameters does this neural network have?</p></li>
<li><p>Why does each layer of a neural network combine an affine linear map <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>↦</mo><mi>W</mi><mi>v</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">v \mapsto Wv + b</annotation></semantics></math> with a nonlinear activation function?</p></li>
</ol>
<hr />
<h3 id="week-13-notes">Week 13 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 15</td>
<td style="text-align: left;">Markov decision processes</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 17</td>
<td style="text-align: left;">Optimal policies</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 19</td>
<td style="text-align: left;">Policy iteration algorithm</td>
</tr>
</tbody>
</table>
<h3 id="mon-apr-15">Mon, Apr 15</h3>
<p>Today we looked at this example of a Markov decision process.</p>
<center>
<table class="bordered">
<tr>
<td width="72" height="72">
 
</td>
<td width="72">
 
</td>
<td width="72">
 
</td>
<td width="72" style="background-color: darkgreen">
 
</td>
</tr>
<tr>
<td height="72">
 
</td>
<td style="background-color: gray">
 
</td>
<td>
 
</td>
<td style="background-color:red">
 
</td>
</tr>
<tr>
<td height="72">
 
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
</tr>
</table>
</center>
<p>The states are the squares in the grid (except the gray square which cannot be entered). Each square has a reward of zero, except the green square which has a +1 reward and the red square which has a -1 reward. The green and red squares are also absorbing… once you reach them, the game ends (and you are stuck there forever).</p>
<p>The agent can move up, down, left, or right from any square, but must stay on the grid. If they try to move into an invalid square, then the stay at their current position. Furthermore, the agent isn’t in complete control of how they move. When they try to move in one direction, there is a 80% chance they will move one step in that direction, but also a 20% chance of moving perpendicular to the desired direction (equally likely to go left or right).</p>
<p>Let’s label the states with the numbers 0 through 10:</p>
<center>
<table class="bordered">
<tr>
<td width="72" height="72">
0
</td>
<td width="72">
1
</td>
<td width="72">
2
</td>
<td width="72" style="background-color: darkgreen">
3
</td>
</tr>
<tr>
<td height="72">
4
</td>
<td style="background-color: gray">
 
</td>
<td>
5
</td>
<td style="background-color:red">
6
</td>
</tr>
<tr>
<td height="72">
7
</td>
<td>
8
</td>
<td>
9
</td>
<td>
10
</td>
</tr>
</table>
</center>
<p>Before calculating the value for each state, we need to deal with one problem. Since the only states where we get any nonzero reward are absorbing states that you get stuck in forever, the value in the green state would be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">+\infty</annotation></semantics></math> and the value in the red state would be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math>. The way we deal with nonzero rewards when they are in states that are in final classes of a Markov chain or Markov decision process is to use <strong>discounting</strong>. The idea is to discount the value of future rewards in the value iteration equation by multiplying them by a <strong>discount factor</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 &lt; \gamma &lt; 1</annotation></semantics></math>. The smaller <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is, the more we will focus on immediate rewards, and the less we will care about the future. For Markov chains with rewards, the value iteration formula with discounting is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>R</mi><mo>+</mo><mi>γ</mi><mi>Q</mi><mi>v</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">v = R + \gamma Qv.</annotation></semantics></math></p>
<p>For Markov decision processes, where the agent can choose different actions at each state, the value iteration formula is a little more complicated. In general, if the set of actions available to the agent is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="script"><mi>𝒜</mi></mstyle><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math>, and for every <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow><annotation encoding="application/x-tex">i \in \mathcal{A}</annotation></semantics></math>, there is a transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Q_i</annotation></semantics></math> corresponding to that action, then the value iteration formula (which is known as the <strong>Bellman equation</strong> in this context) is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>R</mi><mo>+</mo><mi>γ</mi><munder><mo>max</mo><mrow><mi>i</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow></munder><mo stretchy="false" form="prefix">(</mo><msub><mi>Q</mi><mi>i</mi></msub><mi>v</mi><mo stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">v = R + \gamma \max_{i \in \mathcal{A}} (Q_i v),</annotation></semantics></math> where the maximum is taken entrywise over the different vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mn>1</mn></msub><mi>v</mi></mrow><annotation encoding="application/x-tex">Q_1 v</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mi>v</mi></mrow><annotation encoding="application/x-tex">Q_2 v</annotation></semantics></math>, etc.. Note that this is not the standard way to write the Bellman equations, but it is equivalent to those other notations, and it is particularly convenient if you can construct the full transition matrix for each action (which isn’t always easy or efficient!).</p>
<p>We wrote a Python program to find the value for each state of the MDP above with a discount factor of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\gamma = 0.9</annotation></semantics></math>. We got these results.</p>
<center>
<table class="bordered">
<tr>
<td width="72" height="72">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>6.45</mn><annotation encoding="application/x-tex">6.45</annotation></semantics></math>
</td>
<td width="72">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>7.44</mn><annotation encoding="application/x-tex">7.44</annotation></semantics></math>
</td>
<td width="72">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>8.48</mn><annotation encoding="application/x-tex">8.48</annotation></semantics></math>
</td>
<td width="72" style="background-color: darkgreen">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>
</td>
</tr>
<tr>
<td height="72">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>5.66</mn><annotation encoding="application/x-tex">5.66</annotation></semantics></math>
</td>
<td style="background-color: gray">
 
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>5.72</mn><annotation encoding="application/x-tex">5.72</annotation></semantics></math>
</td>
<td style="background-color:red">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">-10</annotation></semantics></math>
</td>
</tr>
<tr>
<td height="72">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4.91</mn><annotation encoding="application/x-tex">4.91</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4.31</mn><annotation encoding="application/x-tex">4.31</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4.75</mn><annotation encoding="application/x-tex">4.75</annotation></semantics></math>
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2.77</mn><annotation encoding="application/x-tex">2.77</annotation></semantics></math>
</td>
</tr>
</table>
</center>
<ul>
<li><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1r2nTzHykpe0LwegaddwfXazj53kAW5jb?usp=sharing">Markov Decision Processes 2</a></li>
</ul>
<h3 id="wed-apr-17">Wed, Apr 17</h3>
<p>Today we talked about finding optimal policies for an MDP. A <strong>policy</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> for a Markov decision process is a function that inputs states <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and outputs actions: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo>:</mo><mi>S</mi><mo>→</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow><annotation encoding="application/x-tex">\pi: S \rightarrow \mathcal{A}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> is the set of states and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="script"><mi>𝒜</mi></mstyle><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> is the set of actions.</p>
<p>Before we described how to find the optimal policy, we took a look at the Bellman equation again.</p>
<section id="bellman-equation" class="Theorem">
<h4>Bellman Equation</h4>
<p>The entries of the value vector are given by the following recursive formula</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>=</mo><msub><mi>R</mi><mi>i</mi></msub><mo>+</mo><mi>γ</mi><munder><mo>max</mo><mrow><mi>a</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow></munder><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>⋅</mo><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v_i = R_i + \gamma \max_{a \in \mathcal{A}} \left( q_i(a) \cdot v \right)</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> is the state, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is the value vector, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the discount factor, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="script"><mi>𝒜</mi></mstyle><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> is the set of possible actions, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">q_i(a)</annotation></semantics></math> is row <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> of the transition matrix if you choose action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>.</p>
</section>
<p>Once you know all of the entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding="application/x-tex">v_i</annotation></semantics></math> of the value vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, you can use them to choose the best action in each state. We looked at the example from Monday and figured out the best action in state 9 based on the values of the neighboring states. From that example, we derived the following formula for the optimal policy.</p>
<section id="optimal-policy-formula" class="Theorem">
<h4>Optimal Policy Formula</h4>
<p>Given the value vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, the optimal action at each state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> is given by the function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>argmax</mo><mrow><mi>a</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>⋅</mo><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\pi(i) = \operatorname{argmax}_{a \in \mathcal{A}} \left( q_i(a) \cdot v \right).</annotation></semantics></math></p>
</section>
<p>We programmed this function in Python for the example from last time. We also observed how the value vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> and the optimal policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> can chance if you increase or decrease the discount factor. For example, when you increase the discount factor, it makes the agent more concerned with future rewards and less concerned about the present. So the agent tends to pick actions that are more cautious.</p>
<h3 id="fri-apr-19">Fri, Apr 19</h3>
<p>Today we started by talking about an alternative to the value iteration algorithm called policy iteration. With policy iteration, you can (in theory) get both the exact value vector and the optimal policy for a Markov decision process in a finite number of steps. Here is the algorithm:</p>
<section id="policy-iteration-algorithm" class="Theorem">
<h4>Policy Iteration Algorithm</h4>
<ol type="1">
<li>Choose a random policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>.</li>
<li>Using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>, find the transition matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Q</mi><mi>π</mi></msub><annotation encoding="application/x-tex">Q_\pi</annotation></semantics></math> if the agent follows policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>.</li>
<li>Calculate the solution to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>R</mi><mo>+</mo><mi>γ</mi><msub><mi>Q</mi><mi>π</mi></msub><mi>v</mi></mrow><annotation encoding="application/x-tex">v = R + \gamma Q_\pi v</annotation></semantics></math>.</li>
<li>Use the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> to find a new policy <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> that is optimal for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>.<br />
</li>
<li>Repeat steps 2-4 until the policy stops changing.<br />
</li>
</ol>
</section>
<p>Since there are at most <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle><msup><mo stretchy="false" form="prefix">|</mo><mrow><mo stretchy="false" form="prefix">|</mo><mi>S</mi><mo stretchy="false" form="prefix">|</mo></mrow></msup></mrow><annotation encoding="application/x-tex">|\mathcal{A}|^{|S|}</annotation></semantics></math> possible policies, this algorithm is guarantee to find the correct solution after a finite number of steps. Unfortunately, step 3 is usually solved using value iteration, so the policy iteration algorithm usually isn’t any faster than value iteration.</p>
<p>After we talked about policy iteration, we talked about some of the practical issues with implementing MDPs in Python. One recommendation is to implement an MDP using a Python class:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a><span class="kw">class</span> MDP:</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, states, actions, rewards, transitionFunction, discount):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a>        <span class="va">self</span>.states <span class="op">=</span> states </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a>        <span class="va">self</span>.actions <span class="op">=</span> actions </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a>        <span class="va">self</span>.rewards <span class="op">=</span> rewards </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a>        <span class="va">self</span>.transitionFunction <span class="op">=</span> transitionFunction </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true"></a>        <span class="va">self</span>.discount <span class="op">=</span> discount </span></code></pre></div>
<p>We didn’t go into details, but we did talk about one issue with large MDPs. If there are a lot of states, then the transition probability function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(i,j,a)</annotation></semantics></math> which is the probability of moving from state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> to state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> if you take action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> is zero for most combinations of the input. Therefore, when you calculate the Bellman equation, most of the entries of the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">q_i(a)</annotation></semantics></math> are zeros. This is an example of a <strong>sparse</strong> vector (i.e., it has a lot of zero entries). It would save computer time if we didn’t keep track of all of those zeros.</p>
<p>Therefore, when we create a MDP object, we might want to express our transition function differently. We could create a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">T(i,a)</annotation></semantics></math> which computes for any state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> a list that contains tuples <code>(j, p)</code> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> is a possible future state and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is the probability to reach that state. For example in the grid world we could use code like this:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="co"># Example</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a>T(<span class="dv">9</span>,<span class="st">&quot;up&quot;</span>) <span class="co"># should output [(5, 0.8), (8, 0.1), (10, 0.1)]</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a><span class="co"># Bellman equation</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true"></a>new_v[i] <span class="op">=</span> R[i] <span class="op">+</span> discount <span class="op">*</span> <span class="bu">max</span>(np.dot([t[<span class="dv">1</span>] <span class="cf">for</span> t <span class="kw">in</span> T(i,a)], [v[t[<span class="dv">0</span>]] <span class="cf">for</span> t <span class="kw">in</span> T(i,a)]) <span class="cf">for</span> a <span class="kw">in</span> actions)</span></code></pre></div>
<hr />
<h3 id="week-14-notes">Week 14 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 22</td>
<td style="text-align: left;">Markov decision process examples</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 24</td>
<td style="text-align: left;">Q-learning algorithm</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 26</td>
<td style="text-align: left;">Q-learning continued</td>
</tr>
<tr class="even">
<td style="text-align: center;">Mon, Apr 29</td>
<td style="text-align: left;">Review</td>
</tr>
</tbody>
</table>
<h3 id="mon-apr-22">Mon, Apr 22</h3>
<p>Today we did the following workshop.</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/MDPs.pdf">Markov decision processes workshop</a></li>
</ul>
<p>I recommend using the following MDP class to solve each of the problems. Notice that the last problem requires you to add discount factors to the MDP class, since I did not include that in the original code.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true"></a><span class="kw">class</span> MDP:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,states,actions,rewardFunction,transitionFunction):</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true"></a>        <span class="va">self</span>.states <span class="op">=</span> states</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true"></a>        <span class="va">self</span>.actions <span class="op">=</span> actions</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true"></a>        <span class="va">self</span>.reward <span class="op">=</span> rewardFunction</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true"></a>        <span class="va">self</span>.transitionFunction <span class="op">=</span> transitionFunction</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true"></a>        <span class="va">self</span>.value <span class="op">=</span> <span class="va">self</span>.findValue() </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true"></a>        <span class="va">self</span>.optimalPolicy <span class="op">=</span> <span class="va">self</span>.findOptimalPolicy()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true"></a>    <span class="kw">def</span> transition(<span class="va">self</span>, s, a):</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true"></a>        <span class="cf">return</span> <span class="va">self</span>.transitionFunction(<span class="va">self</span>.states, s,a)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true"></a>    <span class="kw">def</span> qvalue(<span class="va">self</span>, s, a, v):</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true"></a>        outcomes <span class="op">=</span> <span class="va">self</span>.transition(s, a)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true"></a>        probabilities <span class="op">=</span> [pair[<span class="dv">1</span>] <span class="cf">for</span> pair <span class="kw">in</span> outcomes]</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true"></a>        nextStates <span class="op">=</span> [pair[<span class="dv">0</span>] <span class="cf">for</span> pair <span class="kw">in</span> outcomes]</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true"></a>        nextValues <span class="op">=</span> [v[s] <span class="cf">for</span> s <span class="kw">in</span> nextStates]</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true"></a>        <span class="cf">return</span> np.dot(probabilities,nextValues)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true"></a>    <span class="kw">def</span> bellman(<span class="va">self</span>, v):</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true"></a>        <span class="cf">return</span> {s:<span class="bu">max</span>(<span class="va">self</span>.reward(s) <span class="op">+</span> <span class="va">self</span>.qvalue(s, a, v) <span class="cf">for</span> a <span class="kw">in</span> <span class="va">self</span>.actions) <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.states}</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true"></a>    <span class="kw">def</span> findValue(<span class="va">self</span>):</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true"></a>        <span class="co"># Uses the value iteration algorithm</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true"></a>        v <span class="op">=</span> {s: <span class="dv">0</span> <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.states} </span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true"></a>        new_v <span class="op">=</span> <span class="va">self</span>.bellman(v)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true"></a>        <span class="cf">while</span> <span class="bu">sum</span>((new_v[s]<span class="op">-</span>v[s])<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.states) <span class="op">&gt;</span> <span class="dv">10</span><span class="op">**</span>(<span class="op">-</span><span class="dv">6</span>):</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true"></a>            v <span class="op">=</span> new_v</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true"></a>            new_v <span class="op">=</span> <span class="va">self</span>.bellman(v)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true"></a>        <span class="cf">return</span> new_v</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true"></a>    <span class="kw">def</span> findOptimalPolicy(<span class="va">self</span>):</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true"></a>        <span class="cf">return</span> {s:np.argmax([<span class="va">self</span>.qvalue(s, a, <span class="va">self</span>.value) <span class="cf">for</span> a <span class="kw">in</span> <span class="va">self</span>.actions]) <span class="cf">for</span> s <span class="kw">in</span> <span class="va">self</span>.states}</span></code></pre></div>
<p>When you are doing the workshop, make sure to pay close attention to the types of the inputs and outputs for each function:</p>
<ul>
<li><code>states</code> is a Python list of states (in most examples, each individual state will by a Python tuple).<br />
</li>
<li><code>actions</code> is a Python list of actions (the individual actions could be strings like “up” or numbers or anything).</li>
<li><code>rewardFunction</code> is a function that inputs states and outputs the number for the reward.</li>
<li><code>transitionFunction</code> needs three arguments:
<ol type="1">
<li>the list of states (<code>states</code>),</li>
<li>a state <code>s</code>, and</li>
<li>an action <code>a</code>.</li>
</ol>
It returns a list of tuples of the form: <code>[(s1, p1), (s2, p2), ...]</code> where the first entry of each tuple is a possible next state and the second entry is the probability of getting to that state.</li>
</ul>
<h3 id="wed-apr-24">Wed, Apr 24</h3>
<p>Today we introduced <strong>Q-learning</strong> which is one of the main algorithms used for reinforcement learning. The idea is similar to a Markov decision process, but there are two important differences:</p>
<ol type="1">
<li>The AI does not know what the rewards are for each state.</li>
<li>The AI does not know what the transition function is.</li>
</ol>
<p>Instead, our AI will have to learn the environment as it goes. At first it will choose actions at random and just see what happens. As it goes, it will store information about the expected values of state-action pairs in a table called a <strong>Q-table</strong></p>
<center>
<table class="bordered">
<tr>
<td>
 
</td>
<td>
action0
</td>
<td>
action1
</td>
<td>
action2
</td>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mi>…</mi><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace><mspace width="0.222em"></mspace></mrow><annotation encoding="application/x-tex">~~~~\ldots~~~~</annotation></semantics></math>
</td>
</tr>
<tr>
<td>
state0
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
</tr>
<tr>
<td>
state1
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
</tr>
<tr>
<td>
state2
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
</tr>
<tr>
<td>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⋮</mi><annotation encoding="application/x-tex">\vdots</annotation></semantics></math>
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
<td>
 
</td>
</tr>
</table>
</center>
<p>Initially, we set all of the entries of the Q-table to zero. Then, any time our agent starts in state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>, chooses action <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>, and ends up in state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">s&#39;</annotation></semantics></math>, we will update entry <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q(s,a)</annotation></semantics></math> in the Q-table using the <strong>q-learning update formula:</strong></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>α</mi><mo stretchy="false" form="prefix">(</mo><mi>R</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>γ</mi><munder><mo>max</mo><mrow><mi>a</mi><mi>′</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow></munder><mo stretchy="false" form="prefix">(</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mi>′</mi><mo>,</mo><mi>a</mi><mi>′</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">Q(s,a) = Q(s,a) + \alpha (R(s) + \gamma \max_{a&#39; \in \mathcal{A}} (Q(s&#39;,a&#39;)) - Q(s,a))</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is a <strong>learning rate</strong> between 0 and 1, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the discount factor (same as for an MDP). Note, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> is one of the final absorbing states, then there is no state <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">s&#39;</annotation></semantics></math> that comes next, so you can leave off the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><msub><mo>max</mo><mrow><mi>a</mi><mi>′</mi><mo>∈</mo><mstyle mathvariant="script"><mi>𝒜</mi></mstyle></mrow></msub><mo stretchy="false" form="prefix">(</mo><mi>Q</mi><mo stretchy="false" form="prefix">(</mo><mi>s</mi><mi>′</mi><mo>,</mo><mi>a</mi><mi>′</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\gamma \max_{a&#39; \in \mathcal{A}} (Q(s&#39;,a&#39;))</annotation></semantics></math> term in the formula.</p>
<p>The Q-learning algorithm combines this formula for updating the Q-table with a second rule about how our agent moves around the environment. We could just let our agent choose actions at random. Or we could ask it to try to find the best policy given its current Q-table. Instead we do a mix of both options. This is called an <strong>epsilon-greedy algorithm</strong> where we choose a constant <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> between 0 and 1, then in each state,</p>
<ol type="1">
<li>With probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>, the agent chooses an action at random,</li>
<li>Otherwise the agent chooses an action which has the highest current value on the Q-table for the given state.</li>
</ol>
<p>By mixing random actions with optimal actions, the AI can still perform reasonably well even as it learns the environment. When our agent reaches an end state, instead of staying there forever, we will reset the agent back to the start state. This is called an episode. Typically in Q-learning we simulate many episodes, to let the AI develop an accurate Q-table.</p>
<p>Notice that the agent never really learns what the transition function or the rewards are. It just learns the values in the Q-table. So it never really has a complete model of the game (Q-learning is called a <strong>model-free</strong> algorithm for this reason). But that is okay because our AI can figure out what to do from the Q-table, and the Q-table will converge to the correct table of values in the long run.</p>
<p>We finished by playing with the following code in class to see how a simple Q-learning example works.</p>
<ul>
<li><strong>Example:</strong> <a href="https://colab.research.google.com/drive/1klnugeoducR6eBKabieOmSt7XuIZwqmK?usp=sharing">Q-learning</a></li>
</ul>
<hr />
<p><br> <br> <br> <br> <br> <br> <br> <br></p>
</body>
</html>
