<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Numerical Analysis Notes</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://bclins.github.io/mockup.css" />
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  <style>
  :root {
    --header-color: #333; 
    --link-color: #00F; 
  }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Numerical Analysis Notes</h1>
</header>
<h2 id="math-342---spring-2026">Math 342 - Spring 2026</h2>
<!--
<ul class="nav">
  <li>[Class Notes](notes.html)</li>
  <li>[Schedule & Syllabus](index.html)</li>
  <li>[Software & Tables](http://people.hsc.edu/faculty-staff/blins/StatsTools/)</li>
</ul>
-->
<center>
Jump to: <a href="index.html">Math 342 homepage</a>, <a
href="#week-1-notes">Week 1</a>, <a href="#week-2-notes">Week 2</a>, <a
href="#week-3-notes">Week 3</a>, <a href="#week-4-notes">Week 4</a>, <a
href="#week-5-notes">Week 5</a>, <a href="#week-6-notes">Week 6</a>, <a
href="#week-7-notes">Week 7</a>, <a href="#week-8-notes">Week 8</a>, <a
href="#week-9-notes">Week 9</a>, <a href="#week-10-notes">Week 10</a>,
<a href="#week-11-notes">Week 11</a>, <a href="#week-12-notes">Week
12</a>, <a href="#week-13-notes">Week 13</a>, <a
href="#week-14-notes">Week 14</a>
</center>
<h3 id="week-1-notes">Week 1 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 12</td>
<td style="text-align: left;">Floating point arithmetic</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 14</td>
<td style="text-align: left;">Relative error, significant digits</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Jan 16</td>
<td style="text-align: left;">Taylorâ€™s theorem</td>
</tr>
</tbody>
</table>
<!--
#### Book Ideas:

* Driscol and Braun: <https://fncbook.com/> - Uses Python, and has brief coverage of most of the topics I want to cover and seems well written, but not super detailed. 
* Young and Mohlenkamp: [link](file:///Users/brian/Documents/Books/NumericalAnalysis/YoungMohlenkamp.pdf) - Even better fit for the topics I want to cover, but exclusively uses Matlab.  It has more good examples, though, and would make a great reference to add material into the course including vector-valued Newton's method, applications of eigenvectors ;
* Chasnov: <https://www.math.hkust.edu.hk/~machas/numerical-methods.pdf>More theoretical, but covers a lot of the topics that I want to cover.
* Kaw: <https://nm.mathforcollege.com/NumericalMethodsTextbookUnabridged/index.html>This one is interesting.  Seems low level (detailed coverage of really basic stuff), but that might be good.  Has okay coverage of most topics I want, but not QR decomposition or SVD. Does seem to have a lot of applications to engineering. 
* Linear Algebra w/ Applications to Engineering & AI: <https://nikolaimatni.github.io/ese-2030/>Nice book that might make a good supplement.


#### Notes from last time:

This was the second time I taught this class.  I followed the same general outline as the first time.  It worked okay, but I think that the course still has some problems.  I did spend more time on Taylor series and the Taylor remainder formula and I do not regret that.  Still, here are some things I would change. 

1. I need to spend more time on some basic concepts like:
    a. Triangle inequality (this crushed them on the Fixed Point Iteration workshop)
    b. Combining separate error terms (worst case total error -> triangle inequality again!)
    c. Orthogonality and Gram-Schmidt. There is no point covering Fourier series or Legendre polynomials if students don't understand how to find the components of a vector in a basis and why orthogonal bases are nice.  

2. I would cut out Gaussian quadrature.  It is a cool trick... but it takes too long to explain. A better use of time would be to introduce adaptive quadrature methods.  
-->
<h3 id="mon-jan-12">Mon, Jan 12</h3>
<p>We talked about how computers store <a
href="https://en.wikipedia.org/wiki/Floating_point">floating point
numbers</a>. Most modern programming languages store floating point
numbers using the <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE
754 standard</a>.</p>
<p><img src="IEEE_754.svg" style="width: 700px" /></p>
<p>In the IEEE 754 standard, a 64-bit floating point number has the
form</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ğ¯</mi><mi>ğš</mi><mi>ğ¥</mi><mi>ğ®</mi><mi>ğ</mi></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>ğ¬</mi><mi>ğ¢</mi><mi>ğ </mi><mi>ğ§</mi></mrow></msup><mspace width="0.167em"></mspace><mo>Ã—</mo><mspace width="0.167em"></mspace><msup><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mi>ğ</mi><mi>ğ±</mi><mi>ğ©</mi><mi>ğ¨</mi><mi>ğ§</mi><mi>ğ</mi><mi>ğ§</mi><mi>ğ­</mi></mrow><mo>âˆ’</mo><mn>1023</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mspace width="0.167em"></mspace><mo>Ã—</mo><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mrow><mi>ğŸ</mi><mi>ğ«</mi><mi>ğš</mi><mi>ğœ</mi><mi>ğ­</mi><mi>ğ¢</mi><mi>ğ¨</mi><mi>ğ§</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{value} = (-1)^\mathbf{sign} \, \times \, 2^{(\mathbf{exponent} - 1023)} \, \times \, (1 + \mathbf{fraction}) </annotation></semantics></math></p>
<p>where</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ¬</mi><mi>ğ¢</mi><mi>ğ </mi><mi>ğ§</mi></mrow><annotation encoding="application/x-tex">\mathbf{sign}</annotation></semantics></math>
is a single bit: 0 for positive, 1 for negative.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğŸ</mi><mi>ğ«</mi><mi>ğš</mi><mi>ğœ</mi><mi>ğ­</mi><mi>ğ¢</mi><mi>ğ¨</mi><mi>ğ§</mi></mrow><annotation encoding="application/x-tex">\mathbf{fraction}</annotation></semantics></math>
is a 52-bit fraction (in binary) with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>â‰¤</mo><mrow><mi>ğŸ</mi><mi>ğ«</mi><mi>ğš</mi><mi>ğœ</mi><mi>ğ­</mi><mi>ğ¢</mi><mi>ğ¨</mi><mi>ğ§</mi></mrow><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 \le \mathbf{fraction} &lt; 1</annotation></semantics></math>.</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ</mi><mi>ğ±</mi><mi>ğ©</mi><mi>ğ¨</mi><mi>ğ§</mi><mi>ğ</mi><mi>ğ§</mi><mi>ğ­</mi></mrow><annotation encoding="application/x-tex">\mathbf{exponent}</annotation></semantics></math>
is the 11-bit binary integer which ranges from 0 to 2047. Only 1 to 2046
are used for regular floating point numbers,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ğ</mi><mi>ğ±</mi><mi>ğ©</mi><mi>ğ¨</mi><mi>ğ§</mi><mi>ğ</mi><mi>ğ§</mi><mi>ğ­</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathbf{exponent}=0</annotation></semantics></math>
is reserved for zero and <a
href="https://en.wikipedia.org/wiki/Subnormal_number">subnormal
numbers</a>, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ğ</mi><mi>ğ±</mi><mi>ğ©</mi><mi>ğ¨</mi><mi>ğ§</mi><mi>ğ</mi><mi>ğ§</mi><mi>ğ­</mi></mrow><mo>=</mo><mn>2047</mn></mrow><annotation encoding="application/x-tex">\mathbf{exponent}=2047</annotation></semantics></math>
is reserved for infinity and NaN (â€œnot a numberâ€).</li>
</ul>
<p>We talked about how to convert between <a
href="https://en.wikipedia.org/wiki/Binary_numeral_system">binary
numbers</a> and decimal numbers.
<!--[scientific notation](https://en.wikipedia.org/wiki/Scientific_notation) and [logarithmic scales](https://en.wikipedia.org/wiki/Logarithmic_scale ).-->
This system works very well, but it leads to weird outcomes like
<code>0.1 + 0.1 + 0.1 = 0.30000000000000004</code>.</p>
<p>When you convert
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac displaystyle="false"><mn>1</mn><mn>10</mn></mfrac><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\tfrac{1}{10} = 0.1</annotation></semantics></math>
to binary, you get a infinitely repeating binary decimal:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac displaystyle="false"><mn>1</mn><mn>10</mn></mfrac><mo>=</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.000110011001100</mn><mi>â€¦</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\tfrac{1}{10} = (0.000110011001100\ldots)_2.</annotation></semantics></math>
So any finite binary representation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac displaystyle="false"><mn>1</mn><mn>10</mn></mfrac><annotation encoding="application/x-tex">\tfrac{1}{10}</annotation></semantics></math>
will have <strong>rounding errors</strong>.</p>
<p>We did the following examples in class:</p>
<ol type="1">
<li><p>Convert (10110)<sub>2</sub> to decimal. (<a
href="https://youtu.be/a2FpnU9Mm3E"
class="uri">https://youtu.be/a2FpnU9Mm3E</a>)</p></li>
<li><p>Convert 35 to binary. (<a href="https://youtu.be/gGiEu7QTi68"
class="uri">https://youtu.be/gGiEu7QTi68</a>)</p></li>
<li><p>What is the 64-bit string that represents the number 35 in the
IEEE standard?</p></li>
<li><p>What are the largest and smallest 64-bit floating point numbers
that can be stored?</p></li>
<li><p>In Python, compute <code>2.0**1024</code> and
<code>2**1024</code>. Why do you get different results?</p></li>
<li><p>In Python, compare <code>2.0**1024</code> with
<code>2.0**(-1024)</code> and <code>2.0**(-1070)</code>. What do you
notice?</p></li>
</ol>
<!--6. What number has mantissa (1011)<sub>2</sub> and exponent (110)<sub>2</sub>? -->
<h3 id="wed-jan-14">Wed, Jan 14</h3>
<p>Today we talked about significant digits. Here is a <a
href="https://youtu.be/l2yuDvwYq5g">quick video on how these
work</a>.</p>
<div class="Theorem">
<p><strong>Rules for Significant Digits.</strong></p>
<ol type="1">
<li><strong>Addition/Subtraction.</strong> The last common digit that is
significant for both numbers is the last significant digit of the
answer.</li>
<li><strong>Multiplication/Division.</strong> Result has significant
digits equal to the minimum number of significant digits of the two
inputs.</li>
</ol>
</div>
<p>Next we defined absolute and relative error:</p>
<div class="Theorem">
<p><strong>Definition.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>x</mi><mo>*</mo></msup><annotation encoding="application/x-tex">x^*</annotation></semantics></math>
be an approximation of a real number
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
<ul>
<li>The <strong>absolute error</strong> is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>x</mi><mo>*</mo></msup><mo>âˆ’</mo><mi>x</mi><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|x^* - x|</annotation></semantics></math>.</li>
<li>The <strong>relative error</strong> is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac displaystyle="true"><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>x</mi><mo>*</mo></msup><mo>âˆ’</mo><mi>x</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>x</mi><mo stretchy="true" form="postfix">|</mo></mrow></mfrac><annotation encoding="application/x-tex">\dfrac{|x^*-x|}{|x|}</annotation></semantics></math>.</li>
</ul>
</div>
<p>The base-10 logarithm of the relative error is approximately the
number of significant digits, so you can think of significant digits as
a measure of relative error.</p>
<p>Intuitively, addition &amp; subtraction â€œplay niceâ€ with absolute
error while multiplication and division â€œplay niceâ€ with relative error.
This can lead to problems:</p>
<ol type="1">
<li><p><strong>Catastrophic cancellation.</strong> When you subtract two
numbers of roughly the same size, the relative error can get much worse.
For example, both 53.76 and 53.74 have 4 significant digits, but
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>53.76</mn><mo>âˆ’</mo><mn>53.74</mn><mo>=</mo><mn>0.02</mn></mrow><annotation encoding="application/x-tex">53.76 - 53.74 = 0.02</annotation></semantics></math>
only has 1 significant digit.</p></li>
<li><p><strong>Useless precision.</strong> If you add two numbers with
very different magnitudes, then having a very low relative error in the
smaller one will not be useful.</p></li>
</ol>
<p>We did these examples in class.</p>
<ol type="1">
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ï€</mi><mo>=</mo><mn>3.141592</mn><mi>.</mi><mi>.</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\pi = 3.141592...</annotation></semantics></math>.
What is the absolute and relative error if your round
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ï€</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>3.14</mn><annotation encoding="application/x-tex">3.14</annotation></semantics></math>?</li>
</ol>
<div class="Theorem">
<p><strong>Rounding Error.</strong> The worst case relative error from
rounding to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
significant digits is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>ğ«</mi><mi>ğ</mi><mi>ğ¥</mi><mi>ğš</mi><mi>ğ­</mi><mi>ğ¢</mi><mi>ğ¯</mi><mi>ğ</mi><mspace width="0.222em"></mspace><mi>ğ</mi><mi>ğ«</mi><mi>ğ«</mi><mi>ğ¨</mi><mi>ğ«</mi></mrow><mo>â‰¤</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mn>5</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mi>k</mi></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">decimal</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><msup><mn>2</mn><mrow><mi>âˆ’</mi><mi>k</mi></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">binary</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\mathbf{relative~error} \le \begin{cases} 5 \times 10^{-k} &amp; (\text{decimal}) \\ 2^{-k} &amp; (\text{binary}). \end{cases}</annotation></semantics></math>
Since 64-bit floating point numbers have up to 53 significant digits,
they typically have a relative error of up to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mi>âˆ’</mi><mn>53</mn></mrow></msup><mo>â‰ˆ</mo><mn>1.11</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>16</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2^{-53} \approx 1.11 \times 10^{-16}</annotation></semantics></math>.
This quantity is known as <strong>machine epsilon</strong>.</p>
</div>
<p>You can sometimes re-write algorithms on a computer to avoid issues
with floating point numbers such as overflow/underflow and catastrophic
cancellation.</p>
<ol start="2" type="1">
<li><p>Consider the function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac displaystyle="true"><mrow><mn>1</mn><mo>âˆ’</mo><mo>cos</mo><mi>x</mi></mrow><mrow><mo>sin</mo><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x) = \dfrac{1 - \cos x}{\sin x}</annotation></semantics></math>.
Use Python to compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>7</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(10^{-7})</annotation></semantics></math>.</p></li>
<li><p>The exact answer to previous question is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.00000005</mn><mo>=</mo><mn>5</mn><mo>Ã—</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">0.00000005 = 5 \times 10^{-8}</annotation></semantics></math>
(accurate to 22 decimal places). Use this to find the relative error in
your previous calculation.</p></li>
<li><p>A better way to compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>
is to use a trick to avoid the catastrophic cancellation:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac displaystyle="true"><mrow><mn>1</mn><mo>âˆ’</mo><mo>cos</mo><mi>x</mi></mrow><mrow><mo>sin</mo><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac displaystyle="true"><mrow><mn>1</mn><mo>âˆ’</mo><mo>cos</mo><mi>x</mi></mrow><mrow><mo>sin</mo><mi>x</mi></mrow></mfrac><mo>â‹…</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mn>1</mn><mo>+</mo><mo>cos</mo><mi>x</mi></mrow><mrow><mn>1</mn><mo>+</mo><mo>cos</mo><mi>x</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac displaystyle="true"><mrow><mo>sin</mo><mi>x</mi></mrow><mrow><mn>1</mn><mo>+</mo><mo>cos</mo><mi>x</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x) = \dfrac{1-\cos x}{\sin x} = \dfrac{1 - \cos x}{\sin x} \cdot \left( \frac{1+ \cos x}{1+\cos x} \right) = \dfrac{\sin x}{1 + \cos x}.</annotation></semantics></math>
Use this new formula to compute
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mn>10</mn><mrow><mi>âˆ’</mi><mn>7</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(10^{-7})</annotation></semantics></math>.
What is the relative error now?</p></li>
</ol>
<p><strong>Stirlingâ€™s formula</strong> is a famous approximation for the
factorial function.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi><mo>â‰ˆ</mo><msqrt><mrow><mn>2</mn><mi>Ï€</mi><mi>n</mi></mrow></msqrt><mfrac><msup><mi>n</mi><mi>n</mi></msup><msup><mi>e</mi><mi>n</mi></msup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">n! \approx \sqrt{2 \pi n} \frac{n^n}{e^n}.</annotation></semantics></math>
We approximated Stirlingâ€™s formula using the following code.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">float</span>(math.factorial(n)))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> n: math.sqrt(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> n) <span class="op">*</span> n <span class="op">**</span> n <span class="op">/</span> math.exp(n)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(f(n))</span></code></pre></div>
<p>Our formula worked well until
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>143</mn></mrow><annotation encoding="application/x-tex">n=143</annotation></semantics></math>,
then we got an overflow error. The problem was that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>n</mi><mi>n</mi></msup><annotation encoding="application/x-tex">n^n</annotation></semantics></math>
got too big to convert to a floating point number. But you can prevent
the overflow error by adjusting the formula slightly to.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">143</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> n: math.sqrt(<span class="dv">2</span> <span class="op">*</span> math.pi <span class="op">*</span> n) <span class="op">*</span> (n <span class="op">/</span> math.e) <span class="op">**</span> n</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(f(n))</span></code></pre></div>
<h3 id="fri-jan-26">Fri, Jan 26</h3>
<p>Today we reviewed Taylor series. We recalled the following important
Maclaurin series (which are Taylor series with center
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c = 0</annotation></semantics></math>):</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><mfrac displaystyle="true"><msup><mi>x</mi><mi>n</mi></msup><mrow><mi>n</mi><mi>!</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">e^x = \sum_{n=0}^\infty \dfrac{x^n}{n!}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><mfrac displaystyle="true"><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msup><mspace width="0.167em"></mspace><msup><mi>x</mi><mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sin(x) = \sum_{n=0}^\infty \dfrac{(-1)^n \, x^{2n+1}}{(2n+1)!}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><mfrac displaystyle="true"><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msup><mspace width="0.167em"></mspace><msup><mi>x</mi><mrow><mn>2</mn><mi>n</mi></mrow></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\cos(x) = \sum_{n=0}^\infty \dfrac{(-1)^n \, x^{2n}}{(2n)!}</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac displaystyle="true"><mn>1</mn><mrow><mn>1</mn><mo>âˆ’</mo><mi>x</mi></mrow></mfrac><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><msup><mi>x</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\dfrac{1}{1-x} = \sum_{n = 0}^\infty x^n</annotation></semantics></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>âˆ</mi></msubsup><mfrac displaystyle="true"><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>âˆ’</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msup><mspace width="0.167em"></mspace><msup><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\ln(1+x) = \sum_{n = 0}^\infty \dfrac{(-1)^n \, x^{n+1}}{n+1}</annotation></semantics></math></li>
</ul>
<p>We graphed Maclaurin polynomials for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\cos x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac displaystyle="true"><mn>1</mn><mrow><mn>1</mn><mo>âˆ’</mo><mi>x</mi></mrow></mfrac><annotation encoding="application/x-tex">\dfrac{1}{1-x}</annotation></semantics></math>
on <a href="https://www.desmos.com/calculator">Desmos</a> to see how
they converge with different <strong>radii of convergence</strong>.</p>
<p>We also use the Maclaurin series for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac displaystyle="true"><mrow><mo>sin</mo><mi>x</mi></mrow><mi>x</mi></mfrac><annotation encoding="application/x-tex">\dfrac{\sin x}{x}</annotation></semantics></math>
to approximate the integral</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>âˆ«</mo><mrow><mi>âˆ’</mi><mi>Ï€</mi></mrow><mi>Ï€</mi></msubsup><mfrac><mrow><mo>sin</mo><mi>x</mi></mrow><mi>x</mi></mfrac><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\displaystyle \int_{-\pi}^\pi \frac{\sin x}{x} \, dx.</annotation></semantics></math></p>
<p>Then we did the following workshop in class.</p>
<ul>
<li>Workshop: <a href="Workshops/TaylorSeries.pdf">Taylor series</a>
<!--* <mark>Interesting idea for some extra problems w/ the triangle inequality: <https://www.deanza.edu/faculty/balmcheryl/documents/M1C_Lab3_updated.pdf></mark>--></li>
</ul>
<hr />
<h3 id="week-2-notes">Week 2 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 19</td>
<td style="text-align: left;">Martin Luther King day - no class</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 21</td>
<td style="text-align: left;">Taylorâ€™s theorem - conâ€™d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Jan 23</td>
<td style="text-align: left;">Bounding error</td>
</tr>
</tbody>
</table>
<h3 id="wed-jan-21">Wed, Jan 21</h3>
<p>Today we reviewed some theorems that we will need throughout the
course. The first is probably the most important theorem in numerical
analysis since it lets us estimate error when using Taylor series
approximations.</p>
<div class="Theorem">
<p><strong>Taylorâ€™s Theorem.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
be a function that has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n+1)</annotation></semantics></math>
derivatives in the interval between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>.
Then there exists a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>
strictly inside the interval from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
such that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><msub><mi>P</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>âˆ’</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(x) - P_n(x) = \frac{f^{(n+1)}(z)}{(n+1)!} (x-c)^{n+1}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>P</mi><mi>n</mi></msub><annotation encoding="application/x-tex">P_n</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>th
degree Taylor polynomial for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
centered at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>.</p>
</div>
<p>A special case of Taylorâ€™s theorem is when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n = 0</annotation></semantics></math>.
Then you get the Mean Value Theorem (MVT):</p>
<div class="Theorem">
<p><strong>Mean Value Theorem.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
be a function that is differentiable in the interval between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>.
Then there exists a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
strictly inside the interval from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
such that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>b</mi><mo>âˆ’</mo><mi>a</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">f&#39;(c) = \frac{f(b) - f(a)}{b-a}.</annotation></semantics></math></p>
</div>
<p>We did this example:</p>
<ol type="1">
<li>Use Taylorâ€™s theorem to estimate the error in using the 0th and 2nd
degree Maclaurin polynomials to estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.03</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\cos(0.03)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.6</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\cos(0.6)</annotation></semantics></math>.</li>
</ol>
<p>Then we started this workshop</p>
<ul>
<li><strong>Workshop:</strong> <a href="Workshops/ErrorBounds.pdf">Error
bounds</a></li>
</ul>
<!--
Note that finding upper bounds for functions is hard to teach!  You need to figure out better examples next time you teach this and maybe find better rules to help guide the students.  For example, the triangle inequality is one rule, but you also might need to spell out other rules, like for products and for different variables? Not sure what resources are available for this, I didn't find much when I looked this year. 

Talk about ways to get upper bounds for arbitrary functions.

1. Look at the endpoints for monotone functions (but not for other functions!)

2. Use 1 for sine & cosine

3. Combine upper bounds in products and using the triangle inequality.  

4. Talk about how we typically want to find upper bounds for the absolute value so that might happen at the minimum not the maximum of a function (if it goes negative).  
-->
<h3 id="fri-jan-23">Fri, Jan 23</h3>
<p>Last time we started <a href="Workshops/ErrorBounds.pdf">this
workshop</a> about using Taylorâ€™s remainder formula and the triangle
inequality to find upper bounds for functions. Today we revisited that
workshop, but first we talked about the following.</p>
<div class="Theorem">
<p><strong>Taylorâ€™s Error Formula.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
be a function that has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(n+1)</annotation></semantics></math>
derivatives in the interval between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>.
Then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>âˆ’</mo><msub><mi>P</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¤</mo><mfrac><mrow><mi>M</mi><mo>â‹…</mo><msup><mrow><mo stretchy="true" form="prefix">|</mo><mi>x</mi><mo>âˆ’</mo><mi>c</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">|f(x) - P_n(x)| \le   \frac{M \cdot |x-c|^{n+1}}{(n+1)!}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
is the maximum value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|f^{(n+1)}(z)|</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>.</p>
</div>
<p>This error formula gives a way to estimate the worst case (absolute)
error when you use a Taylor polynomial approximation.</p>
<ol type="1">
<li>The 3rd degree Taylor polynomial for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\cos x</annotation></semantics></math>
centered at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>=</mo><mi>Ï€</mi></mrow><annotation encoding="application/x-tex">c = \pi</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mn>3</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>âˆ’</mi><mn>1</mn><mo>+</mo><mfrac displaystyle="false"><mn>1</mn><mn>2</mn></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>âˆ’</mo><mi>Ï€</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">P_3(x) = -1 + \tfrac{1}{2}(x-\pi)^2</annotation></semantics></math>
(the coefficient on the 3rd degree term is zero). What is the worst case
absolute error using this polynomial to estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\cos 3</annotation></semantics></math>?</li>
</ol>
<!--
2. What is the worst case absolute error if you use the 10th degree Maclaurin polynomial to estimate $e^x$ on the interval $[-1,1]$? 
-->
<p>After that we talked about the triangle inequality.</p>
<div class="Theorem">
<p><strong>Triangle Inequality.</strong> For any numbers
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
(real or complex),
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¤</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>a</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>b</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">|a+b| \le |a| + |b|.</annotation></semantics></math></p>
</div>
<p>We talked about how you can use the triangle inequality to find
<strong>upper bounds</strong> for functions. We also talked about
<strong>tight upper bounds</strong> versus upper bounds that are not
tight. We did this example.</p>
<ol start="2" type="1">
<li>Use the triangle inequality to find an upper bound for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>x</mi><mo>cos</mo><mi>x</mi><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|x^2 + 3x \cos x|</annotation></semantics></math>.</li>
</ol>
<!-- 
### Fri, Jan 26

************
*** NOTE ***
************
I canceled this one this year because of our snow day on Monday, Jan 26. 

Today we did a workshop about the Babylonian algorithm which is an ancient method for finding square roots.  

* **Workshop**: [The Babylonian algorithm](Workshops/Babylonian.pdf) 

As part of this workshop we also covered how to define variables and functions in Python and also how to use for-loops and while-loops.  
-->
<hr />
<h3 id="week-3-notes">Week 3 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Jan 26</td>
<td style="text-align: left;">No class (snow day)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Jan 28</td>
<td style="text-align: left;">Bisection method</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Jan 30</td>
<td style="text-align: left;">Newtonâ€™s method</td>
</tr>
</tbody>
</table>
<h3 id="wed-jan-28">Wed, Jan 28</h3>
<p>We talked about how to find the roots of a function. Recall that a
<strong>root</strong> (AKA a <strong>zero</strong>) of a function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>-value
where the function hits the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>-axis.
We introduced an algorithm called the <strong>bisection method</strong>
for finding roots of a continuous function that changes sign from
positive to negative or negative to positive on an interval
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[a, b]</annotation></semantics></math>.
We wrote the following code to implement this algorithm.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bisection(f, a, b, N):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies the bisection method recursively up to N times to estimate a root </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    of a continuous function f on an interval [a,b]. </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> N <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> f(m) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> m</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (f(a) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> f(m) <span class="op">&gt;</span> <span class="dv">0</span>) <span class="kw">or</span> (f(a) <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">and</span> f(m) <span class="op">&lt;</span> <span class="dv">0</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> bisection(f, m, b, N <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> bisection(f, a, m, N <span class="op">-</span> <span class="dv">1</span>)</span></code></pre></div>
<p>We tested this algorithm on the function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>tan</mo><mi>x</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f(x) = \tan x - 1</annotation></semantics></math>
which has a root at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac displaystyle="false"><mi>Ï€</mi><mn>4</mn></mfrac><annotation encoding="application/x-tex">\tfrac{\pi}{4}</annotation></semantics></math>.</p>
<p>One feature of the bisection method is that we can easily find the
worst case absolute error in our approximation of a root. That is
because every time we repeat the algorithm and cut the interval in half,
the error reduces by a factor of 2, so that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Absolute error</mtext><mo>â‰¤</mo><mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo>âˆ’</mo><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mn>2</mn><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{Absolute error} \le \frac{(b-a)}{2^{N+1}}.</annotation></semantics></math>
We saw that it takes about 10 iterations to increase the accuracy by 3
decimal places (because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>10</mn></msup><mo>â‰ˆ</mo><msup><mn>10</mn><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">2^{10} \approx 10^3</annotation></semantics></math>).</p>
<h3 id="fri-jan-30">Fri, Jan 30</h3>
<p>Today we covered <strong>Newtonâ€™s method</strong>. This is probably
the most important method for finding roots of differentiable functions.
The formula is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>n</mi></msub><mo>âˆ’</mo><mfrac displaystyle="true"><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex"> x_{n+1} = x_n - \dfrac{f(x_n)}{f&#39;(x_n)}.</annotation></semantics></math>
This formula comes from the idea which is to start with a guess
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math>
for a root and then repeatedly improve your guess by following the
tangent line at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_n</annotation></semantics></math>
until it hits the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>-axis.</p>
<ol type="1">
<li><p>Use Newtonâ€™s method to find roots of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>tan</mo><mi>x</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tan x - 1</annotation></semantics></math>.</p></li>
<li><p>How can you use Newtonâ€™s method to find
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>?
Hint: use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>ln</mo><mi>x</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f(x) = \ln x -1</annotation></semantics></math>.</p></li>
</ol>
<div class="Theorem">
<p><strong>Theorem.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>âˆˆ</mo><msup><mi>C</mi><mn>2</mn></msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f \in C^2[a,b]</annotation></semantics></math>
and suppose that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
has a root
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>âˆˆ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r \in (a,b)</annotation></semantics></math>.
Suppose that there are constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>,</mo><mi>M</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">L,M &gt;0</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¥</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">|f&#39;(x)| \ge L</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>f</mi><mi>â€³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¤</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">|f&#39;&#39;(x)| \le M</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>âˆˆ</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">x \in [a,b]</annotation></semantics></math>.
Then
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ’</mo><mi>r</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¤</mo><mfrac><mi>M</mi><mrow><mn>2</mn><mi>L</mi></mrow></mfrac><msup><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>x</mi><mi>n</mi></msub><mo>âˆ’</mo><mi>r</mi><mo stretchy="true" form="postfix">|</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">|x_{n+1} - r| \le \frac{M}{2L} |x_n-r|^2</annotation></semantics></math>
when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>âˆˆ</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">x_n \in [a,b]</annotation></semantics></math>.</p>
</div>
<p><em>Proof.</em> Start with the first degree Taylor polynomial
(centered at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_n</annotation></semantics></math>)
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(r)</annotation></semantics></math>
including the remainder term and the Newtonâ€™s method iteration
formula:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>f</mi><mi>â€³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0</mn><mo>,</mo></mrow><annotation encoding="application/x-tex">f(r) = f(x_n) + f&#39;(x_n)(r-x_n) + \frac{1}{2} f&#39;&#39;(z)(r-x_n)^2 = 0,</annotation></semantics></math>
and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>n</mi></msub><mo>âˆ’</mo><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>â‡’</mo><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>âˆ’</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">x_{n+1} = x_n - \frac{f(x_n)}{f&#39;(x_n)} \Rightarrow f&#39;(x_n)(x_{n+1} - x_n) + f(x_n)=0.</annotation></semantics></math></p>
<p>Subtract these two formulas to get a formula that relates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(r-x_{n+1})</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(r-x_n)</annotation></semantics></math>.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>â€²</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>f</mi><mi>â€³</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">f&#39;(x_n)(r-x_{n+1}) + \frac{1}{2} f&#39;&#39;(z)(r-x_n)^2 = 0.</annotation></semantics></math></p>
<p>Use this to get an upper bound on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>r</mi><mo>âˆ’</mo><msub><mi>x</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|r-x_{n+1}|</annotation></semantics></math>.
â–¡</p>
<div class="Theorem">
<p><strong>Corollary.</strong> Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mfrac displaystyle="true"><mrow><mn>2</mn><mi>L</mi></mrow><mi>M</mi></mfrac></mrow><annotation encoding="application/x-tex">C = \dfrac{2L}{M}</annotation></semantics></math>.
As long as the Newton method iterates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>n</mi></msub><annotation encoding="application/x-tex">x_n</annotation></semantics></math>
stay in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[a,b]</annotation></semantics></math>,
then the absolute error after
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
steps will satisfy
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mfrac><mrow><msub><mi>x</mi><mi>n</mi></msub><mo>âˆ’</mo><mi>r</mi></mrow><mi>C</mi></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mo>â‰¤</mo><msup><mrow><mo stretchy="true" form="prefix">|</mo><mfrac><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>âˆ’</mo><mi>r</mi></mrow><mi>C</mi></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><msup><mn>2</mn><mi>n</mi></msup></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">|\frac{x_n-r}{C}| \le \left|\frac{x_0 - r}{C} \right|^{2^n}.</annotation></semantics></math></p>
</div>
<p>This corollary explains why, if you start with a good guess in
Newtonâ€™s method, the number of correct decimal places tends to double
with each iteration!</p>
<!--
### Fri, Feb 2

Today we looked at some examples of what can go wrong with Newton's method. We did these examples:

1. What happens if you use Newton's method with $x_0 = 0$ on $f(x) = x^3 - 2x + 2$?

2. Why doesn't Newton's method work for $f(x) = x^{1/3}$?

We also did this workshop.

* **Workshop:** [Newton's method](Workshops/NewtonsMethod.pdf)
-->
<hr />
<h3 id="week-4-notes">Week 4 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 2</td>
<td style="text-align: left;">Secant method</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 4</td>
<td style="text-align: left;">Fixed point iteration</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 6</td>
<td style="text-align: left;">Newtonâ€™s method with complex numbers</td>
</tr>
</tbody>
</table>
<!--
### Mon, Feb 5

We talked about the secant method which is a variation of Newton's method that uses secant lines instead of tangent lines.  The advantage of the secant method is that it doesn't require calculating a derivative.  The disadvantage is that it is a little slower to converge than Newton's method, but it is still much faster than the bisection method.  Here is the formula:

$$x_{n+1} = x_n - \frac{f(x_n) \, (x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}.$$

We wrote the following program in class. 

```python
def Secant(f, a, b, precision = 10**(-8)):
    while abs(b-a) > precision:
        a, b = b, b - f(b)*(b-a)/(f(b)-f(a))
    return b
```

Sometimes a function $f$ might be very time consuming for a computer to compute, so you could improve this function by reducing the number of times you have to call $f$. If speed is a concern, then this would be a better version of the function. 

```python
def Secant(f, a, b, precision = 10**(-8)):
    fa = f(a)
    fb = f(b)
    while abs(b-a) > precision:
        a, b = b, b - fb*(b-a)/(fb-fa) # update x-values
        fa, fb = fb, f(b) # update y-values using the new x-value
    return b
```

Notice how we call the function $f$ three times in each iteration of the while-loop in the first program, but by storing the result in the variables `fa` and `fb`, we only have to call $f$ once per iteration in the second version of the program.  

1. Solve the equation $2^x = 10$ using the secant method.  What would make good initial guesses $x_0$ and $x_1$? 

We finished by talking about the convergence rate of the secant method. 

<div class="Theorem">
**Theorem.** Let $f \in C^2[a,b]$ and suppose that $f$ has a root $r \in (a,b)$. There is a constant $C > 0$ such that for $x_n$, $x_{n-1}$ sufficiently close to $r$ (say $|x_0 - r| < 1/C$ and $|x_1 - r| < 1/C$), the next iterate of the secant method has
$$|x_{n+1} - r| \le C |x_n-r| \, |x_{n-1} - r|.$$
In particular, $|x_n - r|$ will converge to $r$. 
</div>

Note, the constant $C$ might be larger than the constant $\dfrac{M}{2L}$ from Newton's method, but it is usually not much larger.<!-- when $x_n$ and $x_{n-1}$ are not close to $r$, but sufficiently close to $r$ it does converge to $\dfrac{f''r)}{2f'(r)}$. -->
<!--
2. Use this formula to estimate $|x_3-r|$ in terms of $|x_1-r|$ and $|x_0 - r|$. Assume that the same constant $C$ applies for all $x_{n+1}$. 
3. Do the same for $|x_4 - r|$. 
4. Keep going until you find a pattern. 

We saw that the pattern is that the exponents of each factor is a Fibonacci number.  We talked briefly about [Binet's formula](https://en.wikipedia.org/wiki/Fibonacci_sequence#Relation_to_the_golden_ratio) for Fibonacci numbers and the golden ratio $\varphi = \frac{1 + \sqrt{5}}{2} \approx 1.618$. The lead to the following nice rule of thumb: The number of correct decimal places in the secant method increases by a factor of about 1.6 (the golden ratio) every step. 

### Wed, Feb 7

Newton's method is a special case of a method known as fixed point iteration.  A \textbf{fixed point} of a function $f(x)$ is a number $p$ such that $f(p) = p$.  Not every function has a fixed point, but we do have the following existence result:

<div class="Theorem">
**Theorem.** Let $f \in C^0[a,b]$.  If $f(x) \in [a,b]$ for every $x \in [a,b]$, then $f$ must have a fixed point in $[a,b]$.  
</div>

1. Show that $\cos x$ has a fixed point in $[0,\tfrac{\pi}{2}]$. 

2. Explain why $f(x) = e^x$ has no fixed points. 

A fixed point $p$ is **attracting** if for all $x_0$ sufficiently close to $p$, the recursive sequence defined by 
$$x_{n+1} = f(x_n)$$
converges to $p$. It is **repelling** if no (sub)sequence of $x_n$ ever converges to $p$. You can draw a picture of these fixed point iterates by drawing a [cobweb diagram](https://en.wikipedia.org/wiki/Cobweb_plot). 

<center>
<img src="https://upload.wikimedia.org/wikipedia/commons/4/41/CobwebConstruction.gif" width=300></img>
</center>

3. Show that the fixed point of $\cos x$ is attracting by repeatedly iterating. 

4. Show that $g(x) = 1 - 2x -x^5$ has a fixed point, but it is not attracting. 

<div class="Theorem">
**Theorem** If $f$ has a fixed point $p$ and 

1. $|f'(p)| < 1$, then $p$ is attracting, 
2. $|f'(p)| > 1$, then $p$ is repelling,
3. $|f'(p)| = 1$, then no info. 
</div>

You can sometimes use fixed point iteration to solve equations.  For example, here are two different ways to solve the equation $x^3 + 3x + 6 = 0$ using fixed point iteration. 

1. Re-write the equation as $\dfrac{-6}{x^2+3} = x$.

2. Replace $f(x) = 0$ with the equation $x + cf(x) = x$ where $c$ is a small constant. The constant $c = -\tfrac{1}{10}$ works well for the function above. 

When a sequence $x_n$ converges to a root $r$, we say that it has **a linear order of convergence** if there is a constant $0 < C < 1$ such that 
$$|x_{n+1} - r| \le C |x_n - r| \text{ for all } n.$$
We say that the sequence has a **quadratic order of convergence** if there is a constant $C > 0$ such that 
$$|x_{n+1} - r| \le C |x_n - r|^2 \text{ for all } n.$$
More generally, a sequence **converges with order $\alpha$** if there is are constants $C > 0$ and $\alpha > 1$ such that 
$$|x_{n+1} - r| \le C |x_n - r|^\alpha \text{ for all } n.$$

In general, a sequence that converges with order $\alpha > 1$ will have the number of correct decimal places grow by a factor of about $\alpha$ each step.  Newton's method is order 2, Secant method is order $\varphi \approx 1.618$, and the Bisection method is only linear order. 

<div class="Theorem">
**Theorem.** If $f$ is differentiable at a fixed point $p$ and $0 < |f'(p)| < 1$, then for any point $x_0$ sufficiently close to $p$, the fixed point iterates $x_n$ defined by $x_{n+1} = f(x_n)$ converge to $p$ with linear order.  If $f'(p) = 0$, then the iterates converge to $p$ with order $\alpha$ where $f^{(\alpha)}(p)$ is the first nonzero derivative of $f$ at $p$. 
</div>

### Fri, Feb 9

We started with this question:

1. Why is Newton's method a special case of fixed point iteration?  When we apply Newton's method to find a root of $f(x)$, what function $N(x)$ are we iterating?  What is the derivative of $N$ at the root $r$? 

Then we did this workshop in class. 

* **Workshop:** [Fixed point iteration](Workshops/FixedPoints.pdf)

In one step of the workshop, we used the **triangle inequality** which says that for any two numbers $a$ and $b$, 
$$|a+b| \le |a| + |b|.$$

<!--
We've already seen that Newton's method has quadratic order of convergence.  Newton's method is the first in a family of root finding techniques called **Householder methods.** If $f \in C^{k+1}[a,b]$ has a root $r \in (a,b)$, then for $x_0$ close to $r$, we define a sequence
$$x_{n+1} = x_n + k \dfrac{(1/f)^{(k-1)} (x_n)}{(1/f)^{(k)} (x_n)}$$
When $x_0$ is close enough to $r$, the Householder iterates $x_n$ converge to $r$ with order $k+1$.

2. Show that Householder iteration with $k = 1$ is Newton's method.  

We finished with a cool fact about Newton's method.  It also works for to find complex number roots if you use complex numbers.  We talked about the polynomial $x^3 - 1 = (x-1)(x^2+x+1)$ which has three roots: $x = 1$ and $x = \dfrac{-1 \pm i \sqrt{3}}{2}$. We talked about which complex numbers end up converging to which root as you iterate Newton's method.  You get a beautiful fractal pattern:

<center>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/db/Julia_set_for_the_rational_function.png" width=300></img>
<figcaption>Basins of attraction for the roots of $x^3-1$.</figcaption>
</figure>
</center>
-->
<hr />
<h3 id="week-5-notes">Week 5 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 9</td>
<td style="text-align: left;">Solving nonlinear systems</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 11</td>
<td style="text-align: left;">Systems of linear equations</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 13</td>
<td style="text-align: left;">LU decomposition</td>
</tr>
</tbody>
</table>
<!--
### Mon, Feb 12

Today we talked about systems of linear equations and linear algebra. Before we got to that, we looked at one more cool thing about Newton's method.  It also works for to find complex number roots if you use complex numbers.  We talked about the polynomial $x^3 - 1 = (x-1)(x^2+x+1)$ which has three roots: $x = 1$ and $x = \dfrac{-1 \pm i \sqrt{3}}{2}$. We talked about which complex numbers end up converging to which root as you iterate Newton's method.  You get a beautiful fractal pattern:

<center>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/d/db/Julia_set_for_the_rational_function.png" width=300></img>
<figcaption>Basins of attraction for the roots of $x^3-1$.</figcaption>
</figure>
</center>

After that we started a review of row reduction from linear algebra.

1. Suppose you have a jar full of pennies, nickles, dimes, and quarters.  There are 80 coins in the jar, and the total value of the coins is $10.00.  If there are twice as many dimes as quarters, then how many of each type of coin are in the jar?  

We can represent this question as a system of linear equations. 
$$p+n+d+q = 80$$
$$p+5n+10d+25q = 1000$$
$$d = 2q$$
where $p,n,d,q$ are the numbers of pennies, nickles, dimes, and quarters respectively. It is convenient to use matrices to simplify these equations:
$$\begin{pmatrix} 1 & 1 & 1 & 1  \\ 1 & 5 & 10 & 25  \\ 0 & 0 & 1 & -2 \end{pmatrix} \, \begin{pmatrix} p \\ n \\ d \\ q \end{pmatrix} = \begin{pmatrix} 80 \\ 1000 \\ 0 \end{pmatrix}.$$
Here we have a matrix equation of the form $Ax = b$ where $A \in \R^{3 \times 4}$, $x \in \R^4$ is the unknown vector, and $b \in \R^3$. Then you can solve the problem by row-reducing the augmented matrix

$$\left( \begin{array}{cccc|c} 1 & 1 & 1 & 1 & 80 \\ 1 & 5 & 10 & 25 & 1000 \\ 0 & 0 & 1 & -2 & 0\end{array}\right)$$

which can be put into **echelon form**

$$\left( \begin{array}{cccc|c} 1 & 1 & 1 & 1 & 80 \\ 0 & 4 & 9 & 24 & 920 \\ 0 & 0 & 1 & -2 & 0\end{array}\right)$$

Then the variables $p, n$, and $d$ are **pivot variables**, and the last variable $q$ is a **free variable**. Each pivot variable depends on the value(s) of the free variables.  A solution of a system of equations is a formula for the pivot variables as functions of the free variables.   

Recall the following terminology from linear algebra. For any matrix $A \in \R^{m \times n}$ (i.e., that has real number entries with $m$ rows and $n$ columns):

* The **rank** of $A$ is the number of pivots.  It is also the dimension of the column space since the columns of $A$ with pivots are linearly independent and form a basis for the column space. 

* The **null space** of $A$ is the set $\{x \in \R^n \, : \, Ax = 0\}$.

* The **nullity** of $A$ is the number of free variables which is the same as the dimension of the null space of $A$. 

<!--Recall that the number of pivots in a matrix is called the **rank** of the matrix, and does not change when you row reduce. The rank is also the dimension of the **column space** $\on{Col}(A)$ which is the span of the columns of $A$.  The number of free variables of a matrix $A \in \R^{m \times n}$ is the dimension of the **null space** $\on{Null} (A) = \{x \in \R^n : Ax = 0 \}$. The dimension of the null space is sometimes called the **nullity** of the matrix.  Since every column of a matrix contains a pivot or corresponds to a free variable, we have the following important (but simple) theorem.-->
<!--

<div class="Theorem"> 
**Rank + Nullity Theorem.** Let $A \in \R^{m \times n}$.  Then the rank of $A$ plus the nullity of $A$ must equal $n$. 
</div>

A matrix equation $Ax = b$ has a solution if and only if $b$ is in the column space of $A$.  If $b$ is in the column space, then there will be either one unique solution if there are no free variables (i.e., the nullity of $A$ is zero) or there will be infinitely many solutions if there are free variables. 

If $A \in \R^{n \times n}$ (i.e., $A$ is a square matrix) and the rank of $A$ is $n$, then $A$ is **invertible** which means that there is a matrix $A^{-1}$ such that $A A^{-1} = A^{-1} A = I$ where $I$ is the **identity matrix**
$$I = \begin{pmatrix} 1 & 0 & \ldots & 0 \\ 0 & 1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & 1 \end{pmatrix}.$$  
You can use row-reduction to find the inverse of an invertible matrix by row reducing the augmented matrix $\left( \begin{array}{c|c} A & I \end{array} \right)$ until you get $\left( \begin{array}{c|c} I & A^{-1} \end{array} \right)$. 

2. Use row-reduction to find the inverse of $A = \begin{pmatrix} 1 & 3 \\ 2 & 5 \end{pmatrix}$. (<https://youtu.be/cJg2AuSFdjw>)

3. Use the inverse to solve $\begin{pmatrix} 1 & 3 \\ 2 & 5 \end{pmatrix} x = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$. 

In practice, inverse matrices are rarely used to solve systems of linear equations for a couple of reasons. 

1. Most matrices aren't invertible.
2. Finding the inverse is at least as hard computationally as row reduction, so you might as well just use row reduction.  

### Wed, Feb 14

Today we talked about LU decomposition.  We defined the LU decomposition as follows.  The **LU decomposition** of a matrix $A \in \R^{m \times n}$ is a pair of matrices $L \in \R^{m \times m}$ and $U \in \R^{m \times n}$ such that $A = LU$ and $U$ is in echelon form and $L$ is a lower triangular matrix with 1's on the main diagonal, 0's above the main diagonal, and entries $L_{ij}$ in row $i$, column $j$ that are equal to the multiple of row $i$ that you *subtracted* from row $j$ as you row reduced $A$ to $U$. 

1. Compute the LU decomposition of $A = \begin{pmatrix} 1 & 1 & 1 & 1 \\ 2 & 2 & 5 & 3 \\ -1 & -1 & 14 & 4 \end{pmatrix}$. 

2. Use the LU decomposition to solve $Ax = \begin{pmatrix} 2 \\ 6 \\ 8 \end{pmatrix}$. 

We also did this workshop. 

* **Workshop:** [LU decomposition](Workshops/LUdecomposition.pdf)

We finished with one more example. 

3. For what real numbers $a$ and $b$ does the matrix $\begin{pmatrix} 1 & 0 & 1 \\ a & a & a \\ b & b & a \end{pmatrix}$ have an LU decomposition? (<https://youtu.be/-eA2D_rIcNA>)

### Fri, Feb 16


Today we talked about what it means for a linear system to be **ill-conditioned**.  This is when a small change in the vector $b$ can produce a large change in the solution vector $x$ for a linear system $Ax=b$.  

Consider the following matrix:

$$A = \begin{pmatrix} 1 & 1 \\ 1 & 1.001 \end{pmatrix}$$

Let $y = \begin{pmatrix} 2 \\ 2 \end{pmatrix}$ and $z =  \begin{pmatrix} 2 \\ 2.001 \end{pmatrix}$.

1. Solve $Ax = y$ and $Ax = z$. Hint: $A^{-1} = \begin{pmatrix} 1001 & -1000 \\ -1000 & 1000 \end{pmatrix}$. Notice that even though $y$ and $z$ are very close, the two solutions are not close at all.  A matrix $A$ with the property that solutions of $Ax = b$ are very sensitive to small changes in $b$ is called **ill-conditioned**.

Consider the matrix $B = \begin{pmatrix} 0.001 & 1 \\ 1 & 1 \end{pmatrix}$ which has $LU$ decomposition 
$$B = LU = \begin{pmatrix} 1 & 0 \\ 1000 & 1 \end{pmatrix} \,  \begin{pmatrix} 0.001 & 1 \\ 0 & -999 \end{pmatrix}.$$  
Although $B$ is not ill-conditioned, you have to be careful using row reduction to solve equations with this matrix because both $L$ and $U$ in the LU-decomposition for $B$ are ill-conditioned.

2. Use the LU-decomposition to solve $Bx = \begin{pmatrix} 1 \\ 2 \end{pmatrix}.$

<!--To solve the system, 

1. First, solve $Ly = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$ to get $y = \begin{pmatrix} 1 \\ -998 \end{pmatrix}$. 

2. Then, solve $Ux = y$.  You should get $x = \begin{pmatrix} 1.001001 \\ 0.998999 \end{pmatrix}$ by solving the system 
$$0.001x_1 + x_2 = 1,$$
$$-999 x_2 = -998.$$
If you solve this system, it is easy to make a rounding mistake and get $x_2 = 1$ instead of $\frac{998}{999}$. If that happens, then you'll get $x_1 = 0$ instead of its actual value.-->
<!--
3. The inverse of the matrix $L$ in the LU decomposition above is 
$$L^{-1} = \begin{pmatrix} 1 & 0 \\ -1000 & 1 \end{pmatrix}.$$
Show that $L$ is ill-conditioned by finding a vector $y'$ close the $y = \begin{pmatrix} 1 \\ 2\end{pmatrix}$, but such that the corresponding solutions $x$ and $x'$ to the matrix equations $Lx = y$ and $Lx' = y'$ are not close. 



<!--
3. When you row-reduce $\ds \left( \begin{array}{cc|c} 0.001 & 1 & 1 \\ 1 & 1 & 2 \end{array} \right)$ without swapping rows, you get $\ds \left( \begin{array}{cc|c} 0.001 & 1 & 1 \\ 0 & -999 & -998 \end{array} \right)$.  Let $R = \begin{pmatrix} 0.001 & 1 \\ 0 & -999 \end{pmatrix}$. Show that $R$ is ill-conditioned by comparing the solutions of these two systems: 
$$Rx = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \text{ and } Rx = \begin{pmatrix} 1.1 \\ 1 \end{pmatrix}.$$

This can be a problem if there is any rounding error in the extra column after row reduction.  
-->
<!--It is possible to avoid this problem using the **method of partial pivoting**.  The idea is simple: when more than one entry could be the pivot for a column, always choose the one with the largest absolute value.  

In the example above, since both entries in the first column of $B = \begin{pmatrix} 0.001 & 1 \\ 1 & 1 \end{pmatrix}$ are positive, either could be the pivot.  To use **partial pivoting**, swap rows so that the pivot is the entry in column one with the larger absolute value, and the do the usual row reduction.  

4. Show that when you row reduce $\begin{pmatrix} 1 & 1 \\ 0.001 & 1 \end{pmatrix}$ to echelon form using partial pivoting, the resulting matrix is not ill-posed.  <span style="background-color:yellow">Hmmm... this is tricky because you haven't introduced the condition number yet...</span>
-->
<!--
### Norms of Vectors

A **norm** is a function $\|\cdot\|$ from a vector space $V$ to $[0,\infty)$ with the following properties:

1. $\|x\| = 0$ if and only if $x=0$.
2. $\|c x \| = |c| \|x\|$ for all $x \in V$ and $c \in \R$.  
3. $\|x+y\| \le \|x\| + \|y\|$ for all $x, y \in V$.  

Intuitively a norm measures the length of a vector.  But there are different norms and they measure length in different ways.  The three most important norms on the vector space $\R^n$ are:

1. **The $2$-norm** (also known as the **Euclidean norm**) is the most commonly used, and it is exactly the formula for the length of a vector using the Pythagorean theorem. 
$$\|x\|_2 = \sqrt{x_1^2 + x_2^2 + \ldots + x_n^2}.$$

2. **The $1$-norm** (also known as the **Manhattan norm**) is
$$\|x\|_1 = |x_1|+|x_2|+\ldots+|x_n|.$$
This is the distance you would get if you had to navigate a city where the streets are arranged in a rectangular grid and you can't take diagonal paths.  

3. **The $\infty$-norm** (also known as the **Maximum norm**) is 
$$\|x\|_\infty = \max \{ |x_1|, |x_2|, \ldots, |x_n| \}.$$

These are all special cases of **$p$-norms** which have the form
$$\|x\|_p = \sqrt[p]{|x_1|^p + |x_2|^p + \ldots + |x_n|^p}.$$

We used Desmos to graph the set of vectors in $\R^2$ with $p$-norm equal to one, then we could see how those norms change as $p$ varies between 1 and $\infty$. 

<center>
<iframe src="https://www.desmos.com/calculator/ridobsezmp?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
</center>

### Norms of Matrices

The set of all matrices in $\R^{m \times n}$ is a vector space. So it makes sense to talk about the norm of a matrix.  There are many ways to define norms for matrices, but the most important for us are **operator norms** (also known as **induced norms**).  For a matrix $A \in \R^{m \times n}$, the **induced $p$-norm** is 
$$\|A\|_p = \max \{\|Ax\|_p : x \in \R^n, \|x\|=1\}.$$  
Two important special cases are 

1. When $p=2$, the induced norm $\|A\|_2$ is the square root of the largest eigenvalue of $A^T A$.  
2. When $p=\infty$, the induced norm $\|A\|_\infty$ is the largest 1-norm of the rows of $A$.

### Condition Number

For an invertible matrix $A \in \R^{n \times n}$, the **condition number** of $A$ is $\kappa(A) = \|A\| \, \|A^{-1}\|$ (using any induced norm).  


**Rule of thumb.** If the entries of $A$ and $b$ are both accurate to $n$-significant digits and the condition number of $A$ is $\kappa(A) = 10^k$, then the solution of the linear system $Ax = b$ will be accurate to $n-k$ significant digits. 

-->
<hr />
<h3 id="week-6-notes">Week 6 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 16</td>
<td style="text-align: left;">LU decomposition with pivoting</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 18</td>
<td style="text-align: left;">Review</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 20</td>
<td style="text-align: left;"><strong>Midterm 1</strong></td>
</tr>
</tbody>
</table>
<!--
### Mon, Feb 19

<div class="Theorem">
**Theorem.** If $A \in \R^{n \times n}$ is invertible, then the relative error in the solution of the system $A x = b$ is bounded by
$$\frac{\|x-x'\|}{\|x\|} \le \kappa(A) \frac{\|b-b'\|}{\|b\|}.$$
</div>

*Proof.* Using the properties of the induced norm, 
$$\|b\| = \|A x \| \le \|A\| \, \|x\| \text{ and } \|x-x'\| = \|A^{-1}(b-b')\| \le \|A^{-1}\| \, \|b-b'\|,$$
so putting both together gives 
$$\|b\| \|x-x'\| \le \|A\| \, \|A^{-1}\| \, \|x\| \, \|b-b'\|.$$  
This leads directly to the inequality above when you separate the factors with $x$ from those with $b$. â–¡

This explains why the number of significant digits in the solution to $A x = b$ may have up to $k$ fewer significant digits than the entries of $A$ and $b$ when the condition number $\kappa(A) = 10^k$. 

1. Let $A = \begin{pmatrix} 1.000 & 1.001 \\ 1.000 & 1.000 \end{pmatrix}$ and $b = \begin{pmatrix} 2.000 \\ 2.001 \end{pmatrix}$.  How many significant digits does the solution to $Ax = b$ have? 

Last time we saw an example of a matrix $B = \begin{pmatrix} 0.001 & 1 \\ 1 & 1 \end{pmatrix}$ which is not ill-conditioned by itself.  However, both $L$ and $U$ in its LU-decomposition were ill-conditioned. It is possible to avoid this problem using the **method of partial pivoting**.  The idea is simple: when more than one entry could be the pivot for a column, always choose the one with the largest absolute value.  

You keep track of the row swaps as you use the method of partial pivoting, always apply the same row swaps to a copy of the identity matrix. At the end, you will have a **permutation matrix** $P$ and the **LU-decomposition with partial pivoting** is
$$PA = LU.$$
The fixes two problems:

* When there are zero entries where pivots should be, you can't do a regular LU-decomposition.
* When you do an LU-decomposition, the matrices L and U might be ill-conditioned, even if $A$ isn't. The method of partial pivots avoids that problem. 

One nice thing to know about permutation matrices is that they are always invertible and $P^{-1} = P^T$ where $P^T$ is the **transpose** of $P$ obtained by converting every row of $P$ to a column of $P^T$.  

Find the LU-decomposition with partial pivoting for these matrices
 
2. $A = \begin{pmatrix} 0 & 1 & 2 \\ 1 & 1 & 1 \end{pmatrix}$

3. $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}$ <!--$B= \begin{pmatrix} -2 & 8 & 2 \\ 2 & 1 & 4 \\ 4 & 2 & 0 \end{pmatrix}$-->
<!--
Didn't have time for this one:
4. Show that when you row reduce $\begin{pmatrix} 0.001 & 1 \\ 1 & 1 \end{pmatrix}$ to echelon form using partial pivoting, the resulting LU matrices are not ill-posed.  
-->
<!--
### Wed, Feb 21

Today we reviewed for the midterm exam. We reviewed the things you need to memorize. We also talked about the following problems. 

1. Find the LU decomposition of $\begin{pmatrix} 1 & 0 & 3 \\ 4 & 2 & 9 \\ & -2 & -6 & 0 \end{pmatrix}$.

2. Find and classify the fixed points of $f(x) = \dfrac{x^3}{8} + 1$. This was a little hard to solve, because it isn't easy to factor the polynomial $x^3 - 8x + 8$.  But it does have computable roots $2$ and $1 \pm \sqrt{5}$.  

3. How would you use secant method to find the one negative root of $x^3 - 8x + 8$?  What would make good choices for $x_0$ and $x_1$?  What is $x_2$ for those choices? 

4. If $a = 7.911 \times 10^{-17}$ and $b = 5.032 \times 10^{-15}$, then how many significant digits do the following have?
    a. $a - b$.
    b. $a/b$. 
-->
<hr />
<h3 id="week-7-notes">Week 7 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Feb 23</td>
<td style="text-align: left;">Matrix norms and conditioning</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Feb 25</td>
<td style="text-align: left;">Inner-products and orthogonality</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Feb 27</td>
<td style="text-align: left;">Unitary and Hermitian matrices</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-8-notes">Week 8 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 2</td>
<td style="text-align: left;">Gram-Schmidt algorithm</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 4</td>
<td style="text-align: left;">QR decomposition</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 6</td>
<td style="text-align: left;">Orthogonal projections</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-9-notes">Week 9 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 16</td>
<td style="text-align: left;">Least squares problems</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 18</td>
<td style="text-align: left;">Least squares problems - conâ€™d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 20</td>
<td style="text-align: left;">Orthogonal functions</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-10-notes">Week 10 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 23</td>
<td style="text-align: left;">Continuous least squares</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Mar 25</td>
<td style="text-align: left;">Fourier series</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Mar 27</td>
<td style="text-align: left;">Fourier series - conâ€™d</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-11-notes">Week 11 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Mar 30</td>
<td style="text-align: left;">Numerical integration</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 1</td>
<td style="text-align: left;">Newton-Cotes methods</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 3</td>
<td style="text-align: left;">Error in Newton-Cotes methods</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-12-notes">Week 12 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 6</td>
<td style="text-align: left;">Numerical differentiation</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 8</td>
<td style="text-align: left;">Review</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 10</td>
<td style="text-align: left;"><strong>Midterm 2</strong></td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-13-notes">Week 13 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 13</td>
<td style="text-align: left;">Eigenvectors and eigenvalues</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 15</td>
<td style="text-align: left;">Power iteration</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 17</td>
<td style="text-align: left;">Schur triangular decomposition</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="week-14-notes">Week 14 Notes</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Day</th>
<th style="text-align: left;">Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Mon, Apr 20</td>
<td style="text-align: left;">QR algorithm</td>
</tr>
<tr class="even">
<td style="text-align: center;">Wed, Apr 22</td>
<td style="text-align: left;">Singular value decomposition</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Fri, Apr 24</td>
<td style="text-align: left;">Applications of the SVD</td>
</tr>
<tr class="even">
<td style="text-align: center;">Mon, Apr 27</td>
<td style="text-align: left;">Last day, recap &amp; review</td>
</tr>
</tbody>
</table>
<hr />
<p><br> <br> <br> <br> <br> <br> <br> <br></p>
</body>
</html>
